[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intermediate Stata (Statistical Modeling)",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#how-to-use-this-document",
    "href": "index.html#how-to-use-this-document",
    "title": "Intermediate Stata (Statistical Modeling)",
    "section": "How to use this document",
    "text": "How to use this document\nThese notes are published using Quarto. The Stata code is first rendered using a Stata dynamic document. The source code for these notes can be found at https://github.com/CSCAR/workshop-stata-intro for the curious.\nAll images should link to full-size versions to see detail if needed."
  },
  {
    "objectID": "index.html#cscar",
    "href": "index.html#cscar",
    "title": "Intermediate Stata (Statistical Modeling)",
    "section": "CSCAR",
    "text": "CSCAR\nhttp://cscar.research.umich.edu/\nCSCAR is available for free consultations with PhD statisticians (email deskpeople@umich.edu to request a consultation).\nCSCAR also has GSRAs available for more immediate help. Walk-ins to our office in Rackham are welcomed Monday-Friday 9am to 5pm (Closed Tuesdays 12-1pm). Alternatively, on our website, you can self-schedule into an hour consultation with the graduate students, which can be either remote or in-person (these are usually available same-day or next-day).\nCSCAR operates a email for help with statistical questions, feel free to send concise questions to stats-consulting@umich.edu.\nThe current contact for questions about the notes: Josh Errickson (jerrick@umich.edu)."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Intermediate Stata (Statistical Modeling)",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThese notes have evolved over the years thanks to many CSCAR statisticians, including Giselle Kolenic, Brady West, Heidi Reichert, and Lingling Zhang.\nThis material was created for use in workshops and short courses presented by faculty and staff from the Consulting for Statistics, Computing & Analytics Research (CSCAR) at the University of Michigan. No part of this material may be used for other purposes, copied, changed, or sold."
  },
  {
    "objectID": "01-summarizing-data.html#footnotes",
    "href": "01-summarizing-data.html#footnotes",
    "title": "1  (_dta has notes)",
    "section": "",
    "text": "We won’t cover in this class, but there are multiple-equation estimating commands which have syntax command (varlist) (varlist) ... (varlist) [if] [in] [weight] [,options]. ↩︎\nWe could just run correlate, but the postestimation commands following mean are fairly limited, so bare with me here. Postestimation commands following models are much more interesting!↩︎\nThis is true of most p-values - it’s often the case that large sample sizes can provide small p-values for scientifically insignificant effects. However, correlation is especially susceptible to this issue.↩︎"
  },
  {
    "objectID": "02-visualization.html",
    "href": "02-visualization.html",
    "title": "Intermediate Stata (Statistical Modeling)",
    "section": "",
    "text": "# Visualization\nStata has robust graphing capabilities that can both generate numerous types of plots, as well as modify them as needed. We’ll only cover the basics here, for a reference we would recommend A Visual Guide to Stata Graphics by Michael Mitchell, which lays out step-by-step syntax for the countless graphs that can be generated in Stata.\nLet’s reload the auto dataset to make sure we’re starting on the same page.\n. sysuse auto, clear\n(1978 automobile data)\n\n## The graph command\nMost (though not all, see some other graphs below) graphs in Stata are created by the graph command. Generally the syntax is\ngraph &lt;type&gt; &lt;variable(s)&gt;, &lt;options&gt;\nThe “type” is the subcommand.\nFor example, to create a bar chart of price by rep78, we could run\n. graph bar price, over(rep78)\n\n\nFor further information, we could instead construct a boxplot. ~~~~ . graph box price, over(rep78)\n\n&lt;img src=\"Graph1.svg\" &gt;\n\nThere are a few other infrequently used graphs, see `help graph` for details.\n\nThere is a plot subcommand, `twoway`, which takes additional sub-subcommands, and supports a wide range of types.\n\n```\ngraph twoway &lt;type&gt; &lt;variable(s)&gt;, &lt;options&gt;\n```\n\n`twoway` creates most of the scatterplot-esque plots. The \"types\" in `twoway` are subcommands different from the subcommands in non-`twoway` `graph`,\nit takes options such as `scatter` to create a scatterplot:\n\n. graph twoway scatter mpg weight\n\n&lt;img src=\"Graph2.svg\" &gt;\n\n**Note:** For `graph twoway` commands, the `graph` is optional. E.g., these commands are equivalent:\n```\ngraph twoway scatter mpg weight\ntwoway scatter mpg weight\n```\nThis is *not* true of commands like `graph box`.\n\n\nThe options in the graphing commands are quite extensive and enable tweaking of many different settings. Rather than a full catalog of the options,\nhere's an example:\n. twoway scatter mpg weight, msymbol(s) /// &gt; mcolor(blue) /// &gt; mfcolor(yellow) /// &gt; msize(3) /// &gt; title(“Mileage vs Weight”) /// &gt; xtitle(“Weight (in lbs)”) /// &gt; ytitle(“Mileage”) /// &gt; ylabel(15 “15” 25 “25” 35 “35”)\n. ~~~~\n\nGraphs made using twoway have an additional benefit - it is easy to stack them. For example, twoway lfit creates a best-fit line between the points: ~~~~ . twoway lfit mpg weight\n\n&lt;img src=\"Graph4.svg\" &gt;\n\nThis isn't really that useful. It would be much better to overlap those two - generate the scatter plot, then add the best fit line. We can easily do\nthat by passing multiple plots to `twoway`:\n\n. twoway (scatter mpg weight) (lfit mpg weight)\n\n&lt;img src=\"Graph5.svg\" &gt;\n\nNote that the order of the plots matters - if you can tell, the best-fit line was drawn on top of the scatter plot points. If you reversed the order\nin the command (`twoway (lfit mpg weight) (scatter mpg weight)`), the line would be drawn first and the points on top of it.\n\nFinally, note that options can be passed to each individual plot:\n\n. twoway (scatter mpg weight, msymbol(t)) /// &gt; (lfit mpg weight, lcolor(green))\n\n&lt;img src=\"Graph6.svg\" &gt;\n\nPutting these options \"globally\", as `twoway (...) (...), msymbol(to)` would NOT work, as `msymbol` is an option specifically for `twoway scatter`\n(and a few others), not for the more general `twoway`. There are options that apply to the `twoway` command, see `help twoway_options` for details.\n\nThere is an alternate way to specify the overlaid plots. These two commands are equivalent:\n\n```\ntwoway (scatter mpg weight) (lfit mpg weight)\ntwoway scatter mpg weight || lfit mpg weight\n```\n\nWe prefer the former as it makes it's cleaner to distinguish when you have multiple overlaid plots with their own options, but some authors may chose\nthe latter.\n\n^#^^#^ Other graphs\n\nThere are a very large number of graphs which do not exist under the `graph` command. Most are very niche, but the most important general example is\nhistogram, which has its own command.\n\n. histogram mpg (bin=8, start=12, width=3.625)\n\n&lt;img src=\"Graph7.svg\" &gt;\n\nYou can see a full list of the non-graph plots by looking at\n\n```\nhelp graph other\n```\n\n^#^^#^ Plotting by group\n\nAll graph commands accept a `by(&lt;grouping var&gt;)` option which will repeat the graphing command for each level of the grouping variable, and\ndisplay all graphs on the same output. For example,\n\n. twoway (scatter mpg weight) (lfit mpg weight), by(foreign)\n\n&lt;img src=\"Graph8.svg\" &gt;\n\nIt often looks better to see the two plots overlaid on each other for a more direct comparison. To do this, rather than using `by(...)`, we'll instead\nadd each overlay conditionally:\n\n. twoway (scatter mpg weight if foreign == 0) /// &gt; (scatter mpg weight if foreign == 1) /// &gt; (lfit mpg weight if foreign == 0) /// &gt; (lfit mpg weight if foreign == 1)\n\n&lt;img src=\"Graph9.svg\" &gt;\n\nNotice that Stata automatically made each plot a separate color, but not in a logical fashion. Here's a cleaned up version:\n\n. twoway (scatter mpg weight if foreign == 0, mcolor(orange)) /// &gt; (scatter mpg weight if foreign == 1, mcolor(green)) /// &gt; (lfit mpg weight if foreign == 0, lcolor(orange) lwidth(1.4)) /// &gt; (lfit mpg weight if foreign == 1, lcolor(green) lwidth(1.4)), /// &gt; legend(label(1 “Domestic”) label(2 “Foreign”) order(1 2)) /// &gt; title(“Mileage vs Weight”) xtitle(“Weight (lbs)”) /// &gt; ytitle(“Mileage”)\n~~~~\n\n(Since its not entirely clear from the code, the order(1 2) argument inside legend serves two purposes - first, it “orders” the entries in the legend box, but secondly and more importantly, it does not contain 3 or 4. If you look at the previous plot, it had four entries in the legend for the two scatters plus two lfits. By excluding 3 and 4 from order [3 and 4 corresponding to the two lfits], their legend entries are ignored.)\n## Getting help on Graphs\nThere are a ton of options in all these graphs. Rather than list them all, we instead direct you to some various help pages.\nFor general assistance, start with\nhelp graph\nEach individual type of graph has its own help page:\nhelp graph box\nhelp graph twoway\nhelp twoway scatter\nhelp histogram\nThere are various generalized options which are the same over the variety of plots. These can be found in the documentation of each individual graph, or you can access them directly:\n\n\n\n\n\n\n\nTopics\nHelp command\n\n\n\n\nHelp with titles, subtitles, notes, captions.\nhelp title_options\n\n\nAxis labels, tick marks, scaling, etc.\nhelp axis_options\n\n\nManipulating the legend\nhelp legend_options\n\n\nModifying points (e.g. scatter)\nhelp marker_options\n\n\nAdding labels to markers\nhelp marker_label_options\n\n\nOptions for any lines (e.g. lfit)\nhelp cline_options\n\n\n\n## Displaying multiple graphs simultaneously\nYou may have noticed that opening a new plot closes the old one. What if you wanted to compare the plots? The behind-the-scenes reason that the old plots are closed is that Stata names each plot and each plot can only be open once. The default name is “Graph”, so with each new plot, the “Graph” plot is overridden. If you closed a plot and wanted to re-open it, you can run the following at any point until you run another graph just like with estimation commands.\ngraph display Graph\nWhen we create a new plot with the default name, we lose the last one.\nIf we give a plot a non-default name, it will be saved (so that it can be re-displayed later) and more importantly, will open a new window without closing the last. Running two plots with custom names opens two separate windows. (These are not run in the notes because obviously this won’t demonstrate well, but try them on your own.)\nhist price, name(g1)\nhist mpg, name(g2)\nNames can be re-used (and plots re-generated) easily:\nhist price, title(\"Histogram of Price\") name(g1, replace)\nWe can also list (using dir), re-display (using display), or drop graphs (using drop):\ngraph dir\ngraph display g1\ngraph drop g1\ngraph drop _all\nFinally, if you’d rather have all the graphs in one window with tabs instead of separate windows, use\nset autotabgraphs on\n(You can pass the permanently option to not have to do this every time you open Stata.) You still need to name graphs separately.\n## Exercise 2\nReload the NHANES data if you haven’t:\nwebuse nhanes2, clear\nUsing twoway scatter and twoway lfit, create a scatter plot of diastolic and systolic blood pressure, by gender. Be sure to color the lines and points consistenly and to clean up the legend."
  },
  {
    "objectID": "03-univariate-analysis.html#footnotes",
    "href": "03-univariate-analysis.html#footnotes",
    "title": "3  One-sample t test",
    "section": "",
    "text": "This is by the central limit theorem.↩︎\nNamed not for students in a class, but the pseudonym for statistician William Sealy Gosset. Gosset worked at the Guinness brewery when he was performing some of his seminal work and wasn’t allowed to publish under his own name, so he used the pseudonym Student.↩︎\nThis requires the assumption that the ordinal variable can be well approximated by a continuous variable. This assumption is fine if each level of the ordinal variable is “equally” spaced (e.g. the amount of “effort” to go from level 1 to level 2 is the same as from level 4 to level 5).↩︎\nThe statistical reasoning is that when we’re dealing with proportions, the variance is determined directly by the mean. A t-test assumes we need to estimate the variance as well. A Z-test assumes we know the variance, which will be more efficient.↩︎"
  },
  {
    "objectID": "04-regression.html#footnotes",
    "href": "04-regression.html#footnotes",
    "title": "4  _cons | 5.113759 4.889846 1.05 0.299 -4.636317 14.86384",
    "section": "",
    "text": "There are variations of regression with multiple outcomes, but they are for very specialized circumstances and can generally be fit as several basic regression models instead.↩︎\nThe 2 and 71 are degrees of freedom. They don’t typically add any interpretation.↩︎\nThe only exception is if the predictor being added is either constant or identical to another variable.↩︎\nThis is why it’s important to familiarize yourself with the units in your data!↩︎\nOnce we know the value of all but one of the dummies, the last is automatically determined so adds no new information.↩︎\nIt’s a bit confusing because if you look at the codebook for rep78, Stata correctly surmises that it is categorical. However, when it comes to modeling, Stata is not willing to make that assumption.↩︎\nNote that if you provide data with perfect correlation, Stata will drop one of them for you. This in only a thought exercise. If it helps, imagine their correlation is 99% instead of perfect, and add “almost” as a qualifier to most claims.↩︎\nTechnically there could be a scaling factors such that \\(^X_1 = aX_2 + b^\\), but let’s assume without loss of generality that \\(^a=1^\\) and \\(^b=0^\\).↩︎\nTechnically that maximizes likelihood, but that distinction is not important for understanding.↩︎"
  },
  {
    "objectID": "05-mixed-models.html#footnotes",
    "href": "05-mixed-models.html#footnotes",
    "title": "5  _cons | 1.644184 .0161148 102.03 0.000 1.612599 1.675768",
    "section": "",
    "text": "She may have salary information for only a subset of those years, but would have missing values in the other years.↩︎\nThough why called it mixed at that point?↩︎\nTypically, salary information is very right-skewed, and a log transformation makes the data closer to normal.↩︎"
  },
  {
    "objectID": "06-survey-data.html",
    "href": "06-survey-data.html",
    "title": "6  name type format label Variable label",
    "section": "",
    "text": "# Survey Data\nOne major strength of Stata is the ease with which it can analyze data sets arising from complex sample surveys. When working with data collected from a sample with a complex design (anything above and beyond a simple random sample of a population, where the sample design involves clustering and stratification of sampled elements, and multiple stages of sampling), standard statistical analysis procedures that assume a simple random sample (such as everything we’ve discussed so far) will result in very biased estimates of statistics that do not take the design of the sample into account. Two major problems arise when survey data is analyzed without taking the design into account:\n\nRepresentation\nVariance Estimation\n\nIncorporation of the weights corrects for biased estimates (representation) and the stratification and clustering produces correct variance estimates.\nStata is one of the leaders in terms of statistical software that can perform these types of analyses, and offers a wide variety of commands that will perform design-based analyses of data arising from a sample with a complex design. The basic process consists of two steps (similar to mi), first using svyset to describe the complex survey design, secondly using the svy: prefix to perform analyses.\n## Definitions\nComplex survey design is a massive topic which there are entire departments devoted to (Program at Survey Methodology here at Michigan) and which we offer a separate full day workshop (Survey Design). A simple survey design takes a random sample from the population as a whole. There are various reasons why a simple random sample will not work.\n\nIt is often infeasible to do either because of time or cost.\nWith smaller sample sizes, it can be difficult to obtain enough individuals in a given subpopulation.\nFor some small subpopulations, it may be very difficult to even obtain any individuals in a simple random sample.\n\nA complex survey design allows researchers to consider these limitations and design a sampling pattern to overcome them. Three primary techniques are\n\nStratification. Rather than sample all individuals, instead target specific subpopulations and collect from them explicitly. For example, you may stratify by race and aim to collect 50 white, 50 black, 50 Hispanic, etc.\nClustering. Primarily a cost/time saving measure. Similar to stratification, but instead of sampling from all clusters, you take a random sample of clusters and then sample within them. A typical clustering variable is neighborhood or census tract or school.\nWeighting. If certain sets of characteristics are more or less common, or more or less desired, when randomly sampling individuals, we can down-weight those who we don’t want/are more common, and up-weight those we want/are less common.\n\nFor example, we might want to collect data on obesity in school children in Ann Arbor. Rather than randomly sampling across all schools, we cluster by schools and randomly select 3. Then at each of those schools, we stratify by race and take a random sample of all students of each race at each school, weighted by their weight to attempt to capture more overweight students.\nOne final term is primary sampling unit which is the first level at which we randomized. In this example, that would be schools.\n## Describing the survey\nThe general syntax is\nsvyset &lt;psu&gt; [pweight = &lt;weight&gt;], strata(&lt;strata&gt;)\nThe svyset command defines the variables identifying the complex design of the sample to Stata, and only needs to be submitted once in a given Stata session. The &lt;psu&gt; is a variable identifying the primary sampling unit (PSU) that an observation came from. The &lt;weight&gt; is a variable containing sampling weights. Finally, the &lt;strata&gt; is a variable identifying the sampling stratum that an observation came from.\nThe NHANES data we’ve been using in our examples is actually from a complex sample design, which we’ve been ignoring. Let’s incorporate the sampling into the analysis.\n. webuse nhanes2, clear\n\nThe three variables of interest in the data are finalwgt for the sampling weights, strata for the strata, and psu for the clusters.\n. describe finalwgt strata psu\n\nVariable      Storage   Display    Value\nfinalwgt        long    %9.0g                 Sampling weight (except lead)\nstrata          byte    %9.0g                 Stratum identifier\npsu             byte    %9.0g      psulbl     Primary sampling unit\n\nIt’s useful to know that to remove any existing survey design, you can run\n. svyset, clear\n\nLet’s set up the survey design now.\n. svyset psu [pweight = finalwgt], strata(strata)\n\nSampling weights: finalwgt\n             VCE: linearized\n     Single unit: missing\n        Strata 1: strata\n Sampling unit 1: psu\n           FPC 1: &lt;zero&gt;\n\nTo get information about the strata and cluster variables use the following command or menu:\n. svydescribe\n\nSurvey: Describing stage 1 sampling units\n\nSampling weights: finalwgt\n             VCE: linearized\n     Single unit: missing\n        Strata 1: strata\n Sampling unit 1: psu\n           FPC 1: &lt;zero&gt;\n\n                                    Number of obs per unit\n Stratum   # units     # obs       Min      Mean       Max\n----------------------------------------------------------\n       1         2       380       165     190.0       215\n       2         2       185        67      92.5       118\n       3         2       348       149     174.0       199\n       4         2       460       229     230.0       231\n       5         2       252       105     126.0       147\n       6         2       298       131     149.0       167\n       7         2       476       206     238.0       270\n       8         2       338       158     169.0       180\n       9         2       244       100     122.0       144\n      10         2       262       119     131.0       143\n      11         2       275       120     137.5       155\n      12         2       314       144     157.0       170\n      13         2       342       154     171.0       188\n      14         2       405       200     202.5       205\n      15         2       380       189     190.0       191\n      16         2       336       159     168.0       177\n      17         2       393       180     196.5       213\n      18         2       359       144     179.5       215\n      20         2       285       125     142.5       160\n      21         2       214       102     107.0       112\n      22         2       301       128     150.5       173\n      23         2       341       159     170.5       182\n      24         2       438       205     219.0       233\n      25         2       256       116     128.0       140\n      26         2       261       129     130.5       132\n      27         2       283       139     141.5       144\n      28         2       299       136     149.5       163\n      29         2       503       215     251.5       288\n      30         2       365       166     182.5       199\n      31         2       308       143     154.0       165\n      32         2       450       211     225.0       239\n----------------------------------------------------------\n      31        62    10,351        67     167.0       288\n\nOnce the survey is defined with svyset, most common commands can be prefaced by svy: to analyze the data with the sampling structure. The svy: tab command works exactly like the tabulate command, only taking the design of the sample into account when producing estimates and chi-square statistics.\n. svy: tab sex\n(running tabulate on estimation sample)\n\nNumber of strata = 31                            Number of obs   =      10,351\nNumber of PSUs   = 62                            Population size = 117,157,513\n                                                 Design df       =          31\n\n----------------------\n      Sex | proportion\n----------+-----------\n     Male |      .4794\n   Female |      .5206\n          | \n    Total |          1\n----------------------\nKey: proportion = Cell proportion\n\nNext, lets look at the mean weight by gender.\n. svy: mean weight, over(sex)\n(running mean on estimation sample)\n\nSurvey: Mean estimation\n\nNumber of strata = 31            Number of obs   =      10,351\nNumber of PSUs   = 62            Population size = 117,157,513\n                                 Design df       =          31\n\n--------------------------------------------------------------\n             |             Linearized\n             |       Mean   std. err.     [95% conf. interval]\n-------------+------------------------------------------------\nc.weight@sex |\n       Male  |   78.62789   .2097761      78.20004    79.05573\n     Female  |   65.70701    .266384      65.16372    66.25031\n--------------------------------------------------------------\n\nCompare this to the usual mean command, without the design information:\n. mean weight, over(sex)\n\nMean estimation                         Number of obs = 10,351\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\nc.weight@sex |\n       Male  |   77.98423   .1945289      77.60292    78.36555\n     Female  |   66.39418   .1998523      66.00243    66.78593\n--------------------------------------------------------------\n\nAnd compare the svy: results to the usual mean command, with only the weights considered:\n. mean weight [pweight=finalwgt], over(sex)\n\nMean estimation                         Number of obs = 10,351\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\nc.weight@sex |\n       Male  |   78.62789   .2272099      78.18251    79.07326\n     Female  |   65.70701   .2265547      65.26292     66.1511\n--------------------------------------------------------------\n\nWe see that the weights affect on the standard error, whereas the stratification and clustering also affects the estimates.\nMany of the usual commands such as regress or logit can be prefaced by svy:. If a command errors with the svy: prefix, a lot of the time the survey design will not affect it, and the documentation for the command will inform of that.\n## Subset analyses for complex sample survey data\nIn general, analysis of a particular subset of observations from a sample with a complex design should be handled very carefully. It is usually not appropriate to delete cases from the data-set that fall outside the sub-population of interest, or to use an if statement to filter them out. In Stata, sub-population analyses for this type of data are analyzed using a subpop indicator.\nSuppose we want to perform an analysis only for the cases where race is black in the NHANES data set. First, we must create an indicator variable that equals 1 for these cases.\n. gen race_black = race == 2\n\n. replace race_black = . if race == .\n(0 real changes made)\n\nNow we can run a simple regression model only on\n. svy, subpop(race_black): regress weight height i.sex\n(running regress on estimation sample)\n\nSurvey: Linear regression\n\nNumber of strata = 30                            Number of obs   =      10,013\nNumber of PSUs   = 60                            Population size = 113,415,086\n                                                 Subpop. no. obs =       1,086\n                                                 Subpop. size    =  11,189,236\n                                                 Design df       =          30\n                                                 F(2, 29)        =       50.12\n                                                 Prob &gt; F        =      0.0000\n                                                 R-squared       =      0.1131\n\n------------------------------------------------------------------------------\n             |             Linearized\n      weight | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |    .708568   .0728382     9.73   0.000     .5598126    .8573234\n             |\n         sex |\n     Female  |   3.508388   1.348297     2.60   0.014     .7547976    6.261979\n       _cons |  -46.10337   12.56441    -3.67   0.001    -71.76331   -20.44343\n------------------------------------------------------------------------------\nNote: 1 stratum omitted because it contains no subpopulation members.\n\nCompare the svy, subpop( ): results to the usual svy: regress command using an if statement:\n. svy: reg weight height i.sex if race_black == 1\n(running regress on estimation sample)\n\nSurvey: Linear regression\n\nNumber of strata = 30                             Number of obs   =      1,086\nNumber of PSUs   = 55                             Population size = 11,189,236\n                                                  Design df       =         25\n                                                  F(0, 25)        =          .\n                                                  Prob &gt; F        =          .\n                                                  R-squared       =     0.1131\n\n------------------------------------------------------------------------------\n             |             Linearized\n      weight | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |    .708568          .        .       .            .           .\n             |\n         sex |\n     Female  |   3.508388          .        .       .            .           .\n       _cons |  -46.10337          .        .       .            .           .\n------------------------------------------------------------------------------\nNote: Missing standard errors because of stratum with single sampling unit.\n\nThe point estimates and \\(^R^2^\\) are the same, but Stata refuses to even calculate standard errors."
  },
  {
    "objectID": "07-multiple-imputation.html#footnotes",
    "href": "07-multiple-imputation.html#footnotes",
    "title": "7  age | 142 12 12 | 154",
    "section": "",
    "text": "There is technically Little’s MCAR test to compare MCAR vs MAR, but the majority of imputation methods require only MAR, not MCAR, so it’s of limited use. Additionally, it is not yet supported in Stata.↩︎\nTechnically this happens as soon as you run mi set, but they’re not interesting until after mi impute.↩︎"
  },
  {
    "objectID": "solutions.html#one-sample-t-test",
    "href": "solutions.html#one-sample-t-test",
    "title": "8  lead Lead (mcg/dL)",
    "section": "8.1 One-sample t test",
    "text": "8.1 One-sample t test\n\n\n\n\n\n\n\nVariable\nObs Mean Std. err. Std. dev. [95% conf. interval]\n\n\n\n\nheight\n4,915 174.7421 .1026447 7.196115 174.5408 174.9433\n\n\n\n\n\n\n\n\n\nmean = mean(height) t = -12.2553 H0: mean = 176 Degrees of freedom = 4914\n\n\n\n\nGroup | Obs Mean Std. err. Std. dev. [95% conf. interval] ———+——————————————————————– Male | 4,915 47.4238 .2448869 17.1683 46.94372 47.90389 Female | 5,436 47.72057 .2340613 17.25716 47.26171 48.17942 ———+——————————————————————– Combined | 10,351 47.57965 .1692044 17.21483 47.24798 47.91133 ———+——————————————————————– diff | -.2967619 .338842 -.9609578 .3674339\n\n\n\ndiff = mean(Male) - mean(Female)                              t =  -0.8758\nH0: diff = 0 Degrees of freedom = 10349\nHa: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0\nPr(T &lt; t) = 0.1906 Pr(|T| &gt; |t|) = 0.3812 Pr(T &gt; t) = 0.8094\n\nWe fail to reject; there is no difference that the average age differs by gender.\n\n4)\n\n. tab race diabetes\n       |    Diabetes status\n  Race | Not diabe   Diabetic |     Total\n———–+———————-+———- White | 8,659 404 | 9,063 Black | 1,000 86 | 1,086 Other | 191 9 | 200 ———–+———————-+———- Total | 9,850 499 | 10,349\n\nGiven the different scales per race, it's hard to draw a comparison. We can look at the rowwise percents. (If you ran `tab diabetes race`, you'd need\nthe columnwise percents.)\n\n. tab race diabetes, row chi2\n\n\n\n\n\n\n8.2 Key\nfrequency row percentage\n\n\n\n       |    Diabetes status\n  Race | Not diabe   Diabetic |     Total\n———–+———————-+———- White | 8,659 404 | 9,063 | 95.54 4.46 | 100.00 ———–+———————-+———- Black | 1,000 86 | 1,086 | 92.08 7.92 | 100.00 ———–+———————-+———- Other | 191 9 | 200 | 95.50 4.50 | 100.00 ———–+———————-+———- Total | 9,850 499 | 10,349 | 95.18 4.82 | 100.00\n      Pearson chi2(2) =  25.3630   Pr = 0.000\n\nNearly double the percent of blacks have diabetes and the ^$^\\chi^2^$^ test confirms the difference is statistically significant.\n\n^#^^#^ Exercise 4\n\n. webuse nhanes2, clear\n. regress lead i.sex i.race c.age c.weight c.height i.region\n  Source |       SS           df       MS      Number of obs   =     4,948\n————-+———————————- F(9, 4938) = 129.98 Model | 36027.945 9 4003.105 Prob &gt; F = 0.0000 Residual | 152083.33 4,938 30.7985682 R-squared = 0.1915 ————-+———————————- Adj R-squared = 0.1901 Total | 188111.275 4,947 38.0253234 Root MSE = 5.5496\n\n\n\nlead | Coefficient Std. err. t P&gt;|t| [95% conf. interval]\n\n\n————-+—————————————————————-\n\n\nsex |\n\n\nFemale | -5.26415 .2259894 -23.29 0.000 -5.70719 -4.82111\n\n\n|\n\n\nrace |\n\n\nBlack | 2.937124 .2671182 11.00 0.000 2.413454 3.460794\n\n\nOther | -.5521668 .5617011 -0.98 0.326 -1.653351 .549017\n\n\n|\n\n\nage | .0196081 .0048918 4.01 0.000 .0100181 .0291981\n\n\nweight | -.0012435 .0059001 -0.21 0.833 -.0128103 .0103233\n\n\nheight | -.0275532 .0127149 -2.17 0.030 -.05248 -.0026264\n\n\n|\n\n\nregion |\n\n\nMW | -.1099025 .233614 -0.47 0.638 -.5678897 .3480848\n\n\nS | -1.858491 .2346487 -7.92 0.000 -2.318507 -1.398476\n\n\nW | -.2854048 .2380554 -1.20 0.231 -.7520992 .1812897\n\n\n|\n\n\n_cons | 21.16891 2.199034 9.63 0.000 16.85783 25.48\n\n\n\n\n1)\n\nThe F-test rejects so the model is informative. The ^$^R^2^$^ is low, so there is a lot of variability we're not capturing in this model.\n\n2)\n\n. rvfplot\n\n&lt;img src=\"Graph19.svg\" &gt;\n\nThis doesn't look great. We don't see any signs of nonnormality, but we do see a lot of very large positive residuals. If you look at a histogram for\n`lead`,\n\n. hist lead (bin=36, start=2, width=2.1666667)\n\n&lt;img src=\"Graph20.svg\" &gt;\n\nWe see right skew. The maintainers of this data noticed the same concern, as they include a `loglead` variable in the data to attempt to address this.\n\n. desc loglead\nVariable Storage Display Value name type format label Variable label ——————————————————————————- loglead float %9.0g log(lead)\n\nPerhaps we should have run the model with `loglead` as the output instead.\n\n3)\n\n. estat vif\nVariable |       VIF       1/VIF  \n————-+———————- 2.sex | 2.05 0.488262 race | 2 | 1.05 0.950220 3 | 1.06 0.941064 age | 1.13 0.885331 weight | 1.33 0.749151 height | 2.44 0.410091 region | 2 | 1.71 0.583352 3 | 1.77 0.566163 4 | 1.73 0.576881 ————-+———————- Mean VIF | 1.59\n\nNothing of concern here. The only moderately high VIF's are on sex and it's interaction, which does not concern us (of course a main effect and\ninteraction are collinear.).\n\n4)\n\nThe coffecient on \"Female\" is -5 and is statistically significant, so there is evidence that males have higher average lead levels.\n\n5)\n\nThe p-value is very small, so it is statistically significant. However, if we look at lead levels:\n\n. summ lead\nVariable |        Obs        Mean    Std. dev.       Min        Max\n————-+——————————————————— lead | 4,948 14.32033 6.166468 2 80\n\nWe see that lead levels range from 2 to 80. The coefficient on age is about .02, so age would need to increase by about 50 years to see a higher value\nfor the lead score. Unlikely to be clinically interesting! This is a side effect of the massive sample size.\n\n6)\n\n. margins region\nPredictive margins Number of obs = 4,948 Model VCE: OLS\nExpression: Linear prediction, predict()\n\n\n\n| Delta-method\n\n\n| Margin std. err. t P&gt;|t| [95% conf. interval]\n\n\n————-+—————————————————————-\n\n\nregion |\n\n\nNE | 14.93498 .176734 84.51 0.000 14.5885 15.28145\n\n\nMW | 14.82507 .1532375 96.75 0.000 14.52466 15.12549\n\n\nS | 13.07649 .1525888 85.70 0.000 12.77734 13.37563\n\n\nW | 14.64957 .1588465 92.22 0.000 14.33816 14.96098\n\n\n\n. margins region, pwcompare(pv)\nPairwise comparisons of predictive margins Number of obs = 4,948 Model VCE: OLS\nExpression: Linear prediction, predict()\n\n\n\n| Delta-method Unadjusted\n\n\n| Contrast std. err. t P&gt;|t|\n\n\n————-+—————————————\n\n\nregion |\n\n\nMW vs NE | -.1099025 .233614 -0.47 0.638\n\n\nS vs NE | -1.858491 .2346487 -7.92 0.000\n\n\nW vs NE | -.2854048 .2380554 -1.20 0.231\n\n\nS vs MW | -1.748589 .2161764 -8.09 0.000\n\n\nW vs MW | -.1755023 .2216647 -0.79 0.429\n\n\nW vs S | 1.573087 .2227974 7.06 0.000\n\n\n\n\nIt looks like South is significantly lower levels of lead than the other regions, which show no difference between them.\n\n7)\n\n. regress lead i.sex##c.age i.race c.weight c.height i.region\n  Source |       SS           df       MS      Number of obs   =     4,948\n————-+———————————- F(10, 4937) = 123.65 Model | 37676.3642 10 3767.63642 Prob &gt; F = 0.0000 Residual | 150434.91 4,937 30.4709156 R-squared = 0.2003 ————-+———————————- Adj R-squared = 0.1987 Total | 188111.275 4,947 38.0253234 Root MSE = 5.52\n\n\n\nlead | Coefficient Std. err. t P&gt;|t| [95% conf. interval]\n\n\n————-+—————————————————————-\n\n\nsex |\n\n\nFemale | -8.485856 .4923314 -17.24 0.000 -9.451044 -7.520667\n\n\nage | -.0150619 .0067745 -2.22 0.026 -.0283429 -.0017809\n\n\n|\n\n\nsex#c.age |\n\n\nFemale | .0676205 .0091936 7.36 0.000 .0495969 .0856441\n\n\n|\n\n\nrace |\n\n\nBlack | 2.985719 .2657756 11.23 0.000 2.464681 3.506758\n\n\nOther | -.5307013 .5587129 -0.95 0.342 -1.626027 .5646243\n\n\n|\n\n\nweight | -.0044452 .0058847 -0.76 0.450 -.0159819 .0070915\n\n\nheight | -.0266069 .0126477 -2.10 0.035 -.0514021 -.0018118\n\n\n|\n\n\nregion |\n\n\nMW | -.1417546 .2324084 -0.61 0.542 -.5973783 .3138691\n\n\nS | -1.897967 .2334589 -8.13 0.000 -2.35565 -1.440284\n\n\nW | -.2945939 .2367891 -1.24 0.214 -.7588057 .169618\n\n\n|\n\n\n_cons | 22.89998 2.199931 10.41 0.000 18.58714 27.21282\n\n\n\n. margins sex, dydx(age)\nAverage marginal effects Number of obs = 4,948 Model VCE: OLS\nExpression: Linear prediction, predict() dy/dx wrt: age\n\n\n\n| Delta-method\n\n\n| dy/dx std. err. t P&gt;|t| [95% conf. interval]\n\n\n————-+—————————————————————-\n\n\nage |\n\n\nsex |\n\n\nMale | -.0150619 .0067745 -2.22 0.026 -.0283429 -.0017809\n\n\nFemale | .0525586 .006614 7.95 0.000 .0395923 .0655249\n\n\n\n. quietly margins sex, at(age = (20 45 70))\n. marginsplot\nVariables that uniquely identify margins: age sex\n\n&lt;img src=\"Graph21.svg\" &gt;\n\nWe see significance in the interaction, so we looked at the margins. It looks like men show a slight decline in lead as age increases (again,\nrescaling, -.015/year becomes -1.5 over 100 years - not very interesting) while women show a much more significant increase as age increases (roughly\n1 unit every 20 years). The marginal plot helps us to visualize this. For men, from age 20 to 70, the average lead decreases barely half a point. For\nwomen, we see nearly a 3 point average increase.\n\n\n^#^^#^ Exercise 5\n\n. webuse nhanes2, clear\n. logit diabetes i.sex i.race c.age weight height i.region\nIteration 0: Log likelihood = -1999.7591\nIteration 1: Log likelihood = -1819.9899\nIteration 2: Log likelihood = -1777.7462\nIteration 3: Log likelihood = -1776.9939\nIteration 4: Log likelihood = -1776.9935\nIteration 5: Log likelihood = -1776.9935\nLogistic regression Number of obs = 10,349 LR chi2(9) = 445.53 Prob &gt; chi2 = 0.0000 Log likelihood = -1776.9935 Pseudo R2 = 0.1114\n\n\n\ndiabetes | Coefficient Std. err. z P&gt;|z| [95% conf. interval]\n\n\n————-+—————————————————————-\n\n\nsex |\n\n\nFemale | .0934803 .137068 0.68 0.495 -.175168 .3621286\n\n\n|\n\n\nrace |\n\n\nBlack | .5781259 .1326244 4.36 0.000 .3181869 .8380648\n\n\nOther | .408093 .3623376 1.13 0.260 -.3020757 1.118262\n\n\n|\n\n\nage | .058799 .0039646 14.83 0.000 .0510286 .0665695\n\n\nweight | .0268805 .0031111 8.64 0.000 .0207829 .0329782\n\n\nheight | -.0220928 .0076659 -2.88 0.004 -.0371175 -.007068\n\n\n|\n\n\nregion |\n\n\nMW | -.0265833 .1418506 -0.19 0.851 -.3046054 .2514388\n\n\nS | .0998071 .1375642 0.73 0.468 -.1698138 .3694281\n\n\nW | -.0984779 .1457444 -0.68 0.499 -.3841316 .1871758\n\n\n|\n\n\n_cons | -4.645124 1.3496 -3.44 0.001 -7.290292 -1.999956\n\n\n\n\n1)\n\n. estat gof\nGoodness-of-fit test after logistic model Variable: diabetes\n  Number of observations =   10,349\nNumber of covariate patterns = 10,349 Pearson chi2(10339) = 10025.54 Prob &gt; chi2 = 0.9860\n. estat gof, group(20) note: obs collapsed on 20 quantiles of estimated probabilities.\nGoodness-of-fit test after logistic model Variable: diabetes\nNumber of observations = 10,349 Number of groups = 20 Hosmer–Lemeshow chi2(18) = 15.92 Prob &gt; chi2 = 0.5983\n. lroc\nLogistic model for diabetes\nNumber of observations = 10349 Area under ROC curve = 0.7665\n\n&lt;img src=\"Graph22.svg\" &gt;\n\nWe cannot reject the model fit (even once we switch to the proper Hosmer-Lemeshow test, which used 20 instead of 10 because we have 10 predictors). The ROC and AUC look decent but not great.\n\n2)\n\n. margins race, pwcompare(pv)\nPairwise comparisons of predictive margins Number of obs = 10,349 Model VCE: OIM\nExpression: Pr(diabetes), predict()\n\n\n\n| Delta-method Unadjusted\n\n\n| Contrast std. err. z P&gt;|z|\n\n\n—————-+—————————————\n\n\nrace |\n\n\nBlack vs White | .0301076 .0081584 3.69 0.000\n\n\nOther vs White | .0197925 .0204751 0.97 0.334\n\n\nOther vs Black | -.0103152 .0220354 -0.47 0.640\n\n\n\n. margins region, pwcompare(pv)\nPairwise comparisons of predictive margins Number of obs = 10,349 Model VCE: OIM\nExpression: Pr(diabetes), predict()\n\n\n\n| Delta-method Unadjusted\n\n\n| Contrast std. err. z P&gt;|z|\n\n\n————-+—————————————\n\n\nregion |\n\n\nMW vs NE | -.0011484 .0061371 -0.19 0.852\n\n\nS vs NE | .0045414 .0062131 0.73 0.465\n\n\nW vs NE | -.0041309 .0061392 -0.67 0.501\n\n\nS vs MW | .0056899 .0056911 1.00 0.317\n\n\nW vs MW | -.0029825 .0056876 -0.52 0.600\n\n\nW vs S | -.0086724 .0057535 -1.51 0.132\n\n\n\n\nBlacks are more likely to have diabetes than whites or others. Age and weight are positive predictors whereas height is a negative predictor for some\nreason. There is no effect of gender or region.\n\n^#^^#^ Exercise 6\n\n. webuse chicken, clear\n. melogit complain grade i.race i.gender tenure age income /// &gt; nworkers i.genderm || restaurant:\nFitting fixed-effects model:\nIteration 0: Log likelihood = -1341.7541\nIteration 1: Log likelihood = -1337.7735\nIteration 2: Log likelihood = -1337.7659\nIteration 3: Log likelihood = -1337.7659\nRefining starting values:\nGrid node 0: Log likelihood = -1331.542\nFitting full model:\nIteration 0: Log likelihood = -1331.542\nIteration 1: Log likelihood = -1323.2469\nIteration 2: Log likelihood = -1321.7555\nIteration 3: Log likelihood = -1321.7325\nIteration 4: Log likelihood = -1321.7325\nMixed-effects logistic regression Number of obs = 2,763 Group variable: restaurant Number of groups = 500\n                                            Obs per group:\n                                                          min =          3\n                                                          avg =        5.5\n                                                          max =          8\nIntegration method: mvaghermite Integration pts. = 7\n                                            Wald chi2(9)      =     117.56"
  },
  {
    "objectID": "solutions.html#log-likelihood--1321.7325-prob-chi2-0.0000",
    "href": "solutions.html#log-likelihood--1321.7325-prob-chi2-0.0000",
    "title": "8  lead Lead (mcg/dL)",
    "section": "8.3 Log likelihood = -1321.7325 Prob > chi2 = 0.0000",
    "text": "8.3 Log likelihood = -1321.7325 Prob &gt; chi2 = 0.0000\ncomplain | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n————-+—————————————————————- grade | .0616416 .0463355 1.33 0.183 -.0291742 .1524574 | race | 2 | .5512152 .140432 3.93 0.000 .2759736 .8264568 3 | 1.180759 .137298 8.60 0.000 .9116599 1.449858 | 1.gender | .5866638 .1059402 5.54 0.000 .3790247 .7943029 tenure | -.0699963 .1727861 -0.41 0.685 -.4086509 .2686582 age | -.0748883 .0230589 -3.25 0.001 -.1200828 -.0296938 income | -.0034531 .0016933 -2.04 0.041 -.006772 -.0001343 nworkers | -.0303442 .0391781 -0.77 0.439 -.1071318 .0464435 1.genderm | .0795912 .1245047 0.64 0.523 -.1644336 .3236159 _cons | .3910687 1.103955 0.35 0.723 -1.772644 2.554781 ————-+—————————————————————- restaurant | var(_cons)| .5755458 .1417722 .3551455 .9327247 —————————————————————————— LR test vs. logistic model: chibar2(01) = 32.07 Prob &gt;= chibar2 = 0.0000\n\n1)\n\nWe can't look at fit statistics, but the ^$^\\chi^2^$^ is significant, so we're doing better than chance.\n\n2)\n\n. margins race, pwcompare(pv)\nPairwise comparisons of predictive margins Number of obs = 2,763 Model VCE: OIM\nExpression: Marginal predicted mean, predict()\n\n\n\n| Delta-method Unadjusted\n\n\n| Contrast std. err. z P&gt;|z|\n\n\n————-+—————————————\n\n\nrace |\n\n\n2 vs 1 | .0665377 .0166607 3.99 0.000\n\n\n3 vs 1 | .1685295 .0185599 9.08 0.000\n\n\n3 vs 2 | .1019918 .019303 5.28 0.000\n\n\n\n~~~~\nUnfortunately, this data is poorly labeled so we can’t talk in specifics about things, but generally\n\nRace 1, 2 and 3 have increasing odds of a complaint.\nGender 1 has significantly higher odds than gender 2.\nAge and income are negatively related to the odds of a complaint (older, more well paid employees are less likely to have complaints).\nNeither restaurant level characteristic is significant once server characteristics are accounted for.\n\n\n\n\nThe estimated random variance is non-zero, so yes, the random effects for restaurants are warranted."
  }
]