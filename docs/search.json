[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intermediate Stata (Statistical Modeling)",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#how-to-use-this-document",
    "href": "index.html#how-to-use-this-document",
    "title": "Intermediate Stata (Statistical Modeling)",
    "section": "How to use this document",
    "text": "How to use this document\nThese notes are published using Quarto. The Stata code is first rendered using a Stata dynamic document. The source code for these notes can be found at https://github.com/CSCAR/workshop-stata-intro for the curious.\nAll images should link to full-size versions to see detail if needed."
  },
  {
    "objectID": "index.html#cscar",
    "href": "index.html#cscar",
    "title": "Intermediate Stata (Statistical Modeling)",
    "section": "CSCAR",
    "text": "CSCAR\nhttp://cscar.research.umich.edu/\nCSCAR is available for free consultations with PhD statisticians (email deskpeople@umich.edu to request a consultation).\nCSCAR also has GSRAs available for more immediate help. Walk-ins to our office in Rackham are welcomed Monday-Friday 9am to 5pm (Closed Tuesdays 12-1pm). Alternatively, on our website, you can self-schedule into an hour consultation with the graduate students, which can be either remote or in-person (these are usually available same-day or next-day).\nCSCAR operates a email for help with statistical questions, feel free to send concise questions to stats-consulting@umich.edu.\nThe current contact for questions about the notes: Josh Errickson (jerrick@umich.edu)."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Intermediate Stata (Statistical Modeling)",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThese notes have evolved over the years thanks to many CSCAR statisticians, including Giselle Kolenic, Brady West, Heidi Reichert, and Lingling Zhang.\nThis material was created for use in workshops and short courses presented by faculty and staff from the Consulting for Statistics, Computing & Analytics Research (CSCAR) at the University of Michigan. No part of this material may be used for other purposes, copied, changed, or sold."
  },
  {
    "objectID": "01-summarizing-data.html#describe-summarize-codebook",
    "href": "01-summarizing-data.html#describe-summarize-codebook",
    "title": "1  Summarizing Data",
    "section": "1.1 describe, summarize, codebook",
    "text": "1.1 describe, summarize, codebook\nThe majority of the Stata modeling commands require all variables to be numeric. String variables can be used in some places, but there are plenty of times when you might expect them to work, but they don’t. As a result, I recommend converting all categorical variables into numeric. To help with this, the describe command can tell us what variables are string and which aren’t.\n. describe\n\nContains data from /Applications/Stata/ado/base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2022 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign\nHere we can see that “make” is a string; but make is unique per row (it identifies the make and model of each car) so it’s not something we’re going to use in the model. If you wanted to use string functions (see help string functions for details) to extract out the manufacturer of each car (e.g. there are 7 Buicks in the data), that resultant “manufacturer” variable would be something we’d need to convert to a numeric. The main tools you’d need would be destring (which converts numeric values saved as strings into numbers) and encode (which converts strings to numerics with appropriate value labels).\ndescribe is also useful to get a sense of the size of your data.\nOnce we’ve taken a look at the structure of the data, we can start exploring each variable. The summarize and codebook commands contains almost the same information, presented in slightly different ways. It can be useful to look at both. For example,\n. summ price, detail\n\n                            Price\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%         3291           3291\n 5%         3748           3299\n10%         3895           3667       Obs                  74\n25%         4195           3748       Sum of wgt.          74\n\n50%       5006.5                      Mean           6165.257\n                        Largest       Std. dev.      2949.496\n75%         6342          13466\n90%        11385          13594       Variance        8699526\n95%        13466          14500       Skewness       1.653434\n99%        15906          15906       Kurtosis       4.819188\n\n. codebook price\n\n-------------------------------------------------------------------------------\nprice                                                                     Price\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (int)\n\n                 Range: [3291,15906]                  Units: 1\n         Unique values: 74                        Missing .: 0/74\n\n                  Mean: 6165.26\n             Std. dev.:  2949.5\n\n           Percentiles:     10%       25%       50%       75%       90%\n                           3895      4195    5006.5      6342     11385\nThings to look for here include\n\nValues which are outside of expected values. The summarize commands gives the 1st and 99th percentiles (1% and 99% of values are below those thresholds, respectively) and codebook gives the range. If, for example, we saw a minimum value of -203 or a maximum value of 145200 (keep in mind these are 1978 dollars!), that’s an indication that there is an issue with the data, likely a mistake.\nThe mean is as expected. If this is higher or lower than expected, it might be an indication of skew or the existence of outliers. If it is very close to the minimum or maximum value, perhaps you have a point mass (e.g. if you polled 18-21 year old’s on their number of children, there would be a lot of 0’s but a few non-zeros).\nIf the standard deviation is very small (relative to the mean), then the variable has very consistent values. A standard deviation of 0 indicates a constant.\nThe codebook reports the number of missing; if you have missing data, double check that it is not an error in the data. Perhaps multiple imputation is needed.\nIf the variable is categorical (e.g. race), is the number of unique entries reported in the codebook as expected?"
  },
  {
    "objectID": "01-summarizing-data.html#mean",
    "href": "01-summarizing-data.html#mean",
    "title": "1  Summarizing Data",
    "section": "1.2 mean",
    "text": "1.2 mean\nThe mean command gives summary statistics on the mean of a variable.\n. mean price\n\nMean estimation                             Number of obs = 74\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n       price |   6165.257   342.8719      5481.914      6848.6\n--------------------------------------------------------------\nThese are characteristics of the estimated mean of the “price” variable. The standard deviation reported from the summarize command above represents the variability among individual cars; the standard error reported by mean the variability of means: if we were to repeatedly draw samples of size 74, the standard error is a measure of the variability of the means from all those samples.\nThe confidence interval is interpreted as if we were to continue drawing those samples of size 74, we would expect 95% of those samples to have an estimated mean within those bounds. It is not that we’re 95% confident that the true population mean falls in that range - either it does or it doesn’t!"
  },
  {
    "objectID": "01-summarizing-data.html#estimation-commands",
    "href": "01-summarizing-data.html#estimation-commands",
    "title": "1  Summarizing Data",
    "section": "1.3 Estimation Commands",
    "text": "1.3 Estimation Commands\nThe introduction of mean allows us to discuss estimation commands. An estimation command is any command that fits a statistical model - some of these are obvious such as regress for linear regression, but others such as mean which we just ran are also estimation commands because it is estimating a confidence interval. summarize is not because it only provides statistics about the current sample instead of making inference into the population.\nAlmost all estimation commands have the same general syntax:\ncommand varlist [if] [in] [weight] [,options]\nThe sections inside [ and ] are optional. The command can sometimes consist of a main command and one or more subcommands. The varlist can be empty, have a single entry, or have multiple entries (the order of which is sometimes of importance - generally the first is some outcome or dependent variable and the rest are predictors or independent variables).1\nEstimation commands are stored after they are run, and persist regardless of how many other non-estimation commands are run in between them. These non-estimation commands include data manipulation and postestimation commands. As soon as another estimation command is run, the first is dropped and the new one is saved.\nThis allows interesting things such as replaying a command (calling the estimation command again without any varlist to re-display it’s results) even if the data is gone!\n. clear\n\n. list\n\n. mean\n\nMean estimation                             Number of obs = 74\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n       price |   6165.257   342.8719      5481.914      6848.6\n--------------------------------------------------------------\nA larger benefit of this is that if you are fitting a model on one data set and want to get predicted values on another, you could do something like this (this is pseudo-code, not real Stata!):\nuse fitting_data\nmodel y x1 x2\nuse newdata, clear\npredict fitted\n\n1.3.1 Postestimation commands\nSince the last estimation command is saved, any commands which need to reference it (called postestimation commands) do so inherently, no need to specify. For example, let’s reload the data and run mean on a few variables.\n. sysuse auto, clear\n(1978 automobile data)\n\n. mean mpg headroom length\n\nMean estimation                             Number of obs = 74\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n         mpg |    21.2973   .6725511       19.9569    22.63769\n    headroom |   2.993243   .0983449      2.797242    3.189244\n      length |   187.9324   2.588409      182.7737    193.0911\n--------------------------------------------------------------\nLet’s say we want to obtain the correlation matrix2\n. estat vce, corr\n\nCorrelation matrix of coefficients of mean model\n\n        e(V) |      mpg  headroom    length \n-------------+-----------------------------\n         mpg |   1.0000                     \n    headroom |  -0.4138    1.0000           \n      length |  -0.7958    0.5163    1.0000 \nHere we see that both length and headroom are negatively correlated with mpg; as the car gets larger, its mileage decreases. Headroom and length are positively correlated, so cars aren’t just growing in one direction!\nThe estat command is somewhat generic, we will see other uses of it later.\nSimilar to how you can get help with any command with help, e.g. help mean, you can get a list of all postestimation commands that a given estimation command supports:\nhelp mean postestimation\nThere is also a link to the postestimation page in the help for the estimation command.\n\n\n1.3.2 Storing and restoring estimation commands\nThe obvious downside to Stata’s approach to saving the most recent estimation command is that you lose all earlier commands. If you have only a limited number of commands and each is fast, this isn’t a big deal. However, with some more advanced approaches, modeling can become very slow, so you may not want to lose the results. Stata has a solution for this, allowing us to store and recall estimation commands without having to re-run them. This has an obvious parallel to the preserve/restore commands that affect the data.\nYou have the choice of saving the results temporarily (in memory) or permanently (to a file). There are the obvious pro’s and con’s to each approach. For these notes I will focus primarily on storing the results in memory, but I will point out where the commands differ if saving to a file. Let’s run a fresh mean call to work with. The estimates command will be used.\n. mean price mpg\n\nMean estimation                             Number of obs = 74\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n       price |   6165.257   342.8719      5481.914      6848.6\n         mpg |    21.2973   .6725511       19.9569    22.63769\n--------------------------------------------------------------\n\n. estimates query\n(active results produced by mean; not yet stored)\nThe query subcommand tells us what estimation command was last run, and whether it has already been saved. Here it has not. Let’s save these results.\n. estimates store mean1\nTo save to a file, use estimates save instead. Now let’s run a second mean commands.\n. mean mpg headroom length\n\nMean estimation                             Number of obs = 74\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n         mpg |    21.2973   .6725511       19.9569    22.63769\n    headroom |   2.993243   .0983449      2.797242    3.189244\n      length |   187.9324   2.588409      182.7737    193.0911\n--------------------------------------------------------------\n\n. est store mean2\n\n. est query\n(active results produced by mean; also stored as mean2)\nNow query is telling us that the current estimation commands are (obviously) stored as “mean2”. Let’s use estimates restore to jump between the two. (If saving to a file, use estimates use instead.)\n. est restore mean1\n(results mean1 are active now)\n\n. estat vce, corr\n\nCorrelation matrix of coefficients of mean model\n\n        e(V) |    price       mpg \n-------------+-------------------\n       price |   1.0000           \n         mpg |  -0.4686    1.0000 \n\n. est query\n(active results produced by mean; also stored as mean1)\nTo “replay” an estimation command (re-display the results without re-running the model), you can either restore it and call the blank command again:\n. est restore mean2\n(results mean2 are active now)\n\n. mean\n\nMean estimation                             Number of obs = 74\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n         mpg |    21.2973   .6725511       19.9569    22.63769\n    headroom |   2.993243   .0983449      2.797242    3.189244\n      length |   187.9324   2.588409      182.7737    193.0911\n--------------------------------------------------------------\nor use estimates replay directly:\n. est query\n(active results produced by mean; also stored as mean2)\n\n. est replay mean1\n\n-------------------------------------------------------------------------------\nModel mean1\n-------------------------------------------------------------------------------\n\nMean estimation                             Number of obs = 74\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n       price |   6165.257   342.8719      5481.914      6848.6\n         mpg |    21.2973   .6725511       19.9569    22.63769\n--------------------------------------------------------------\nOne use of stored estimates that can be useful is creating a table to include all the results.\n. est table mean1 mean2\n\n----------------------------------------\n    Variable |   mean1        mean2     \n-------------+--------------------------\n       price |  6165.2568               \n         mpg |  21.297297    21.297297  \n    headroom |               2.9932432  \n      length |               187.93243  \n----------------------------------------\nIf you are familiar with regression, you should be able to see how useful this might be!\nFinally, we can see all saved estimates with dir, drop a specific estimation command with drop, or remove all with clear:\n. est dir\n\n--------------------------------------------------------------\n             |           Dependent  Number of        \n        Name | Command    variable     param.  Title \n-------------+------------------------------------------------\n       mean1 | mean           Mean          2  Mean estimation\n       mean2 | mean           Mean          3  Mean estimation\n--------------------------------------------------------------\n\n. est drop mean1\n\n. est dir\n\n--------------------------------------------------------------\n             |           Dependent  Number of        \n        Name | Command    variable     param.  Title \n-------------+------------------------------------------------\n       mean2 | mean           Mean          3  Mean estimation\n--------------------------------------------------------------\n\n. est clear\n\n. est dir"
  },
  {
    "objectID": "01-summarizing-data.html#tab",
    "href": "01-summarizing-data.html#tab",
    "title": "1  Summarizing Data",
    "section": "1.4 tab",
    "text": "1.4 tab\nContinuing on with exploring the data, categorical variables are not summarized well by the mean. Instead, we’ll look at a tabulation.\n. tabulate rep78\n\n     Repair |\nrecord 1978 |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          1 |          2        2.90        2.90\n          2 |          8       11.59       14.49\n          3 |         30       43.48       57.97\n          4 |         18       26.09       84.06\n          5 |         11       15.94      100.00\n------------+-----------------------------------\n      Total |         69      100.00\nThis gives us the count at each level, the percent at each level, as well as the cumulative percent (e.g. 57.97% of observations have a value of 3 or below). The cumulative percentage is only informative for an ordinal variable (a categorical variable that has an ordering too it), and not an unordered categorical variable such as race.\nNote that it is counting a total of 69 observations to total 100% of the data. However, you may have noticed earlier that we have 74 rows of data. By default, tabulate does not include any information about missing values. The missing option corrects that.\n. tab rep78, missing\n\n     Repair |\nrecord 1978 |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          1 |          2        2.70        2.70\n          2 |          8       10.81       13.51\n          3 |         30       40.54       54.05\n          4 |         18       24.32       78.38\n          5 |         11       14.86       93.24\n          . |          5        6.76      100.00\n------------+-----------------------------------\n      Total |         74      100.00\nIt’s important to keep in mind the difference between the percentages of the two outputs. For example, 11.59% of non-missing values of rep78 are 2, whereas only 10.81% of all values are 2.\nThere are a few other options related to how the results are visualized which we will not cover.\n\n1.4.1 Two-way tables\nWe will cover two-way tables (also known as “crosstabs”) later in univariate analysis, but there is a peculiarity to tabulate related to it. If you pass two variables to tabulate, it creates the crosstab:\n. tabulate rep78 foreign, missing\n\n    Repair |\n    record |      Car origin\n      1978 |  Domestic    Foreign |     Total\n-----------+----------------------+----------\n         1 |         2          0 |         2 \n         2 |         8          0 |         8 \n         3 |        27          3 |        30 \n         4 |         9          9 |        18 \n         5 |         2          9 |        11 \n         . |         4          1 |         5 \n-----------+----------------------+----------\n     Total |        52         22 |        74 \n\n\n1.4.2 Generating dummy variables\nAlthough Stata has excellent categorical variable handling capabilities, you may occasionally have the situation where you want the dummy variables instead of a category. For an example of the difference, consider a “campus” variable with three options, “central”, “north” and “medical”. Imagine our data looks like:\n\n\n\nid\ncampus\ncampuscentral\ncampusnorth\ncampusmedical\n\n\n\n\n1\nnorth\n0\n1\n0\n\n\n2\ncentral\n1\n0\n0\n\n\n3\nnorth\n0\n1\n0\n\n\n4\nnorth\n0\n1\n0\n\n\n5\nmedical\n0\n0\n1\n\n\n\nNotice that the information in campus and the information encoded in campuscentral, campusnorth, and campusmedical are identical. A 1 in the campus____ variables represents “True” and 0 represents “False”, and only a single 1 is allowed per row.\nAs mentioned, we will most of the time use categorical variables such as campus over dummy variables like campus_____ (these are used in the actual model, but Stata creates them for you behind the scenes so you don’t need to worry about them), but if necessary, you can create the dummy variables using tabulate:\n. list rep* in 1/5\n\n     +-------+\n     | rep78 |\n     |-------|\n  1. |     3 |\n  2. |     3 |\n  3. |     . |\n  4. |     3 |\n  5. |     4 |\n     +-------+\n\n. tabulate rep78, gen(reps)\n\n     Repair |\nrecord 1978 |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          1 |          2        2.90        2.90\n          2 |          8       11.59       14.49\n          3 |         30       43.48       57.97\n          4 |         18       26.09       84.06\n          5 |         11       15.94      100.00\n------------+-----------------------------------\n      Total |         69      100.00\n\n. list rep* in 1/5\n\n     +-----------------------------------------------+\n     | rep78   reps1   reps2   reps3   reps4   reps5 |\n     |-----------------------------------------------|\n  1. |     3       0       0       1       0       0 |\n  2. |     3       0       0       1       0       0 |\n  3. |     .       .       .       .       .       . |\n  4. |     3       0       0       1       0       0 |\n  5. |     4       0       0       0       1       0 |\n     +-----------------------------------------------+\nIf you are not familiar with the list command, it prints out data. Giving it a variable (or multiple) restricts it to those (here we restricted it to rep\\*, which is any variable that starts with “rep” - the * is a wildcard), and the in statement restricts to the first 5 observations (we just want a quick visualization, not to print everything).\nTake note of how the missing value is propogated when creating the dummies."
  },
  {
    "objectID": "01-summarizing-data.html#correlate",
    "href": "01-summarizing-data.html#correlate",
    "title": "1  Summarizing Data",
    "section": "1.5 correlate",
    "text": "1.5 correlate\nWith the use of tabulate for crosstabs, we’ve left univariate summaries and moved to joint summaries. For continuous variables, we can use the correlation to examine how similar two continuous variables are. The most common version of correlation is Pearson’s correlation, which ranges from -1 to 1.\n\nA value of 0 represents no correlation; having information about one variable provides no insight into the other.\nA value towards 1 represents positive correlation; as one value increases, the other tends to increase as well. A correlation of 1 would be perfect correlation; the two variables differ by only a transformation, e.g. your height in inches and centimeters.\nA value towards -1 represents negative correlation; as one value increases, the other tends to decreases (and vice-versa). A correlation of -1 would be perfect negative correlation, e.g. during a road trip, your distance travel is perfectly negatively correlated with your distance to you destination (ignoring detours).\n\nWe can calculate the Pearson’s correlation with correlate.\n. correlate weight length\n(obs=74)\n\n             |   weight   length\n-------------+------------------\n      weight |   1.0000\n      length |   0.9460   1.0000\n\nThis produces whats known as the correlation matrix. The diagonal entries are both 1, because clearly each variable is perfectly correlated with itself! The off-diagonal entries are identical since correlation is a symmetric operation. The value of .95 is extremely close to one, as we would expect - longer cars are heavier and perhaps vice-versa. Another way to think of it is that once we know weight, learning length does not add much information. On the other hand,\n. correlate price turn\n(obs=74)\n\n             |    price     turn\n-------------+------------------\n       price |   1.0000\n        turn |   0.3096   1.0000\n\nwith a correlation of .31, learning turn when you already know price does add a lot of information.\nWe can look at multiple correlations at once as well.\n. correlate mpg weight length\n(obs=74)\n\n             |      mpg   weight   length\n-------------+---------------------------\n         mpg |   1.0000\n      weight |  -0.8072   1.0000\n      length |  -0.7958   0.9460   1.0000\n\nWe see the .9460 we saw earlier, but notice also that mpg is negatively correlated with both weight and length - a larger car gets worse mileage and low mileage cars tend to be large. A few notes:\n\nThe amount of information contained is irrespective of the sign; knowing the mpg of a car, adding information about its weight doesn’t add much information.\nThe two correlations with mpg are extremely similar. We might generally expect that, given that weight and length are so strongly correlated. Note that despite that we expect that, it is not a rule - it is entirely possible (though unlikely) that the correlations with mpg could be very dissimilar.\n\nWhat are thresholds for a “low” or “high” correlation? This will depend greatly on your field and setting, a common metric is that .3 is a low correlation, .6 is a moderate correlation, and .8 is a strong correlation.\nIt is possible to obtain p-values testing whether each of those correlations are significantly distinguishable from 0 by passing the sig option. However, p-values for correlations are extremely dependent on sample size and should not be trusted.3 The magnitude of the correlation is much more important than it’s significance.\n\n1.5.1 varlists in Stata\nConsider if we wanted to look at all the continuous variables in the data. We could write corr price mpg ... and make a very long command. The collection of all variables would be a “varlist”. Stata has several ways of short cutting this.\nThe first we’ve already seen when we used the wildcard “*” above. We can use * anywhere in the variable name to denote any number of additional characters. E.g. “this*var” matches “thisvar”, “thisnewvar”, “this-var”, “thisHFJHDJSHFKDHFKSHvar”, etc. A second wildcard, “?”, represents just a single variable, so “this*var” would match only “this-var” from that list, as well as “thisAvar”, “thisJvar”, etc.\nSecondly, we can match a subset of variables that are next to each other using “-”. All variable, starting with the one to the left of the - and ending with the one to the right of the - are included. For example,\n. desc, simple\nmake          headroom      turn          reps1         reps5\nprice         trunk         displacement  reps2\nmpg           weight        gear_ratio    reps3\nrep78         length        foreign       reps4\n\n. desc trunk-turn\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\nWe can combine those two, as well as specifying individual variables.\n. corr price-rep78 t* displacement\n(obs=69)\n\n             |    price      mpg    rep78    trunk     turn displa~t\n-------------+------------------------------------------------------\n       price |   1.0000\n         mpg |  -0.4559   1.0000\n       rep78 |   0.0066   0.4023   1.0000\n       trunk |   0.3232  -0.5798  -0.1572   1.0000\n        turn |   0.3302  -0.7355  -0.4961   0.6008   1.0000\ndisplacement |   0.5479  -0.7434  -0.4119   0.6287   0.8124   1.0000\n\nprice, mpg and rep78 are included as part of price-rep78, t\\* matches trunk and turn, and displacement is included by itself.\nFinally, there is the special variable list _all, which is shorthand for all variables (e.g. firstvar-lastvar). It is accepted in most but not all places that take in variables.\n. corr _all\n(make ignored because string variable)\n(obs=69)\n\n             |    price      mpg    rep78 headroom    trunk   weight   length\n-------------+---------------------------------------------------------------\n       price |   1.0000\n         mpg |  -0.4559   1.0000\n       rep78 |   0.0066   0.4023   1.0000\n    headroom |   0.1112  -0.3996  -0.1480   1.0000\n       trunk |   0.3232  -0.5798  -0.1572   0.6608   1.0000\n      weight |   0.5478  -0.8055  -0.4003   0.4795   0.6691   1.0000\n      length |   0.4425  -0.8037  -0.3606   0.5240   0.7326   0.9478   1.0000\n        turn |   0.3302  -0.7355  -0.4961   0.4347   0.6008   0.8610   0.8631\ndisplacement |   0.5479  -0.7434  -0.4119   0.4763   0.6287   0.9316   0.8621\n  gear_ratio |  -0.3802   0.6565   0.4103  -0.3790  -0.5107  -0.7906  -0.7232\n     foreign |  -0.0174   0.4538   0.5922  -0.3347  -0.4053  -0.6460  -0.6110\n       reps1 |  -0.0945  -0.0086  -0.4230  -0.2550  -0.2175   0.0149   0.0054\n       reps2 |  -0.0223  -0.1346  -0.5180   0.1603   0.0586   0.1480   0.1778\n       reps3 |   0.0859  -0.2796  -0.3622   0.1726   0.2724   0.2975   0.2218\n       reps4 |  -0.0153   0.0384   0.3592  -0.0195  -0.0589  -0.1223  -0.0909\n       reps5 |  -0.0351   0.4542   0.7065  -0.2337  -0.2498  -0.3925  -0.3492\n\n             |     turn displa~t gear_r~o  foreign    reps1    reps2    reps3\n-------------+---------------------------------------------------------------\n        turn |   1.0000\ndisplacement |   0.8124   1.0000\n  gear_ratio |  -0.7005  -0.8381   1.0000\n     foreign |  -0.6768  -0.6383   0.7266   1.0000\n       reps1 |   0.0471  -0.0131  -0.0355  -0.1143   1.0000\n       reps2 |   0.2939   0.1733  -0.2468  -0.2395  -0.0626   1.0000\n       reps3 |   0.2526   0.3038  -0.2449  -0.3895  -0.1515  -0.3176   1.0000\n       reps4 |  -0.1748  -0.1231   0.2287   0.2526  -0.1026  -0.2151  -0.5211\n       reps5 |  -0.4110  -0.4093   0.2894   0.4863  -0.0752  -0.1577  -0.3820\n\n             |    reps4    reps5\n-------------+------------------\n       reps4 |   1.0000\n       reps5 |  -0.2587   1.0000\n\nNotice that it automatically ignored the string variable make. Not all commands will work this well, so _all may occasionally fail unexpectedly.\n\n\n1.5.2 Pairwise completion vs complete case\nYou may have noticed that the correlate command reports the number of observations it used, for example, the first few correlations all used 74 observations, but the _all version used on 69. correlate uses what’s known as complete cases analysis - any observation missing any value used in the command is excluded. rep78 is missing 5 observations (run the misstable summarize command to see this).\nOn the other hand, pairwise completion only excludes missing values from the relevant comparisons. If a given correlation doesn’t involve rep78, it will use all the data. We can obtain this with pwcorr.\n. corr rep78 price trunk\n(obs=69)\n\n             |    rep78    price    trunk\n-------------+---------------------------\n       rep78 |   1.0000\n       price |   0.0066   1.0000\n       trunk |  -0.1572   0.3232   1.0000\n\n\n. pwcorr rep78 price trunk\n\n             |    rep78    price    trunk\n-------------+---------------------------\n       rep78 |   1.0000 \n       price |   0.0066   1.0000 \n       trunk |  -0.1572   0.3143   1.0000 \nNotice the two correlations involving rep78 are identical - the same set of observations are dropped in both. However, the correlation between price and trunk differs - in correlate, it is only using 69 observations, whereas in pwcorr it uses all 74.\nIt may seem that pwcorr is always superior (and, in isolation it is). However, most models such as regression only support complete cases analysis, so in those cases, if you are exploring your data, it does not make sense to do pairwise comparison. Ultimately, the choice remains up to you. If the results from correlate and pwcorr do differ drastically, that is a sign of something else going on!\n\n\n1.5.3 Spearman correlation\nOne limitation of Pearson’s correlation is that it is detecting linear relationships only. A famous example of this is Anscombe’s quartet:\n\nIn each pair, the Pearson correlation is an identical .8162! In the first, that’s what we want. In the second, the relationship is strong but non-linear. In the third, only one value is not perfectly correlated, so the Pearsons correlation is diminished. In the fourth, only the existence of the single outlier is driving the relationship.\nSpearman correlation is an alternative to Pearson correlation. It works by ranking each variable and then performing Pearson’s correlation. The command in Stata is spearman.\n. corr price trunk\n(obs=74)\n\n             |    price    trunk\n-------------+------------------\n       price |   1.0000\n       trunk |   0.3143   1.0000\n\n\n. spearman price trunk, matrix\n\nNumber of observations = 74\n\n             |    price    trunk\n-------------+------------------\n       price |   1.0000 \n       trunk |   0.3996   1.0000 \nThe matrix option forces output to mirror correlate, otherwise it produces a slightly different output when given only two variables. spearman uses complete cases; to use pairwise complete instead, pass the option pw:\n. spearman mpg-headroom, pw\n\nNumber of observations:\n                      min = 69\n                      avg = 71\n                      max = 74\n\n             |      mpg    rep78 headroom\n-------------+---------------------------\n         mpg |   1.0000 \n       rep78 |   0.3098   1.0000 \n    headroom |  -0.4866  -0.1583   1.0000 \nHow does Spearman’s correlation compare to Pearson’s for Anscombe’s quartet?\n\n\n\nComparison\nPearson\nSpearman\n\n\n\n\n\\(y_1, x_1\\)\n.8162\n.8182\n\n\n\\(y_2, x_2\\)\n.8162\n.6909\n\n\n\\(y_3, x_3\\)\n.8162\n.9909\n\n\n\\(y_4, x_4\\)\n.8162\n.5000\n\n\n\nThe second correlation diminishes, the third drastically increases, and the fourth decreases as well."
  },
  {
    "objectID": "01-summarizing-data.html#exercise-1",
    "href": "01-summarizing-data.html#exercise-1",
    "title": "1  Summarizing Data",
    "section": "1.6 Exercise 1",
    "text": "1.6 Exercise 1\nFor these exercises, we’ll be using data from NHANES, the National Health And Nutrition Examination Survey. The data is on Stata’s website, and you can load it via\nwebuse nhanes2, clear\n\nUse describe to get a sense of the data. How many observations? How many variables?\nUse tab, summarize, codebook, and/or mean to get an understanding of the some of variables that we’ll be using going forward:\n\nregion\nsex\ndiabetes\nlead\n\nDoes race have any missing data? Does diabetes? Does lead?\nWhat is more highly correlated? A person’s height and weight, or their diastolic and systolic blood pressure?"
  },
  {
    "objectID": "01-summarizing-data.html#footnotes",
    "href": "01-summarizing-data.html#footnotes",
    "title": "1  Summarizing Data",
    "section": "",
    "text": "We won’t cover in this class, but there are multiple-equation estimating commands which have syntax command (varlist) (varlist) ... (varlist) [if] [in] [weight] [,options]. ↩︎\nWe could just run correlate, but the postestimation commands following mean are fairly limited, so bare with me here. Postestimation commands following models are much more interesting!↩︎\nThis is true of most p-values - it’s often the case that large sample sizes can provide small p-values for scientifically insignificant effects. However, correlation is especially susceptible to this issue.↩︎"
  },
  {
    "objectID": "02-visualization.html#the-graph-command",
    "href": "02-visualization.html#the-graph-command",
    "title": "2  Visualization",
    "section": "2.1 The graph command",
    "text": "2.1 The graph command\nMost (though not all, see some other graphs below) graphs in Stata are created by the graph command. Generally the syntax is\ngraph &lt;type&gt; &lt;variable(s)&gt;, &lt;options&gt;\nThe “type” is the subcommand.\nFor example, to create a bar chart of price by rep78, we could run\n. graph bar price, over(rep78)\n\nFor further information, we could instead construct a boxplot.\n. graph box price, over(rep78)\n\nThere are a few other infrequently used graphs, see help graph for details.\nThere is a plot subcommand, twoway, which takes additional sub-subcommands, and supports a wide range of types.\ngraph twoway &lt;type&gt; &lt;variable(s)&gt;, &lt;options&gt;\ntwoway creates most of the scatterplot-esque plots. The “types” in twoway are subcommands different from the subcommands in non-twoway graph, it takes options such as scatter to create a scatterplot:\n. graph twoway scatter mpg weight\n\nNote: For graph twoway commands, the graph is optional. E.g., these commands are equivalent:\ngraph twoway scatter mpg weight\ntwoway scatter mpg weight\nThis is not true of commands like graph box.\nThe options in the graphing commands are quite extensive and enable tweaking of many different settings. Rather than a full catalog of the options, here’s an example:\n. twoway scatter mpg weight, msymbol(s) ///\n&gt;                            mcolor(blue) ///\n&gt;                            mfcolor(yellow) ///\n&gt;                            msize(3) ///\n&gt;                            title(\"Mileage vs Weight\") ///\n&gt;                            xtitle(\"Weight (in lbs)\") ///\n&gt;                            ytitle(\"Mileage\") ///\n&gt;                            ylabel(15 \"15\" 25 \"25\" 35 \"35\")\n\n. \n\nGraphs made using twoway have an additional benefit - it is easy to stack them. For example, twoway lfit creates a best-fit line between the points:\n. twoway lfit mpg weight\n\nThis isn’t really that useful. It would be much better to overlap those two - generate the scatter plot, then add the best fit line. We can easily do that by passing multiple plots to twoway:\n. twoway (scatter mpg weight) (lfit mpg weight)\n\nNote that the order of the plots matters - if you can tell, the best-fit line was drawn on top of the scatter plot points. If you reversed the order in the command (twoway (lfit mpg weight) (scatter mpg weight)), the line would be drawn first and the points on top of it.\nFinally, note that options can be passed to each individual plot:\n. twoway (scatter mpg weight, msymbol(t)) ///\n&gt;        (lfit mpg weight, lcolor(green))\n\nPutting these options “globally”, as twoway (...) (...), msymbol(to) would NOT work, as msymbol is an option specifically for twoway scatter (and a few others), not for the more general twoway. There are options that apply to the twoway command, see help twoway_options for details.\nThere is an alternate way to specify the overlaid plots. These two commands are equivalent:\ntwoway (scatter mpg weight) (lfit mpg weight)\ntwoway scatter mpg weight || lfit mpg weight\nWe prefer the former as it makes it’s cleaner to distinguish when you have multiple overlaid plots with their own options, but some authors may chose the latter."
  },
  {
    "objectID": "02-visualization.html#other-graphs",
    "href": "02-visualization.html#other-graphs",
    "title": "2  Visualization",
    "section": "2.2 Other graphs",
    "text": "2.2 Other graphs\nThere are a very large number of graphs which do not exist under the graph command. Most are very niche, but the most important general example is histogram, which has its own command.\n. histogram mpg\n(bin=8, start=12, width=3.625)\n\nYou can see a full list of the non-graph plots by looking at\nhelp graph other"
  },
  {
    "objectID": "02-visualization.html#plotting-by-group",
    "href": "02-visualization.html#plotting-by-group",
    "title": "2  Visualization",
    "section": "2.3 Plotting by group",
    "text": "2.3 Plotting by group\nAll graph commands accept a by(&lt;grouping var&gt;) option which will repeat the graphing command for each level of the grouping variable, and display all graphs on the same output. For example,\n. twoway (scatter mpg weight) (lfit mpg weight), by(foreign)\n\nIt often looks better to see the two plots overlaid on each other for a more direct comparison. To do this, rather than using by(...), we’ll instead add each overlay conditionally:\n. twoway (scatter mpg weight if foreign ==  0) ///\n&gt;          (scatter mpg weight if foreign == 1) ///\n&gt;          (lfit mpg weight if foreign ==  0) ///\n&gt;          (lfit mpg weight if foreign == 1)\n\nNotice that Stata automatically made each plot a separate color, but not in a logical fashion. Here’s a cleaned up version:\n. twoway (scatter mpg weight if foreign ==  0, mcolor(orange)) ///\n&gt;          (scatter mpg weight if foreign == 1, mcolor(green)) ///\n&gt;          (lfit mpg weight if foreign ==  0, lcolor(orange) lwidth(1.4)) ///\n&gt;          (lfit mpg weight if foreign == 1, lcolor(green) lwidth(1.4)), ///\n&gt;       legend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2)) ///\n&gt;       title(\"Mileage vs Weight\") xtitle(\"Weight (lbs)\") ///\n&gt;       ytitle(\"Mileage\")\n\n(Since its not entirely clear from the code, the order(1 2) argument inside legend serves two purposes - first, it “orders” the entries in the legend box, but secondly and more importantly, it does not contain 3 or 4. If you look at the previous plot, it had four entries in the legend for the two scatters plus two lfits. By excluding 3 and 4 from order [3 and 4 corresponding to the two lfits], their legend entries are ignored.)"
  },
  {
    "objectID": "02-visualization.html#getting-help-on-graphs",
    "href": "02-visualization.html#getting-help-on-graphs",
    "title": "2  Visualization",
    "section": "2.4 Getting help on Graphs",
    "text": "2.4 Getting help on Graphs\nThere are a ton of options in all these graphs. Rather than list them all, we instead direct you to some various help pages.\nFor general assistance, start with\nhelp graph\nEach individual type of graph has its own help page:\nhelp graph box\nhelp graph twoway\nhelp twoway scatter\nhelp histogram\nThere are various generalized options which are the same over the variety of plots. These can be found in the documentation of each individual graph, or you can access them directly:\n\n\n\n\n\n\n\nTopics\nHelp command\n\n\n\n\nHelp with titles, subtitles, notes, captions.\nhelp title_options\n\n\nAxis labels, tick marks, scaling, etc.\nhelp axis_options\n\n\nManipulating the legend\nhelp legend_options\n\n\nModifying points (e.g. scatter)\nhelp marker_options\n\n\nAdding labels to markers\nhelp marker_label_options\n\n\nOptions for any lines (e.g. lfit)\nhelp cline_options"
  },
  {
    "objectID": "02-visualization.html#displaying-multiple-graphs-simultaneously",
    "href": "02-visualization.html#displaying-multiple-graphs-simultaneously",
    "title": "2  Visualization",
    "section": "2.5 Displaying multiple graphs simultaneously",
    "text": "2.5 Displaying multiple graphs simultaneously\nYou may have noticed that opening a new plot closes the old one. What if you wanted to compare the plots? The behind-the-scenes reason that the old plots are closed is that Stata names each plot and each plot can only be open once. The default name is “Graph”, so with each new plot, the “Graph” plot is overridden. If you closed a plot and wanted to re-open it, you can run the following at any point until you run another graph just like with estimation commands.\ngraph display Graph\nWhen we create a new plot with the default name, we lose the last one.\nIf we give a plot a non-default name, it will be saved (so that it can be re-displayed later) and more importantly, will open a new window without closing the last. Running two plots with custom names opens two separate windows. (These are not run in the notes because obviously this won’t demonstrate well, but try them on your own.)\nhist price, name(g1)\nhist mpg, name(g2)\nNames can be re-used (and plots re-generated) easily:\nhist price, title(\"Histogram of Price\") name(g1, replace)\nWe can also list (using dir), re-display (using display), or drop graphs (using drop):\ngraph dir\ngraph display g1\ngraph drop g1\ngraph drop _all\nFinally, if you’d rather have all the graphs in one window with tabs instead of separate windows, use\nset autotabgraphs on\n(You can pass the permanently option to not have to do this every time you open Stata.) You still need to name graphs separately."
  },
  {
    "objectID": "02-visualization.html#exercise-2",
    "href": "02-visualization.html#exercise-2",
    "title": "2  Visualization",
    "section": "2.6 Exercise 2",
    "text": "2.6 Exercise 2\nReload the NHANES data if you haven’t:\nwebuse nhanes2, clear\nUsing twoway scatter and twoway lfit, create a scatter plot of diastolic and systolic blood pressure, by gender. Be sure to color the lines and points consistenly and to clean up the legend."
  },
  {
    "objectID": "03-univariate-analysis.html#one-sample-t-test",
    "href": "03-univariate-analysis.html#one-sample-t-test",
    "title": "3  Univariate (and some Bivariate) Analysis",
    "section": "3.1 One-sample t-test",
    "text": "3.1 One-sample t-test\nA one-sample t-test tests whether the mean of a variable is equal to some constant. It is not needed a lot of the time (if we hypothesize that the average test score is 70, and every students get above an 80, why would we need to test this?), but we introduce it here just as a basis of further methods.\nThere are several assumptions necessary for a one-sample t-test, most of which are trivial/not that important. The two relatively important ones are\n\nIndependence. Each value must come from an independent source. If you have repeated measures (e.g. two different measures for the same person), this is violated. See the section of mixed models for dealing with this sort of data.\nThe distribution of the mean is normal. Note that this assumption is not about the data itself. This assumption is valid if either the sample suggests that the data is normal (a bell-curve) or the sample size is large (above ~30)1. If this assumption does not hold, we generally still use the t-test, although there are tests called “non-parametric” tests which do not require this assumption. Not everyone is convinced they are necessary.\n\nThere is no way to test the independence assumption, you must determine this based upon your knowledge of the data.\nWe can try and determine whether the normal distribution is valid for the data, but note that failing to find this does not imply the normality of the mean assumption is violated. This can be checked with a histogram or a qq-plot.\nLet’s test whether the average mileage in the data is different from 20.\n. hist mpg, normal\n(bin=8, start=12, width=3.625)\n\nIn a histogram, we are looking for violations from the bell-shape of a normal curve. We added the normal curve to the plot by the normal option. We can see that while the data is not perfectly normal, it also is not too far off. We can look at a qq-plot for further details.\n. qnorm mpg\n\nWith a qq-plot, we are looking for the points to roughly fall in a straight line, alone the 45-degree line. Here we see some mild violations of those, especially at either end.\nPut together, this indicates that normality is not perfect, but its also not a terrible violation.\nHowever, we don’t really care! The sample size is large enough that that assumption is valid. We can therefore perform our test using the ttest command.\n. ttest mpg == 20\n\nOne-sample t test\n------------------------------------------------------------------------------\nVariable |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\n     mpg |      74     21.2973    .6725511    5.785503     19.9569    22.63769\n------------------------------------------------------------------------------\n    mean = mean(mpg)                                              t =   1.9289\nH0: mean = 20                                    Degrees of freedom =       73\n\n    Ha: mean &lt; 20               Ha: mean != 20                 Ha: mean &gt; 20\n Pr(T &lt; t) = 0.9712         Pr(|T| &gt; |t|) = 0.0576          Pr(T &gt; t) = 0.0288\nOur null hypothesis, the claim we are trying to see whether we have the evidence to reject, is that the average mpg in the population represented by this data is 20. We sometimes also just call 20, the value the null hypothesis is testing, the null hypothesis (e.g. “We are testing against the null hypothesis of 20.”).\nLooking at the analysis, we see that we get some summary statistics. The 95% confidence interval represents the range which, if we were to draw repeated samples of the same size (74) from the population of all cars, we would expect 95% of the estimated means to fall in that range.\nBelow that, we see some details of the test being run. The important detail is that the null Hypothesis (H0, called “H nought”) is what you meant for it to be.\nFinally, we get our p-values. There are three separate ones given for three different alternative hypotheses - testing whether the mean is less than 20, greater than 20, or simply not equal to 20. Each p-value represents the probability of observing a mean as extreme as the one we saw if 20 was the true population mean. In other words, we get the following three interpretations:\n\nThere is 97.12% chance that if the true mean were 20, we would observe a sample mean of 21.2973 or lower.\nThere is 2.88% chance that if the true mean were 20, we would observe a sample mean of 21.2973 or higher.\nThere is 5.76% chance that if the true mean were 20, we would observe a sample mean of 21.2973 or higher or a sample mean of 18.7027 (this is calculated as 20 - (21.2973 - 20), in other words, 1.2973 below 20).\n\nThe last interpretation (corresponding to Ha: mean != 20 from the output) is known as the two-sided p-value, and should be your default. The other two, known as one-sided p-values, can be used if a priori you decide that you’re only interested in one direction.\nThe typical threshold used is p = .05, so we would fail to reject the null hypothesis (since .0576 &gt; .05), although this is extremely close to significant!\n(Of course that first p-value & interpretation is unnecessary - the observed test statistic is greater than 20, so there’s no chance we’d be able to make the argument that the true mean is less than 20!)"
  },
  {
    "objectID": "03-univariate-analysis.html#two-sample-t-test",
    "href": "03-univariate-analysis.html#two-sample-t-test",
    "title": "3  Univariate (and some Bivariate) Analysis",
    "section": "3.2 Two-sample t-test",
    "text": "3.2 Two-sample t-test\nWhile the one-sided t-test isn’t used very often, the two-sample version is. There are two different variations of it.\n\n3.2.1 Independent\nThe independent two-sample t-test arises when you have two independent groups which have the same measurement, and you wish to test whether the mean of the two groups is equivalent. The same basic assumptions, independence (although here it’s both within group [each observation is independent from each other] and between groups [the two groups are independent, e.g. these are not repeated measures on the same person]) and normality of the mean.\nFor example in our data, we can test whether foreign and American cars have the same average mileage. We will again use the ttest command, though slightly differently.\n. ttest mpg, by(foreign)\n\nTwo-sample t test with equal variances\n------------------------------------------------------------------------------\n   Group |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\nDomestic |      52    19.82692     .657777    4.743297    18.50638    21.14747\n Foreign |      22    24.77273     1.40951    6.611187    21.84149    27.70396\n---------+--------------------------------------------------------------------\nCombined |      74     21.2973    .6725511    5.785503     19.9569    22.63769\n---------+--------------------------------------------------------------------\n    diff |           -4.945804    1.362162               -7.661225   -2.230384\n------------------------------------------------------------------------------\n    diff = mean(Domestic) - mean(Foreign)                         t =  -3.6308\nH0: diff = 0                                     Degrees of freedom =       72\n\n    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0\n Pr(T &lt; t) = 0.0003         Pr(|T| &gt; |t|) = 0.0005          Pr(T &gt; t) = 0.9997\nThe output is very similar to above. We get some descriptives by group, and combined, as well as a confidence interval for the difference. Notice that the null hypothesis (Ho) is simply 0 because we are testing whether the difference between the groups is statistically significant, regardless of the actual values of the means.\nThe p-values have the same interpretation, though in terms of the difference of means rather than just the means. More-so than even in the one-sided test, you should use the two-sided p-value, so our p-value is 0.0005. Just as a note, do not report this p-value as 0 if you round! Instead, report that the p-value is less than .01 or less than .001.\n“There is less than a .1% chance that, if the average mileage was equivalent in domestic and foreign cars, we would observe a difference in the average mileage of 4.945804 or more.” Note that the sign of the difference doesn’t matter here because we’re looking at the two-sided version; the sign tells us that foreign cars in the sample have higher average mileage than domestic cars.\nHowever, there is an additional assumption here that we neglected; namely that the variances of the two groups are equivalent. If you make this assumption, then the above analysis is sufficient. However, if you don’t make this assumption, you can instead run the two-sample t-test with unequal variances by adding the unequal option.\n. ttest mpg, by(foreign) unequal\n\nTwo-sample t test with unequal variances\n------------------------------------------------------------------------------\n   Group |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\nDomestic |      52    19.82692     .657777    4.743297    18.50638    21.14747\n Foreign |      22    24.77273     1.40951    6.611187    21.84149    27.70396\n---------+--------------------------------------------------------------------\nCombined |      74     21.2973    .6725511    5.785503     19.9569    22.63769\n---------+--------------------------------------------------------------------\n    diff |           -4.945804    1.555438               -8.120053   -1.771556\n------------------------------------------------------------------------------\n    diff = mean(Domestic) - mean(Foreign)                         t =  -3.1797\nH0: diff = 0                     Satterthwaite's degrees of freedom =  30.5463\n\n    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0\n Pr(T &lt; t) = 0.0017         Pr(|T| &gt; |t|) = 0.0034          Pr(T &gt; t) = 0.9983\nThe unequal test is slightly more conservative, but there has been some work (e.g. Delacre et al 2017, Ruxton 2006) showing that you should always use the unequal version.\nThe equal variance version (the first one we ran) is known as Student’s2 t-test, and the unequal variance version (with the unequal option) is known as Welch’s t-test.\nIf you want to test whether the variance between the two groups is equal, you can use sdtest in a similar fashion to ttest (sdtest mpg, by(foreign)).\n\n\n3.2.2 Paired\nWe noted in the assumptions above that we need the two groups to be independent. What if they aren’t? An example of paired data would include before-and-after measures or measures from two family members. In both cases, it is reasonable to assume the two measures from the same person or family are more similar than a measure from one person against a measure from another person.\nThe auto data set does not have a good example of paired data, so lets use the “bpwide” data instead.\n. sysuse bpwide, clear\n(Fictional blood-pressure data)\n\n. list in 1/5\n\n     +-----------------------------------------------+\n     | patient    sex   agegrp   bp_bef~e   bp_after |\n     |-----------------------------------------------|\n  1. |       1   Male    30-45        143        153 |\n  2. |       2   Male    30-45        163        170 |\n  3. |       3   Male    30-45        153        168 |\n  4. |       4   Male    30-45        153        142 |\n  5. |       5   Male    30-45        146        141 |\n     +-----------------------------------------------+\nWe have measures from different patients, including a before and after measure of the blood pressure. We again use ttest to perform the test:\n. ttest bp_after == bp_before\n\nPaired t test\n------------------------------------------------------------------------------\nVariable |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\nbp_after |     120    151.3583    1.294234    14.17762    148.7956     153.921\nbp_bef~e |     120      156.45    1.039746    11.38985    154.3912    158.5088\n---------+--------------------------------------------------------------------\n    diff |     120   -5.091667    1.525736     16.7136   -8.112776   -2.070557\n------------------------------------------------------------------------------\n     mean(diff) = mean(bp_after - bp_before)                      t =  -3.3372\n H0: mean(diff) = 0                              Degrees of freedom =      119\n\n Ha: mean(diff) &lt; 0           Ha: mean(diff) != 0           Ha: mean(diff) &gt; 0\n Pr(T &lt; t) = 0.0006         Pr(|T| &gt; |t|) = 0.0011          Pr(T &gt; t) = 0.9994\nWe see that the blood pressure after is slightly lower (151 vs 156), and the two-sided p-value is statistically significant!\nWhile we technically used the paired t-test for this, behind the scenes, this is identical to generating a difference variable (e.g. post score - pre score) and performing the one-sample t-test:\n. gen bp_change = bp_after - bp_before\n\n. ttest bp_change == 0\n\nOne-sample t test\n------------------------------------------------------------------------------\nVariable |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\nbp_cha~e |     120   -5.091667    1.525736     16.7136   -8.112776   -2.070557\n------------------------------------------------------------------------------\n    mean = mean(bp_change)                                        t =  -3.3372\nH0: mean = 0                                     Degrees of freedom =      119\n\n    Ha: mean &lt; 0                 Ha: mean != 0                 Ha: mean &gt; 0\n Pr(T &lt; t) = 0.0006         Pr(|T| &gt; |t|) = 0.0011          Pr(T &gt; t) = 0.9994\n\n\n3.2.3 Comparing Proportions\nIf you have a ordinal variable (a categorical variable that has an ordering to it), a t-test is often still appropriate3. However, for binary variables, we should not use a regular t-test. Instead we use a Z-test4.\nThe command in stata is prtest and it functions identically to ttest:\nOne-sample:\n. webuse pneumoniacrt, clear\n(Bacterial pneumonia episodes data from CRT (Hayes and Moulton 2009))\n\n. prtest pneumonia = .25\n\nOne-sample test of proportion                   Number of obs      =       449\n\n------------------------------------------------------------------------------\n    Variable |       Mean   Std. err.                     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   pneumonia |   .1959911   .0187338                      .1592736    .2327086\n------------------------------------------------------------------------------\n    p = proportion(pneumonia)                                     z =  -2.6429\nH0: p = 0.25\n\n    Ha: p &lt; 0.25                 Ha: p != 0.25                 Ha: p &gt; 0.25\n Pr(Z &lt; z) = 0.0041         Pr(|Z| &gt; |z|) = 0.0082          Pr(Z &gt; z) = 0.9959\nIndependent two-sample:\n. prtest pneumonia, by(vaccine)\n\nTwo-sample test of proportions                  MnCC: Number of obs =      238\n                                              PnCRM7: Number of obs =      211\n------------------------------------------------------------------------------\n       Group |       Mean   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        MnCC |   .2226891   .0269686                      .1698316    .2755466\n      PnCRM7 |   .1658768   .0256075                      .1156871    .2160665\n-------------+----------------------------------------------------------------\n        diff |   .0568123   .0371894                     -.0160775    .1297021\n             |  under H0:   .0375355     1.51   0.130\n------------------------------------------------------------------------------\n        diff = prop(MnCC) - prop(PnCRM7)                          z =   1.5136\n    H0: diff = 0\n\n    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0\n Pr(Z &lt; z) = 0.9349         Pr(|Z| &gt; |z|) = 0.1301          Pr(Z &gt; z) = 0.0651\nPaired two-sample\n. sysuse bpwide, clear\n(Fictional blood-pressure data)\n\n. gen bp_beforehigh = bp_before &gt; 160\n\n. gen bp_afterhigh = bp_after &gt; 160\n\n. list *high in 1/5\n\n     +---------------------+\n     | bp_bef~h   bp_aft~h |\n     |---------------------|\n  1. |        0          0 |\n  2. |        1          1 |\n  3. |        0          1 |\n  4. |        0          0 |\n  5. |        0          0 |\n     +---------------------+\n\n. prtest bp_afterhigh = bp_beforehigh\n\nTwo-sample test of proportions          bp_afterhigh: Number of obs =      120\n                                        bp_beforehig: Number of obs =      120\n------------------------------------------------------------------------------\n    Variable |       Mean   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nbp_afterhigh |   .2583333    .039958                       .180017    .3366496\nbp_beforehig |        .35   .0435412                      .2646609    .4353391\n-------------+----------------------------------------------------------------\n        diff |  -.0916667   .0590972                      -.207495    .0241617\n             |  under H0:   .0593927    -1.54   0.123\n------------------------------------------------------------------------------\n        diff = prop(bp_afterhigh) - prop(bp_beforehig)            z =  -1.5434\n    H0: diff = 0\n\n    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0\n Pr(Z &lt; z) = 0.0614         Pr(|Z| &gt; |z|) = 0.1227          Pr(Z &gt; z) = 0.9386"
  },
  {
    "objectID": "03-univariate-analysis.html#chi-square-test",
    "href": "03-univariate-analysis.html#chi-square-test",
    "title": "3  Univariate (and some Bivariate) Analysis",
    "section": "3.3 Chi-square test",
    "text": "3.3 Chi-square test\nWe’ve seen how to handle continuous and binary variables, and even some ordinal variables. However, if we want to look at a categorical variable which is not binary and we cannot treat as continuous, we can instead use a \\(\\chi^2\\) test (\\(\\chi\\) is a Greek letter which is spelled “chi” in English, and rhymes with “why”). There are a few variations on the \\(\\chi^2\\) test, the version we talk of here is a test of association, where we are testing the null hypothesis that the distribution of one variable is the same at every level of another variable. Let’s return to the auto data set, and compare rep78 (Repair Record 1978) and foreign. We’ll start by looking at a crosstab.\n. sysuse auto, clear\n(1978 automobile data)\n\n. tab foreign rep78\n\n           |                   Repair record 1978\nCar origin |         1          2          3          4          5 |     Total\n-----------+-------------------------------------------------------+----------\n  Domestic |         2          8         27          9          2 |        48 \n   Foreign |         0          0          3          9          9 |        21 \n-----------+-------------------------------------------------------+----------\n     Total |         2          8         30         18         11 |        69 \nIf you’re not familiar with crosstabs, each cell represents the number of observations which fall into the category - for example, there are 8 cars which are both Domestic and have a Repair Record of 2.\nWe can think of testing whether the association between these two variables exists from either direction - we can either ask “Is the probability of a car having a specific repair record the same among domestic cars as among foreign cars”, or “Is the probability of a car being foreign or domestic the same across all levels of repair record”. We test it by adding the chi2 option to tab:\n. tab foreign rep78, chi2\n\n           |                   Repair record 1978\nCar origin |         1          2          3          4          5 |     Total\n-----------+-------------------------------------------------------+----------\n  Domestic |         2          8         27          9          2 |        48 \n   Foreign |         0          0          3          9          9 |        21 \n-----------+-------------------------------------------------------+----------\n     Total |         2          8         30         18         11 |        69 \n\n          Pearson chi2(4) =  27.2640   Pr = 0.000\nHere we can see that the p-value (pr) associated with the \\(\\chi^2\\) test is very low (less than .001, p-values are never 05!), so we can reject the null hypothesis and claim there is evidence of a statistically significant association between foreign and rep78.\nNote that the \\(\\chi^2\\) test ignores scale - if we had 4,800 Domestic cars instead of 48, and the percentage at each Repair Record were the same (e.g. instead of 8 Domestic cars with Repair Record of 2, we had 800), we would get the same test result.\nFinally, the critiques above about the lack of usefulness of t-tests extends to \\(\\chi^2\\) tests as well.\n\n3.3.1 Fisher’s Exact Test\nHowever, we probably should not have performed a \\(\\chi^2\\) test here. One important assumption of the \\(\\chi^2\\) test is that the expected count in each cell is at least 5. The expected count is the row total times the column total divided by the total number. For example, we looked at the 8 vehicles that were domestic which a Repair Record of 2. The row total is the number of domestic cars (48) and the column total is the number of cars with Repair Record of 2 (8), with a total of 69 cars overall (rep78 has 5 missing values). Therefore, the expected count is\n\\[\n    \\frac{48*8}{69} = 5.565\n\\]\nThis passes the assumption. To emphasize - this passed because the expected count is above 5. The fact that the observed count (8) is above 5 plays no role in this assumption.\nRather than manually calculating this for each entry, the expected option tells Stata to do it for us.\n. tab foreign rep78, expected\n\n+--------------------+\n| Key                |\n|--------------------|\n|     frequency      |\n| expected frequency |\n+--------------------+\n\n           |                   Repair record 1978\nCar origin |         1          2          3          4          5 |     Total\n-----------+-------------------------------------------------------+----------\n  Domestic |         2          8         27          9          2 |        48 \n           |       1.4        5.6       20.9       12.5        7.7 |      48.0 \n-----------+-------------------------------------------------------+----------\n   Foreign |         0          0          3          9          9 |        21 \n           |       0.6        2.4        9.1        5.5        3.3 |      21.0 \n-----------+-------------------------------------------------------+----------\n     Total |         2          8         30         18         11 |        69 \n           |       2.0        8.0       30.0       18.0       11.0 |      69.0 \nWe see that the assumption fails in a number of cells.\nThere are several possible solutions. One is to perform instead of a \\(\\chi^2\\) test, use a Fisher’s Exact test. Originally designed for 2x2 designs (two binary variables), it can be carried out for any arbitrarily large crosstab - in theory. In practice, the calculation is simply too demanding for large tables. This 2x5 table is fine, but for example, a 9x9 table would fail.\n. tab foreign rep78, exact\n\nEnumerating sample-space combinations:\nstage 5:  enumerations = 1\nstage 4:  enumerations = 3\nstage 3:  enumerations = 24\nstage 2:  enumerations = 203\nstage 1:  enumerations = 0\n\n           |                   Repair record 1978\nCar origin |         1          2          3          4          5 |     Total\n-----------+-------------------------------------------------------+----------\n  Domestic |         2          8         27          9          2 |        48 \n   Foreign |         0          0          3          9          9 |        21 \n-----------+-------------------------------------------------------+----------\n     Total |         2          8         30         18         11 |        69 \n\n           Fisher's exact =                 0.000\nWe obtain the same conclusion.\nIf your crosstab is too large for the exact test, you may need to consider rebinning or an entirely different analysis method such as some sort of regression such as logistic regression if one of the variables was binary, or multinomial or ordinal regression if neither are binary. (Multinomial and ordinal regression will not be covered in this workshop.)"
  },
  {
    "objectID": "03-univariate-analysis.html#exercise-3",
    "href": "03-univariate-analysis.html#exercise-3",
    "title": "3  Univariate (and some Bivariate) Analysis",
    "section": "3.4 Exercise 3",
    "text": "3.4 Exercise 3\nLoad up the NHANES data.\nsysuse nhanes2, clear\n\nWe don’t care about normality assumptions here. Why?\nThe average height of all men in America is 5’9” (176 cm). Does this sample appear representative of this fact? (E.g. is there evidence that the average in the sample differs from this?)\nWe would expect height and weight to differ by gender. Is there any evidence that age does?\nTake a look at a crosstab of race and diabetes. Does it look like the percentage of diabetics is the same across race? The col or row option might help here. Test this with a \\(\\chi^2\\) test."
  },
  {
    "objectID": "03-univariate-analysis.html#citations",
    "href": "03-univariate-analysis.html#citations",
    "title": "3  Univariate (and some Bivariate) Analysis",
    "section": "3.5 Citations",
    "text": "3.5 Citations\n\nDelacre, Marie, Daniël Lakens, and Christophe Leys. “Why Psychologists Should by Default Use Welch’s t-test Instead of Student’s t-test.” International Review of Social Psychology 30.1 (2017).\nRuxton, Graeme D. “The unequal variance t-test is an underused alternative to Student’s t-test and the Mann–Whitney U test.” Behavioral Ecology 17.4 (2006): 688-690."
  },
  {
    "objectID": "03-univariate-analysis.html#footnotes",
    "href": "03-univariate-analysis.html#footnotes",
    "title": "3  Univariate (and some Bivariate) Analysis",
    "section": "",
    "text": "This is by the central limit theorem.↩︎\nNamed not for students in a class, but the pseudonym for statistician William Sealy Gosset. Gosset worked at the Guinness brewery when he was performing some of his seminal work and wasn’t allowed to publish under his own name, so he used the pseudonym Student.↩︎\nThis requires the assumption that the ordinal variable can be well approximated by a continuous variable. This assumption is fine if each level of the ordinal variable is “equally” spaced (e.g. the amount of “effort” to go from level 1 to level 2 is the same as from level 4 to level 5).↩︎\nThe statistical reasoning is that when we’re dealing with proportions, the variance is determined directly by the mean. A t-test assumes we need to estimate the variance as well. A Z-test assumes we know the variance, which will be more efficient.↩︎\nUnless you’re in a pathological setting, i.e. you’re testing whether the average height of a population is less than 6 feet, and somehow you get an average height of -20 feet….↩︎"
  },
  {
    "objectID": "04-regression.html#terminology",
    "href": "04-regression.html#terminology",
    "title": "4  Regression",
    "section": "4.1 Terminology",
    "text": "4.1 Terminology\nWhen discussing any form of regression, we think of predicting the value of one variable1 based upon several other variables.\nThe variable we are predicting can be called the “outcome”, the “response” or the “dependent variable”.\nThe variables upon which we are predicting can be called “predictors”, “covariates”, or “independent variables”."
  },
  {
    "objectID": "04-regression.html#linear-regression",
    "href": "04-regression.html#linear-regression",
    "title": "4  Regression",
    "section": "4.2 Linear Regression",
    "text": "4.2 Linear Regression\nLinear regression (also known as Ordinary Least Squares (OLS) regression) is the most basic form of regression, where the response variable is continuous. Technically the response variable can also be binary or categorical but there are better regression models for those types of outcomes. Linear regression fits this model:\n\\[\n  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p + \\epsilon\n\\]\n\n\\(Y\\) represents the outcome variable.\n\\(X_1, X_2, \\cdots, X_p\\) represent the predictors, of which there are \\(p\\) total.\n\\(\\beta_0\\) represents the intercept. If you have a subject for which every predictor is equal to zero, \\(\\beta_0\\) represents their predicted outcome.\nThe other \\(\\beta\\)’s are called the coefficients, and represent the relationship between each predictor and the response. We will cover their interpretation in detail later.\n\\(\\epsilon\\) represents the error. Regression is a game of averages, but for any individual observation, the model will contain some error.\n\nLinear regression models can be used to predict expected values on the response variable given values on the predictors, and \\(\\epsilon\\) represents the difference between a prediction based on the model and what the actual value of the response variable is. Stata can be used to estimate the regression coefficients in a model like the one above, and perform statistical tests of the null hypothesis that the coefficients are equal to zero (and thus that predictor variables are not important in explaining the response). Note that the response \\(Y\\) is modeled as a linear combination of the predictors and their coefficients.\nSome introductory statistical classes distinguish between simple regression (with only a single predictor) and multiple regression (with more than one predictor). While this is useful for developing the theory of regression, simple regression is not commonly used for real analysis, as it ignores one of the main benefits of regression, controlling for other predictors (to be discussed later).\nWe will now fit a model, discussing assumptions afterwards, because almost all assumption checks can only occur once the model is fit!\n\n4.2.1 Fitting the model\nStata’s regress command fit the linear regression model. It is followed by a varlist where the first variable is the outcome variable and all others are predictors. For this example, let’s use the auto data and fit a relatively simple model, predicting mpg based on gear_ratio and headroom.\n. sysuse auto, clear\n(1978 automobile data)\n\n. regress mpg gear_ratio headroom\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(2, 71)        =     25.48\n       Model |   1021.0804         2  510.540202   Prob &gt; F        =    0.0000\n    Residual |  1422.37906        71  20.0335078   R-squared       =    0.4179\n-------------+----------------------------------   Adj R-squared   =    0.4015\n       Total |  2443.45946        73  33.4720474   Root MSE        =    4.4759\n\n------------------------------------------------------------------------------\n         mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n  gear_ratio |   6.801356   1.240026     5.48   0.000     4.328815    9.273898\n    headroom |  -1.443796   .6688077    -2.16   0.034     -2.77736   -.1102312\n       _cons |   5.113759   4.889846     1.05   0.299    -4.636317    14.86384\n------------------------------------------------------------------------------\nThere is a lot of important output here, so we will step through each piece.\nFirst, the top left table is the ANOVA table. If you were to fit a regression model with a single categorical predictor, this would be identical to running ANOVA via oneway. In general we don’t need to interpret anything here, as there are further measures of model fit in the regression frameworks.\nNext, the top right part has a series of measures.\n\nRegression performs complete case analysis - any observations missing any variable involved in this model is ignored in the model. (See multiple imputation for details on getting around this.) Check Number of obs to ensure the number of observations is what you expect. Here, the data has 74 rows, so the regression model is using all the data (there is no missingness in mpg, weight or displacement).\nThe F-test which follows (F(2, 71)2 and Prob &gt; F) is testing the null hypothesis that all coefficients are 0. In other words, if this test fails to reject, the conclusion is the model captures no relationships. In this case, do not continue interpreting the results; either your conclusion is that there is no relationship, or you need to return to the model design phase. If this test does reject, you can continue interpreting.\nThe \\(R^2\\) (R-squared) is a measure of model fit. It ranges from 0 to 1 and is a percentage, explaining what percent in the variation in the response is explained by the linear relationship with the predictors. What’s considered a “large” \\(R^2\\) depends greatly on your field and the situation, in very general terms, .6 is good and above .8 is great. However, if you know that there are a lot of unmeasured variables, a much lower threshold for a “good” \\(R^2\\) may be used.\nMathematically, adding a new predictor to the model will increase the \\(R^2\\), regardless of how useless the variable is.3 This makes \\(R^2\\) poor for model comparison, as it would always select the model with the most predictors. Instead, the adjusted \\(R^2\\) (“Adj R-Squared”) accounts for this; it penalizes the \\(R^2\\) by the number of predictors in the model. Use the \\(R^2\\) to measure model fit, use the adjusted \\(R^2\\) for model comparison.\nThe root mean squared error (Root MSE, as known as RMSE) is a measure of the average difference between the observed outcome and the predicted outcome. It can be used as another measure of model fit, as it is on the scale of the outcome variable. So for this example, the RMSE is 4.4759 so the average error in the model is about 4.5 mpg.\n\nFinally, we get to the coefficient table. Each row represents a single predictor. The _cons row is the intercept; it’s Coef. of 5.1138 represents the average response when all other predictors are 0. This is usually not interesting; how many cars weighing 0 lbs do you know of? So we’ll ignore this and consider only the other rows. The columns are:\n\nCoef.: These are the \\(\\beta\\) from the above model. We interpret each as “For a 1 increase in the value of the covariate with all other predictors held constant, we would predict this change in the response, on average.” For example, for every additional inch4 of headroom in a car (while its gear ratio is constant), it is predicted to have an average of 1.4438 lower mpg.\nStd. Err.: This represents the error attached to the coefficient. This is rarely interpreted; but if it gets extremely large or extremely small (and the Coef. doesn’t likewise go to extremes), its an indication there may be something wrong.\nt and P&gt;|t: These are testing the hypothesis that the coefficient are equal to zero. In this model, we see that both gear_ratio and headroom have significant p-values. The p-value is specifically measuring the probability of seeing a coefficient as extreme or more extreme than what we observed, if the true coefficient were 0.\n[95% Conf. interval]: A typically confidence interval for the Coefficient.\n\nWhenever we look at any model, a distinction needs to be drawn between statistical significance and practical significance. While these two interpretations of significance often align, they are not guaranteed to. We often have statistical significance (a p-value less than .05) when there is no practical significance (aka clinical significance, a difference that isn’t scientifically interesting). This is mostly a function of sample size; with a large sample even very small effects can have small p-values. Alternatively, a large practical significance with a low statistical significance can occur with very noisy data or a small sample size, which might indicate further study with a larger sample is needed.\n\n\n4.2.2 Including categorical predictors\nLet’s say we want to add rep78, a categorical variable with 5 levels, to the model. Regression can’t actually handle categorical variables; instead it creates dummy variables. A dummy variable is needed for all but one of the levels of the categorical, the excluded categorical variable is the baseline that all others are compared to5.\nNaively, let’s simply add the categorical rep78 to the model:\n. regress mpg gear_ratio headroom rep78\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(3, 65)        =     19.94\n       Model |  1121.49787         3  373.832624   Prob &gt; F        =    0.0000\n    Residual |  1218.70503        65  18.7493081   R-squared       =    0.4792\n-------------+----------------------------------   Adj R-squared   =    0.4552\n       Total |   2340.2029        68  34.4147485   Root MSE        =      4.33\n\n------------------------------------------------------------------------------\n         mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n  gear_ratio |   6.631208   1.330095     4.99   0.000     3.974825    9.287591\n    headroom |   -1.22008   .6651028    -1.83   0.071    -2.548382    .1082225\n       rep78 |   .9568854   .5816842     1.65   0.105    -.2048182    2.118589\n       _cons |   1.802318   4.849991     0.37   0.711    -7.883782    11.48842\n------------------------------------------------------------------------------\nWe only get a single coefficient. Stata is treating rep78 as continuous.\nThe issue is that Stata doesn’t know we want to treat rep78 as categorical6. If we prefix the variable name with i., Stata will know it is categorical.\n. regress mpg gear_ratio headroom i.rep78\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(6, 62)        =     11.50\n       Model |  1232.82371         6  205.470619   Prob &gt; F        =    0.0000\n    Residual |  1107.37918        62  17.8609546   R-squared       =    0.5268\n-------------+----------------------------------   Adj R-squared   =    0.4810\n       Total |   2340.2029        68  34.4147485   Root MSE        =    4.2262\n\n------------------------------------------------------------------------------\n         mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n  gear_ratio |   6.847879   1.308106     5.23   0.000     4.233012    9.462745\n    headroom |  -.9726907    .688578    -1.41   0.163    -2.349138    .4037571\n             |\n       rep78 |\n          2  |   1.203595   3.505518     0.34   0.733    -5.803835    8.211025\n          3  |   .0441395   3.232826     0.01   0.989    -6.418187    6.506466\n          4  |  -.0010255   3.309233    -0.00   1.000    -6.616088    6.614037\n          5  |   4.401329   3.363577     1.31   0.196    -2.322367    11.12502\n             |\n       _cons |   2.809121   5.272816     0.53   0.596    -7.731087    13.34933\n------------------------------------------------------------------------------\nFirst, note that headroom no longer has a significant coefficient! This implies that rep78 and headroom are correlated, and in the first model where we did not include rep78, all of rep78’s effect was coming through headroom. Once we control for rep78, headroom is no longer significant. We will discuss multicollinearity later, as well as why this is why model selection is bad.\nNow we see 4 rows for rep78, each corresponding to a comparison between response 1 and the row. For example, the first row, 2, is saying that when rep78 is 2 compared to when it is 1 (with gear_ratio and headroom held at some fixed level), the average predicted mpg drops by 1.204 (though it is not statistical significant). The last row, 5, is saying that when rep78 is 5 compare to when it is 1 (with gear_ratio and headroom held at some fixed level, the average predicted mpg increases by 4.401 (again, not statistically significant).\nWhat about the other comparison? (Does 2 differ from 4?) Let’s use the margins command. First, we’ll get the marginal means at each level - if every car in the data set had a rep78 level of \\(i\\), what is the average mileage?\n. margins rep78\n\nPredictive margins                                          Number of obs = 69\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       rep78 |\n          1  |   20.42972   3.123396     6.54   0.000     14.18614     26.6733\n          2  |   21.63332   1.548599    13.97   0.000     18.53771    24.72892\n          3  |   20.47386   .7900386    25.92   0.000      18.8946    22.05313\n          4  |    20.4287   1.021406    20.00   0.000     18.38694    22.47046\n          5  |   24.83105   1.341573    18.51   0.000     22.14929    27.51282\n------------------------------------------------------------------------------\nNote that the difference in the marginal means corresponds to the coefficients above. E.g. the marginal means at repair record 1 and 2 are 20.42972 and 21.63332 respectively, for a difference of 1.2036 - which is exactly (with rounding) the coefficient we saw on rep78 = 2 above!\nThe t-tests here are simply testing whether the marginal means are different than zero - of no real interest.\nNow, we can look at all pairwise comparisons by adding the pwcompare(pv) option to margins:\n. margins rep78, pwcompare(pv)\n\nPairwise comparisons of predictive margins                  Number of obs = 69\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\n\n-----------------------------------------------------\n             |            Delta-method    Unadjusted\n             |   Contrast   std. err.      t    P&gt;|t|\n-------------+---------------------------------------\n       rep78 |\n     2 vs 1  |   1.203595   3.505518     0.34   0.733\n     3 vs 1  |   .0441395   3.232826     0.01   0.989\n     4 vs 1  |  -.0010255   3.309233    -0.00   1.000\n     5 vs 1  |   4.401329   3.363577     1.31   0.196\n     3 vs 2  |  -1.159456   1.698355    -0.68   0.497\n     4 vs 2  |  -1.204621   1.896518    -0.64   0.528\n     5 vs 2  |   3.197733   2.129823     1.50   0.138\n     4 vs 3  |  -.0451649   1.315328    -0.03   0.973\n     5 vs 3  |   4.357189   1.601822     2.72   0.008\n     5 vs 4  |   4.402354   1.642697     2.68   0.009\n-----------------------------------------------------\nThis result is similar to a post-hoc test from ANOVA if you are familiar with it. The only statistical significance we find is 5 vs 3 and 5 vs 4, suggesting that 5 is dissimilar from 3 and 4. (Confusingly, 3 and 4 are not dissimilar from 1 or 2, but 5 is similar to 1 and 2! These sort of things can happen; its best to focus only on the comparisons that are of theoretical interest.)\nBy default, using i. makes the first level (lowest numerical value) as the reference category. You can adjust this by using ib#. instead, such as:\n. regress mpg headroom gear_ratio ib3.rep78\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(6, 62)        =     11.50\n       Model |  1232.82371         6  205.470619   Prob &gt; F        =    0.0000\n    Residual |  1107.37918        62  17.8609546   R-squared       =    0.5268\n-------------+----------------------------------   Adj R-squared   =    0.4810\n       Total |   2340.2029        68  34.4147485   Root MSE        =    4.2262\n\n------------------------------------------------------------------------------\n         mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    headroom |  -.9726907    .688578    -1.41   0.163    -2.349138    .4037571\n  gear_ratio |   6.847879   1.308106     5.23   0.000     4.233012    9.462745\n             |\n       rep78 |\n          1  |  -.0441395   3.232826    -0.01   0.989    -6.506466    6.418187\n          2  |   1.159456   1.698355     0.68   0.497    -2.235507    4.554419\n          4  |  -.0451649   1.315328    -0.03   0.973    -2.674469    2.584139\n          5  |   4.357189   1.601822     2.72   0.008     1.155193    7.559185\n             |\n       _cons |   2.853261   4.978251     0.57   0.569    -7.098121    12.80464\n------------------------------------------------------------------------------\nThis does not fit a different model. Both models (with i.rep78 and ib3.rep78) are identical, we’re just adjusting what is reported. If the models do change (especially the model fit numbers in the top right), something has gone wrong!\n\n\n4.2.3 Interactions\nEach coefficient we’ve look at so far is only testing whether there is a relationship between the predictor and response when the other predictors are held constant. What if we think the relationship changes based on the value of other predictors? For example, we might be interested in whether the relationship between a car’s headroom and its mileage depends on it’s gear ratio. Perhaps we think that cars with higher gear ratio (a high gear ratio is indicative of a sportier car) won’t be as affected by headroom as a stand-in for size, because sportier cars generally are better made.\nMathematically an interaction is nothing more than a literal multiplication. For example, if our model has only two predictors,\n\\[\n  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon\n\\]\nthen to add an interaction between \\(X_1\\) and \\(X_2\\), we simply add a new multiplicative term.\n\\[\n  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1\\times X_2) + \\epsilon\n\\]\n\n\\(\\beta_1\\) represents the relationship between \\(X_1\\) and \\(Y\\) when \\(X_2\\) is identically equal to 0.\n\\(\\beta_2\\) represents the relationship between \\(X_2\\) and \\(Y\\) when \\(X_1\\) is identically equal to 0.\n\\(\\beta_3\\) represents both:\n\nthe change in the relationship between \\(X_1\\) and \\(Y\\) as \\(X_2\\) changes.\nthe change in the relationship between \\(X_2\\) and \\(Y\\) as \\(X_1\\) changes.\n\n\nAdding these to the regress call is almost as easy. We’ll use # or ## instead. # includes only the interaction, whereas ## includes both the interaction and the main effects.\n\na#b: Only the interaction\na##b: Main effect for a, main effect for b, and the interaction.\na b a#b: Same as a##b\na b a##b: Same as a##b, except it’ll be messier because you’re including main effects twice and one will be dropped.\n\nLet’s change up the model a bit. Let’s predict mileage based upon price and gear ratio. We include mileage to address whether higher price cars have better mileage (better engineering). However, sportier cars (higher gear ratio) are generally expensive and may have worse mileage for the sake of performance. By including an interactino between price and gear ratio, we can test whether the relationship between price and mileage depends on the sportiness of the car.\n. regress mpg c.price##c.gear_ratio\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(3, 70)        =     27.15\n       Model |  1314.01011         3   438.00337   Prob &gt; F        =    0.0000\n    Residual |  1129.44935        70  16.1349907   R-squared       =    0.5378\n-------------+----------------------------------   Adj R-squared   =    0.5180\n       Total |  2443.45946        73  33.4720474   Root MSE        =    4.0168\n\n------------------------------------------------------------------------------\n         mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       price |    .002536   .0009513     2.67   0.010     .0006387    .0044333\n  gear_ratio |   14.30867   2.545822     5.62   0.000     9.231186    19.38615\n             |\n     c.price#|\nc.gear_ratio |   -.001145    .000342    -3.35   0.001    -.0018271   -.0004629\n             |\n       _cons |  -16.67042   7.509179    -2.22   0.030      -31.647   -1.693832\n------------------------------------------------------------------------------\nNote that we used c., similar to i.. c. forces Stata to treat it as continuous. Stata assumes anything in an interaction is categorical, so we need c. here! This can get pretty confusing, but it’s never wrong to include i. or c. when specifying a regression.\nLots of significance here, but once we include an interaction, the relationship between the variables included in the interaction and the response are not constant - the relationship depends on the value of the other interacted variables. This can be hard to visualize with the basic regression output, so we’ll look at margins again instead. We’ll want to look at the relationship between mpg and price at a few different values of gear_ratio to get a sense of the pattern. gear_ratio ranges from 2.19 to 3.89 (this can be obtained with summarize or codebook, just don’t forget to save the results or re-run the regress command to gain access to the postestimation commands again), so let’s look at the relationship at those extremes and at 3:\n. margins, dydx(price) at(gear_ratio = (2.25 2.75 3.25 3.75))\n\nAverage marginal effects                                    Number of obs = 74\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  price\n1._at: gear_ratio = 2.25\n2._at: gear_ratio = 2.75\n3._at: gear_ratio = 3.25\n4._at: gear_ratio = 3.75\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nprice        |\n         _at |\n          1  |  -.0000403   .0002367    -0.17   0.865    -.0005124    .0004318\n          2  |  -.0006128   .0001679    -3.65   0.001    -.0009477   -.0002779\n          3  |  -.0011853   .0002426    -4.89   0.000    -.0016691   -.0007015\n          4  |  -.0017578   .0003847    -4.57   0.000     -.002525   -.0009906\n------------------------------------------------------------------------------\nThis tells an interesting story. Notice the dy/dx column. All the values are negative and getting more negative. As the gear ratio increases, the relationship between price and mileage gets more negative. This effect is significant at all levels except with the lowest gear ratio. In the regression model above, the coefficient on price was positve and highly signifcant. How do we explain this seeming contradiction? Remember, the interpretation of the main effects in the presence of interactions is it’s the relationship between price and mileage when gear ratio is 0. Gear ratio is never 0! We are extrapolating.\nTo better understand the pattern we’re seeing, let’s visualize this:\n. marginsplot\n\nVariables that uniquely identify margins: gear_ratio\n\nWith low gear_ratio, there is no relationship between price and mileage - increasing the cost of a low gear ratio car is predicted to have no effect on milage, on average. As the gear ratio increases, the relationship becomes significant and negative - among sporty cars (higher gear ratio), the more expensive the car is the worse it’s mileage!\nWe can create a rather complicated looking plot to examine this.\n. twoway (scatter mpg price if gear_ratio &lt; 2.5, ///\n&gt;          msymbol(circle) mcolor(red)) ///\n&gt;        (lfit mpg price if gear_ratio &lt; 2.5, ///\n&gt;           lcolor(red)) ///\n&gt;        (scatter mpg price if gear_ratio &gt;= 2.5 & gear_ratio &lt; 3.0, ///\n&gt;           msymbol(triangle) mcolor(green)) ///\n&gt;        (lfit mpg price if gear_ratio &gt;= 2.5 & gear_ratio &lt; 3.0, ///\n&gt;           lcolor(green)) ///\n&gt;        (scatter mpg price if gear_ratio &gt;= 3.0 & gear_ratio &lt; 3.5, ///\n&gt;           msymbol(square) mcolor(blue)) ///\n&gt;        (lfit mpg price if gear_ratio &gt;= 3.0 & gear_ratio &lt; 3.5, ///\n&gt;           lcolor(blue)) ///\n&gt;        (scatter mpg price if gear_ratio &gt;= 3.5 & gear_ratio &lt; ., ///\n&gt;           msymbol(diamond) mcolor(purple))  ///\n&gt;        (lfit mpg price if gear_ratio &gt;= 3.5 & gear_ratio &lt; ., ///\n&gt;           lcolor(purple)) , ///\n&gt;                 legend(label(1 \"&lt; 2.5\") label(3 \"2.5 - 3.0\") ///\n&gt;                    label(5 \"3.0 - 3.5\") label(7 \"&gt; 3.5\") ///\n&gt;                    order(1 3 5 7)) ///\n&gt;             ytitle(\"mpg\")\n\nEach color represents a selection of cars with the given gear ratios. The cars with the lowest gear_ratio, in red, show no relationship between price and mileage. The green and purple cars with mid-range gear ratio, show negative relationships. The strongest negative relationship is among the purple cars with the highest gear ratio.\nThere are a few variations of the margins call we might want to use. First, we could reverse the roles of price and gear_ratio. We would do this if our hypothesis was that sportier cars decrease mileage, and the more expensive the car is, the stronger this effect.\nmargins, dydx(gear_ratio) at(price = (5000 10000 15000))\nNext, what if we have a categorical by continuous interaction? Instead of dydx, we’d simply give the categorical variable in the varlist:\nregress c.continuous##i.categorical\nmargins categorical, at(continuous = (1.5 5 8.5))\nmargins, dydx(continuous) at(categorical = (1 2 3))\nFinally, for categorical by categorical interactions, we get one additional variation:\nregress i.cat1##i.cat2\nmargins cat1#cat2\nOften discovering which variety of these you want may require some trial and error!\n\n4.2.3.1 Centering\nSome sources suggest centering continuous predictors before including them in an interaction. This can help slightly with interpretation (the main effects are the relationship when the other variable involved in the interaction are at their mean, rather than at zero) but doesn’t actually affect model fit.\nTo center, use the following:\n. summ gear_ratio\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n  gear_ratio |         74    3.014865    .4562871       2.19       3.89\n\n. gen gear_ratioc = gear_ratio - `r(mean)'\n\n. summ price\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n\n. gen pricec = price - `r(mean)'\n\n. summ gear_ratioc pricec\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n gear_ratioc |         74    2.92e-09    .4562871  -.8248648   .8751352\n      pricec |         74   -.0000154    2949.496  -2874.257   9740.743\n\n. regress mpg c.pricec##c.gear_ratioc\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(3, 70)        =     27.15\n       Model |  1314.01011         3  438.003371   Prob &gt; F        =    0.0000\n    Residual |  1129.44935        70  16.1349907   R-squared       =    0.5378\n-------------+----------------------------------   Adj R-squared   =    0.5180\n       Total |  2443.45946        73  33.4720474   Root MSE        =    4.0168\n\n------------------------------------------------------------------------------\n         mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      pricec |  -.0009161   .0001927    -4.75   0.000    -.0013005   -.0005317\n gear_ratioc |   7.249342   1.102393     6.58   0.000     5.050688    9.447996\n             |\n    c.pricec#|\n          c. |\n gear_ratioc |   -.001145    .000342    -3.35   0.001    -.0018271   -.0004629\n             |\n       _cons |   20.82048   .4881841    42.65   0.000     19.84683    21.79413\n------------------------------------------------------------------------------\nIf you compare fit characteristics and the interaction coefficient (and other coefficients), you’ll notice nothing has changed save the coefficients for pricec and gear_ratioc. Now the interpretation the price main effect (which before was nonsense because gear ratio was never 0) is -.0009161, which corresponds to the relationship between price and mileage when gear_ratio is 3.014865.\nIf we were to re-run the margins commands from before, we’d see the same results (albeit with different scaling both in the at( ) option and in the axes of the response.)\n\n\n\n4.2.4 Robust standard errors\nThe standard error associated with each coefficient are determined with the assumption that the model is “true” and that, were we given an infinite sample size, the estimates \\(\\hat{\\beta}\\) would converge to the true \\(\\beta\\). In many situations, this is clearly untrue.\nIf you believe this is untrue, the estimates will be unaffected, but their standard errors will be incorrect. We can adjust for this by using “robust” standard errors, also known as Sandwich estimators or Huber-White estimators, with the vce(robust) option to regress.\n. regress mpg c.price##c.gear_ratio, vce(robust)\n\nLinear regression                               Number of obs     =         74\n                                                F(3, 70)          =      25.72\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.5378\n                                                Root MSE          =     4.0168\n\n------------------------------------------------------------------------------\n             |               Robust\n         mpg | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       price |    .002536   .0008763     2.89   0.005     .0007883    .0042837\n  gear_ratio |   14.30867    2.55241     5.61   0.000     9.218048    19.39929\n             |\n     c.price#|\nc.gear_ratio |   -.001145   .0003043    -3.76   0.000    -.0017519   -.0005381\n             |\n       _cons |  -16.67042   7.455919    -2.24   0.029    -31.54078   -1.800056\n------------------------------------------------------------------------------\nNotice that compared to the previous model, the Coef estimates are the same but the standard errors (and corresponding t-statistic, p-value and confidence interval) are slightly different.\nTypically, the robust standard errors should be slightly larger than the non-robust standard errors, but not always. The only common situation where the robust standard errors will decrease is when the error variance is highest for observations near the average value of the predictors. This does not often happen (generally the higher residuals occur in observations that could be considered outliers).\nThere has been some argument that robust standard errors should always be used, because if the model is correctly specified, the robust standard errors and regular standard errors should be almost identical, so there is no harm in using them.\n\n\n4.2.5 Assumptions\nThere are three main assumptions when running a linear regression. Some we can test, some we cannot (and need to rely on our knowledge of the data).\n\n4.2.5.1 Relationship is linear and additive\nRecall the linear regression model:\n\\[\n  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p + \\epsilon\n\\]\nThis very explicitly assumes that the relationship is linear (as opposed to something non-linear, such as quadratic or exponential) and additive (as opposed to multiplicative). We can examine this assumption by looking at plots of the residuals (estimated errors):\n. rvfplot\n\nWhat we’re seeing here is a scatterplot between the fitted values (the predicted values for each individual) and their errors (the difference between the predicted values and observed values). If you can see a pattern in the scatterplot, that is evidence that this assumption is violated. Importantly, not seeing any pattern is not evidence that the assumption is valid! You’ll still need to cover this assumption with theory and knowledge of the data.\nThis image, from Julian Faraway’s Linear Models with R book, demonstrates a lack of pattern (the first) and a pattern (the third). (We will discuss the second plot below).\n\nIf this assumption is violated, you will need to reconsider the structure in your model, perhaps by adding a squared term (e.g. reg y c.x c.x#c.x).\n\n4.2.5.1.1 Obtaining predicted values and residuals\nIn the rvfplot, we plotted residuals versus predicted values - neither of which we have in the data. If there is some analysis beyond what rvfplot produces that you’re interested in, the predict command can obtain these. The general syntax for predict is:\npredict &lt;new var name&gt;, &lt;statistic&gt;\nThere are quite a few options for the “statistic”, but the two most commonly used ones are:\n\nxb: The linear prediction (also the default). This is the predicted value for each individual based on the model.\nresiduals: The residuals. The difference between the predicted value and observed value.\n\nIn other words, we can replicate the above rvfplot via:\n. predict linearpredictor, xb\n\n. predict resids, residuals\n\n. twoway scatter resids linearpredictor\n\nOf course we can clean this plot up with the typical options.\n\n\n\n4.2.5.2 Errors are homogeneous\n“Homogeneity” is a fancy term for “uniform in distribution”, whereas “heterogeneity” represents “not uniform in distribution”. If we were to take a truly random sample of all individuals in Michigan, the distribution of their heights would be homogeneous - it is reasonable to assume there is only a single distribution at work there. If on the other hand, we took a random sample of basketball players and school children, this would definitely be heterogeneous. The basketball players have a markedly difference distribution of heights that school children!\nIn linear regression, the homogeneity assumption is that the distribution of the errors is uniform. Violations would include errors changing as the predictor increased, or several groups having very different noise in their measurements.\nThis is an assumption we can examine, again with the residuals vs fitted plot. We’re looking for either a blatant deviation from a mean of 0, or an increasing/decreasing variability on the y-axis over time. Refer back to the image above, looking at the middle plot. As the fitted values increase, the error spreads out.\nIf this assumption is violated, you may consider restructuring your model as above, or transforming either your response or predictors using log transforms.\n\n\n4.2.5.3 Independence\nThis last assumption is that each row of your data is independent. If you have repeated measures, this is violated. If you have subjects drawn from groups (i.e. students in classrooms), this is violated. There is no way to test for this, it requires knowing the data set.\nIf this assumption is violated, consider fitting a mixed model instead.\n\n\n\n4.2.6 Miscellaneous concerns\n\n4.2.6.1 Multicollinearity\nMulticollinearity is an issue when 2 or more predictors are correlated. If only two are correlated, looking at their correlation (with pwcorr or correlate) may provide some indication, but you can have many-way multicollinearity where each pairwise correlation is low. You can use the variance inflation factor to try and identify if this is an issue.\nLet’s fit a model with a bunch of main effects. We use quietly to suppress the output to save space.\n. quietly regress mpg headroom trunk weight length turn displacement\n\n. estat vif\n\n    Variable |       VIF       1/VIF  \n-------------+----------------------\n      weight |     15.06    0.066412\n      length |     12.53    0.079833\ndisplacement |      5.17    0.193585\n        turn |      4.25    0.235511\n       trunk |      2.80    0.357428\n    headroom |      1.81    0.551207\n-------------+----------------------\n    Mean VIF |      6.93\nThe rule of thumb is VIF &gt; 10 or 1/VIF (called the tolerance) &lt; .1 suggests that the variable is involved in multicollinearity and more exploration may be needed.\nShockingly, we see that weight and length have high VIF’s - not surprisingly these are related!\nNote that generally, a high VIF on an interaction or a main effect in the presence of an interaction is not a concern (of course an interaction is collinear with its main effects!).\nMulticollinearity can be an issue because the more correlated predictors are, the more likely that their combined effect will be inappropriately spread among them. For a very simple example, imagine that we have the model\n\\[\n  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon\n\\]\nIf \\(X_1\\) and \\(X_2\\) are uncorrelated, then we can estimate \\(\\beta_1\\) and \\(\\beta_2\\) without a problem. Consider the extreme situations where \\(X_1\\) and \\(X_2\\) are perfectly correlated.7 We can therefore rewrite the equation as\n\\[\n  Y = \\beta_0 + (\\beta_1 + \\beta_2)X_1 + \\epsilon\n\\]\nsince with perfect correlation, \\(X_1\\) and \\(X_2\\) are identical.8 Now, when we fit the model, we would have estimates of \\(\\beta_1\\) and \\(\\beta_2\\) which sum to the “truth”, but the individual level of each of \\(\\beta_1\\) and \\(\\beta_2\\) could be anything. For example, if the “true” \\(\\beta_1\\) and \\(\\beta_2\\) are 1 and 3, they sum to 4. We could get estimated coefficients of 1 and 3, or 3 and 1, or -20 and 24!\nThis is an extreme example, but in practice we can be close to this situation.\n\n\n4.2.6.2 Overfitting\nOverfitting occurs when a model includes so many predictors that you can no longer generalize to the population. The rule of thumb is that you should have no more than one predictor for every 10-20 observations. The smaller your sample size, the more conservative you should be. For example, a sample size of 100 should use no more than 5-10 predictors. Recall that a categorical predictor with \\(k\\) different levels adds \\(k-1\\) predictors!\n\n\n4.2.6.3 Model Selection is bad\nThere is a literature on the idea of model selection, that is, an automated (or sometimes manual) way of testing many versions of a model with a different subset of the predictors in an attempt to find the model that fits best. These are sometimes called “stepwise” procedures.\nThis method has a number of flaws, including\n\nDoing this is basically “p-value mining”, that is, running a lot of tests till you find a p-value you like.\nYour likelihood of making a false positive is very high.\nAs we saw earlier, adding a new variable can have an effect on existing predictors.\n\nInstead of doing model selection, you should use your knowledge of the data to select a subset of the variables which are either a) of importance to you, b) theoretically influential on the outcome (e.g. demographic variables) or c) what others (reviewers) would think are influential on the outcome. Then you can fit a single model including all of this. The “subset” can be all predictors if the sample size is sufficient.\nNote that adjustments to fix assumptions (e.g. transformations) or multicollinearity would not fall into the category of model selection and are fine to use."
  },
  {
    "objectID": "04-regression.html#exercise-4",
    "href": "04-regression.html#exercise-4",
    "title": "4  Regression",
    "section": "4.3 Exercise 4",
    "text": "4.3 Exercise 4\nReload the NHANES data.\nwebuse nhanes2, clear\nFit a linear regression model predicting lead based upon sex, race, age, weight, height, and region. Make sure to handle categorical variables appropriately! Answer the following questions which may or may not require running additional commands.\n\nHow well does the model fit?\nDo any assumptions appear violated?\nDoes multicollinearity appear to be a concern?\nDoes one gender tend to have higher levels of lead? Does this appear practically interesting?\nIs the coefficient on age statistically significant? Do you think it is practically interesting?\nLooking at all the differences between regions, what conclusion can you draw?\nAdd an interaction between gender and age. What is the interpretation here?"
  },
  {
    "objectID": "04-regression.html#logistic-regression",
    "href": "04-regression.html#logistic-regression",
    "title": "4  Regression",
    "section": "4.4 Logistic Regression",
    "text": "4.4 Logistic Regression\nLet’s violate one of the assumptions. Instead of the relationship being linear, we can generalize to allow the relationship to be any functional form. These types of models are called “Generalized Linear Models” or “GLMs”, mainly because we can transform the model to be linear in some sense. Logistic regression is one specific form of a GLM, others include Poisson and Negative Binomial regression.\nLogistic regression is used when the outcome is dichotomous - either a positive outcome (1) or a negative outcome (0). For example, presence or absence of some disease. The equation for this model is\n\\[\n    P(Y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p)}} + \\epsilon.\n\\]\nThe right hand side is what’s known as the “logit” function, that is, logit(\\(z\\)) = \\(\\frac{1}{1 + e^{-z}}\\). Understanding this form isn’t crucial to understanding the model, but there are two quirks that need to be noted.\n\nThe relationship between the \\(X\\)’s and the outcome is nonlinear.\nNote that the left-hand side of this model is not just \\(Y\\), but rather, the probability of \\(Y\\) being 1 (a positive result) given the predictors. Unlike linear regression where we are explicitly predicting the outcome, a logistic model is instead trying to predict everyone’s probability of a positive outcome.\n\n\n4.4.1 Fitting the logistic model\nWe can fit this using the logit command in State. It works very similarly to regress. Let’s predict whether a car is foreign based on headroom and gear ratio again.\n. logit foreign gear_ratio headroom\n\nIteration 0:  Log likelihood =  -45.03321  \nIteration 1:  Log likelihood = -24.070574  \nIteration 2:  Log likelihood = -21.955086  \nIteration 3:  Log likelihood = -21.905247  \nIteration 4:  Log likelihood = -21.905069  \nIteration 5:  Log likelihood = -21.905069  \n\nLogistic regression                                     Number of obs =     74\n                                                        LR chi2(2)    =  46.26\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -21.905069                             Pseudo R2     = 0.5136\n\n------------------------------------------------------------------------------\n     foreign | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n  gear_ratio |   5.731199    1.32087     4.34   0.000     3.142342    8.320056\n    headroom |  -.2450151    .453866    -0.54   0.589    -1.134576    .6445459\n       _cons |   -18.2719   4.565065    -4.00   0.000    -27.21926   -9.324539\n------------------------------------------------------------------------------\nWhen you try this yourself, you may notice that its not quite as fast as regress. That is because for linear regression we have a “closed form solution” - we just do some quick math and reach an answer. However, almost every other type of regression lacks a closed form solution, so instead we solve it iteratively - Stata guesses at the best coefficients that minimize error9, and keeps guessing (using the knowledge of the previous guesses) until it stops getting significantly different results.\nFrom this output, we get the “Number of obs” again. Instead of an ANOVA table with a F-statistic to test model significance, there is instead a \\(\\chi^2\\) testing whether all coefficients are identically 0.\nWhen we move away from linear regression, we no longer get an \\(R^2\\) measure. There have been various pseudo-\\(R^2\\)’s suggested, and Stata reports one here, but be careful assigning too much meaning to it. It is not uncommon to get pseudo-\\(R^2\\) values that are negative or above 1. We’ll discuss measuring goodness of fit below.\nThe coefficients table is interpreted in almost the same way as with regression. We see that gear_ratio has a significant positive coefficient, and headroom’s coefficient is indistinguishable from 0. We cannot nicely interpret the coefficients. All we can say is that “As gear ratio increases, the probability of a car being foreign increases.”\nTo add any interpretability to these coefficients, we should instead look at the odds ratios (these coefficients are known as the log-odds). We can obtain this with the or options.\n. logit, or\n\nLogistic regression                                     Number of obs =     74\n                                                        LR chi2(2)    =  46.26\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -21.905069                             Pseudo R2     = 0.5136\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n  gear_ratio |   308.3386   407.2751     4.34   0.000     23.15803    4105.388\n    headroom |   .7826927   .3552376    -0.54   0.589     .3215584    1.905122\n       _cons |   1.16e-08   5.30e-08    -4.00   0.000     1.51e-12    .0000892\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\nNotice that the LR chi2(2), Pseudo R2, z and P&gt;|z| do not change - we’re fitting the same model! We’re just changing how the coefficients are represented, similar to changing the baseline of a categorical variable.\nOdds ratios null hypothesis is at 1, not at 0. Odds ratios are always positive. So a significant odds ratio will be away from 1, rather than away from 0 as in linear regression or the log odds. We can interpret odds ratios as percentage changes in the odds. The odds ratio for gear_ratio is 308.3386, suggesting that a 1 increase in the odds ratio leads to a 30,833% increase in the odds that the car is foreign! This is massive! But keep in mind, gear_ratio had a very narrow range - an increase of 1 is very large. Let’s rescale gear_ratio and try again.\n. gen gear_ratio2 = gear_ratio*10\n\n. logit foreign gear_ratio2 headroom, or\n\nIteration 0:  Log likelihood =  -45.03321  \nIteration 1:  Log likelihood = -24.070576  \nIteration 2:  Log likelihood = -21.955089  \nIteration 3:  Log likelihood = -21.905249  \nIteration 4:  Log likelihood = -21.905071  \nIteration 5:  Log likelihood = -21.905071  \n\nLogistic regression                                     Number of obs =     74\n                                                        LR chi2(2)    =  46.26\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -21.905071                             Pseudo R2     = 0.5136\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n gear_ratio2 |   1.773792   .2342948     4.34   0.000      1.36921    2.297922\n    headroom |   .7826928   .3552376    -0.54   0.589     .3215585    1.905122\n       _cons |   1.16e-08   5.30e-08    -4.00   0.000     1.51e-12    .0000892\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\nNote once again that the model fit characteristics haven’t changed; we’ve fit the same model, just with different units. Now the interpretation is more reasonable, for every .1 increase in gear_ratio (which corresponds to a 1 increase in gear_ratio2), we predict an increase in the odds of a car being foreign by 77.4%.\nThe odds ratio on headroom is not distinguishable from 1, however, if it were, the interpretation is that increasing the headroom by 1 inch is predicted to decrease the odds of a car being foreign by about 21.7% on average (1 - 0.783).\n\n\n4.4.2 Categorical Variables and Interactions\nBoth categorical variables and interactions can be included as they were in linear regression, with the appropriate interpretation of coefficients/odds ratios. The margins command also works the same.\nThe predict command adds a new statistic. xb now is the linear predictor, which is often not useful. Instead, the pr statistic returns the estimated probability of a positive outcome.\n\n\n4.4.3 Logistic regression assumptions\nThe assumptions for logistic regression are simpler than linear. The outcome measure must be binary with a 0 for a negative response and 1 for a positive. (Technically they don’t have to be positive/negative. We could think of predicting male/female and code male = 0 and female = 1. However, the convention would be to consider the outcome as “The person is female” so a 1 represents a “success” and a 0 a “failure” of that statement.) The errors in a logistic regression model are fairly contained (you can’t be wrong than more than 1!) so there are no real assumptions about them. The independence assumption is still here and still important, again, a mixed model (specifically a logistic mixed model) may be appropriate if the data is not independent.\n\n\n4.4.4 Logistic goodness of fit\nAs we mentioned earlier, there are various issues with the Pseudo \\(R^2\\) reported by logit, so use it carefully. There are two alternate approaches.\nThe first is to look at the ROC curve:\n. lroc\n\nLogistic model for foreign\n\nNumber of observations =       74\nArea under ROC curve   =   0.9384\n\nThe ROC curve is a bit complicated to explain, but basically over a range of thresholds, it classifies each observation as predicting a 0 or 1. E.g., with a threshold of .2, any observation whose predicted probability is below .2 is classified as a “0”, any observation whose predicted probability is above .2 is classified as a “1”. For each such threshold, the specificity (fraction of correctly classified “0”’s) versus the sensitivity(fraction of correctly classified “1”’s) are computed and plotted against each other. If the two fractions are equal, it’s a coin-flip and the model does no better than chance. On the other hand, the higher the ratio of sensitivity/(1 - specificity) is, the better classified we are.\nThe bottom left corner of the ROC curve is a threshold of 0 and the top right of 1, both of which would classify every observation the same, so they’re no good. We want to the ROC curve to be as close to “filling” the top area as possible. The Area under the ROC curve (called AUC) is a measure of fit; the fit here is .9384, which is very good. (The AUC ranges from .5 to 1. .5 indicates, as we’ve said, chance, and 1 indicates a perfect fit. An AUC of 1 is actually troublesome as we might be overfit.)\nThe second is a more formal test. There are two variants, a Pearson \\(\\chi^2\\) test and the Hosmer-Lemeshow test. Both are fit with the estat gof command. Both are testing the hypothesis that the observed positive responses match predicted positive responses in subgroups of the population. Therefore we do not want to reject these tests, and a large p-value is desired.\n. estat gof\n\nGoodness-of-fit test after logistic model\nVariable: foreign\n\n      Number of observations =     74\nNumber of covariate patterns =     58\n            Pearson chi2(55) =  42.92\n                 Prob &gt; chi2 = 0.8817\nWe see here a p-value of 0.882, failing to reject the null, so there is no evidence that the model does not fit well.\nThere is some concern that when the “number of covariate patterns” is close to the number of observations , the Pearson test is invalid. In this data, we see that 58 is indeed “close” to 74. Instead, we can use the Hosmer-Lemeshow by passing the group(#) option:\n. estat gof, group(10)\nnote: obs collapsed on 10 quantiles of estimated probabilities.\n\nGoodness-of-fit test after logistic model\nVariable: foreign\n\n Number of observations =     74\n       Number of groups =     10\nHosmer–Lemeshow chi2(8) =   9.89\n            Prob &gt; chi2 = 0.2727\nThe p-value remains insignificant at 0.273, still no evidence of poor model fit.\nWhy did we choose 10 groups? It’s just the standard. The only thing to keep in mind is that the Hosmer-Lemeshow test is only appropriate if the number of groups is greater than the number of predictors (including the intercept). In this model, we had two predictors, so that’s 3 total (including the intercept), so 10 is OK.\nThere are some limitations to Hosmer-Lemeshow, and there are more modern alternatives. However, Stata has not implemented any yet that I’m aware of.\n\n\n4.4.5 Separation\nSince the logistic regression model is solved iteratively, this can fail for a number of reasons. Before you begin interpreting the model, you’ll want to glance at the iteration steps and make sure that no errors were printed. The most common failure is due to separation.\nWith a binary outcome instead of a continuous outcome, it is much easier to have a predictor (or set of predictors) that perfectly predict the outcome. Consider trying to predict gender based on height. With a smaller sample, it’s not hard to imagine a scenario where every male is taller than every female. This is called “perfect separation”; using this sample, knowing height gives perfect information about gender\n“Partial separation” can also occur; this is when prediction is perfect only for one limit. Take the height scenario again; say everyone above 5’8” is male, and there are two men and all the women below 5’8”. Here, we will always predict Male for heights above 5’8”.\nSeparation (of either type) often produces coefficients to be extreme with large standard errors. Stata will sometimes warn about this, but not always. If you notice these exceptional coefficients or if Stata does warn about separation, you’ll need to investigate and consider excluding certain predictors. It may seem counter-intuitive to exclude extremely highly predictive variables, but if a variable produces perfect separation, you don’t need a model to inform you of that.\nYou can examine separation by looking at a table. Imagine we wanted to restructure rep78 as a binary variable, with low repairs (repair record below 3) and high repairs (repair records 3 and above).\n. gen repair_binary = rep78 &gt;= 3\n\n. replace repair_binary = . if rep78 &gt;= .\n(5 real changes made, 5 to missing)\n\n. label define repbinlabel 0 \"Low repairs\" 1 \"High repairs\"\n\n. label value repair_binary repbinlabel\n\n. tab repair_binary\n\nrepair_binar |\n           y |      Freq.     Percent        Cum.\n-------------+-----------------------------------\n Low repairs |         10       14.49       14.49\nHigh repairs |         59       85.51      100.00\n-------------+-----------------------------------\n       Total |         69      100.00\nLet’s try fitting the model:\n. logit foreign repair_binary\n\nnote: repair_binary != 1 predicts failure perfectly;\n      repair_binary omitted and 10 obs not used.\n\nIteration 0:  Log likelihood = -38.411464  \nIteration 1:  Log likelihood = -38.411464  \n\nLogistic regression                                     Number of obs =     59\n                                                        LR chi2(0)    =   0.00\n                                                        Prob &gt; chi2   =      .\nLog likelihood = -38.411464                             Pseudo R2     = 0.0000\n\n------------------------------------------------------------------------------\n     foreign | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nrepair_bin~y |          0  (omitted)\n       _cons |  -.5930637   .2719096    -2.18   0.029    -1.125997   -.0601307\n------------------------------------------------------------------------------\nNotice the note at the top of the model, and notice that nothing is estimated for repair_binary. We have partial separation:\n. tab foreign repair_binary\n\n           |     repair_binary\nCar origin | Low repai  High repa |     Total\n-----------+----------------------+----------\n  Domestic |        10         38 |        48 \n   Foreign |         0         21 |        21 \n-----------+----------------------+----------\n     Total |        10         59 |        69 \nSince we have a zero in one of the cells, that’s partial separation. Complete separation would be zero in both off-diagonal cells.\n\n\n4.4.6 logit Miscellaneous\nThe logit model supports the margins command just like regress does. It does not support estat vif because variance inflation factors are not defined for logistic models.\nCollinearity, overfitting, and model selection remain concerns in the logistic model.\nRobust standard errors via vce(robust) are supported.\n\n\n4.4.7 logit vs logistic\nInstead of logit, we could run the logistic command. The only difference is that logistic reports the odds ratio by default whereas logit reports the log odds. My personal preference is logit, but there’s no need to use one over the other."
  },
  {
    "objectID": "04-regression.html#other-regression-models",
    "href": "04-regression.html#other-regression-models",
    "title": "4  Regression",
    "section": "4.5 Other regression models",
    "text": "4.5 Other regression models\nThere are several other models which we will not cover, but function similarly to the above.\n\nPoisson regression is useful when you have count data; i.e. number of visitors or number of thunderstorms in a month. It can be fit with the poisson command, and results are interpreted similar to logistic regression (coefficients vs odds ratios); but instead of predicting a positive outcome, its predicting a larger count. If you have very large counts (such that a histogram appears to show a bell curve), linear regression can be used instead.\nPoisson has the strong assumption that the mean of the outcome is equal to its variance (small average counts have little noise; large average counts have a lot of noise). If this assumption is violated (or you want to check it), negative binomial regression also handles count data, without that assumptions, using nbreg. The output will include a test of “alpha=0”, if this fails to reject, then Poisson regression is sufficient.\nThere are two extensions to logistic regression, ordinal logistic and multinomial. Ordinal logistic is used when there are more than 2 outcome categories, and they are ordered (e.g. not sick (0), mildly sick (1), very sick (2)). Using ologit, Stata estimates an underlying continuous distribution and returns the “cut points”, allowing categorization. If there are multiple groups but not ordered, e.g. race, use mlogit for multinomial logistic regression. It essentially fits a model predicting membership in each group versus all other, with some restrictions across the models."
  },
  {
    "objectID": "04-regression.html#exercise-5",
    "href": "04-regression.html#exercise-5",
    "title": "4  Regression",
    "section": "4.6 Exercise 5",
    "text": "4.6 Exercise 5\nReload the NHANES data.\nwebuse nhanes2, clear\nThis time we’ll fit a logistic regression model, prediciting diabetes status on sex, race, age, weight, height, and region. As before, be sure to handle categorical variables appropriately.\n\nDoes the model fit well? Does the model classify well?\nWhat predicts higher odds of having diabetes?"
  },
  {
    "objectID": "04-regression.html#footnotes",
    "href": "04-regression.html#footnotes",
    "title": "4  Regression",
    "section": "",
    "text": "There are variations of regression with multiple outcomes, but they are for very specialized circumstances and can generally be fit as several basic regression models instead.↩︎\nThe 2 and 71 are degrees of freedom. They don’t typically add any interpretation.↩︎\nThe only exception is if the predictor being added is either constant or identical to another variable.↩︎\nThis is why it’s important to familiarize yourself with the units in your data!↩︎\nOnce we know the value of all but one of the dummies, the last is automatically determined so adds no new information.↩︎\nIt’s a bit confusing because if you look at the codebook for rep78, Stata correctly surmises that it is categorical. However, when it comes to modeling, Stata is not willing to make that assumption.↩︎\nIf you provide data with perfect correlation, Stata will drop one of them for you. This in only a thought exercise. If it helps, imagine their correlation is 99% instead of perfect, and add “almost” as a qualifier to most claims.↩︎\nTechnically there could be a scaling factors such that \\(X_1 = aX_2 + b\\), but let’s assume without loss of generality that \\(a=1\\) and \\(b=0\\).↩︎\nTechnically that maximizes likelihood, but that distinction is not important for understanding.↩︎"
  },
  {
    "objectID": "05-mixed-models.html#terminology",
    "href": "05-mixed-models.html#terminology",
    "title": "5  Mixed models",
    "section": "5.1 Terminology",
    "text": "5.1 Terminology\nThere are several different names for mixed models which you might encounter, that all fit essentially the same model:\n\nMixed model\nMixed Effects regression/model\nMultilevel regression/model\nHierarchical regression/model (specifically HLM, hierarchical linear model)\n\nThe hierarchical/multilevel variations require thinking about the levels of the data and involves “nesting”, where one variable only occurs within another, e.g. family members nested in a household. The most canonical example of this is students in classrooms, we could have\n\nLevel 1: The lowest level, the students.\nLevel 2: Classroom or teacher (this could also be two separate levels of classrooms inside teacher)\nLevel 3: District\nLevel 4: State\nLevel 5: Country\n\nThis is taking it a bit far; it’s rare to see more than 3 levels, but in theory, any number can exist.\nFor this workshop, we will only briefly discuss this from hierarchical point of view, preferring the mixed models view (with the reminder again that they are the same!).\n\n5.1.1 Econometric terminology\nTo make the terminology a bit more complicated, in econometrics, some of the terms we will use here are overloaded. When you are discussing mixed models with someone with econometric or economics training, it’s important to differentiate between the statistical terms of “fixed effects” and “random effects” which are the two components of a mixed model that we discuss below, and what econometricians called “fixed effects regression” and “random effects regression”.\nWithout going into the full details of the econometric world, what econometricians called “random effects regression” is essentially what statisticians called “mixed models”, what we’re talking about here. The Stata command xtreg handles those econometric models."
  },
  {
    "objectID": "05-mixed-models.html#wide-vs-long-data-time-varying-vs-time-invariant",
    "href": "05-mixed-models.html#wide-vs-long-data-time-varying-vs-time-invariant",
    "title": "5  Mixed models",
    "section": "5.2 Wide vs Long data, Time-varying vs Time-invariant",
    "text": "5.2 Wide vs Long data, Time-varying vs Time-invariant\nBefore you begin your analysis, you need to ensure that the data is in the proper format. Let’s consider the NLS data, where we have measures of women’s salary over 20 years.\nWide format of the data would have row represent a woman, and she would have 20 columns worth of salary information1 (plus additional demographics).\nLong format of the data would have each row represent a woman and a year, so that each woman can have up to 20 rows (if a woman wasn’t measured in a given year, that row & year is missing).\nTo fit a mixed model, we need the data in long format. We can use the reshape command to transform wide data to long. This is covered in the Stata I set of notes.\nAdditionally, there is the concept of time-varying vs time-invariant variables. Time-varying variables are those which can be different for each entry within the same individual. Examples include weight or salary. Time-invariant are those which are the same across all entries. Examples include race or baseline characteristics.\nWhen data is long, time-invariant variables need to be constant per person."
  },
  {
    "objectID": "05-mixed-models.html#linear-mixed-model",
    "href": "05-mixed-models.html#linear-mixed-model",
    "title": "5  Mixed models",
    "section": "5.3 Linear Mixed Model",
    "text": "5.3 Linear Mixed Model\nThe most basic mixed model is the linear mixed model, which extends the linear regression model. A model is called “mixed” because it contains a mixture of fixed effects and random effects.\n\nFixed effects: These are the predictors that are present in regular linear regression. We will obtain coefficients for these predictors and be able to test and interpret them. Technically, an OLS linear model is a mixed model with only fixed effects.2\nRandom effects: These are the “grouping” variables, and must be categorical (Stata will force every variable used to produce random effects as if it were prefaced by i.). These are essentially just predictors as well, however, we do not obtain coefficients to test or interpret. We do get a measure of the variability across groups, and a test of whether the random effect is benefiting the model.\n\nLet’s fit a model using the mixed command. It works similar to regress with a slight tweak. We’ll try and predict log of wages3 using work experience, race and age. The variable idcode identifies individuals.\n. mixed ln_w ttl_exp i.race age || idcode:\n\nPerforming EM optimization ...\n\nPerforming gradient-based optimization: \nIteration 0:  Log likelihood = -10471.727  \nIteration 1:  Log likelihood = -10471.727  \n\nComputing standard errors ...\n\nMixed-effects ML regression                         Number of obs    =  28,510\nGroup variable: idcode                              Number of groups =   4,710\n                                                    Obs per group:\n                                                                 min =       1\n                                                                 avg =     6.1\n                                                                 max =      15\n                                                    Wald chi2(4)     = 5246.25\nLog likelihood = -10471.727                         Prob &gt; chi2      =  0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     ttl_exp |    .042996   .0010126    42.46   0.000     .0410113    .0449807\n             |\n        race |\n      Black  |  -.1178248   .0115814   -10.17   0.000    -.1405238   -.0951257\n      Other  |   .0995432   .0484246     2.06   0.040     .0046328    .1944536\n             |\n         age |  -.0069699   .0006836   -10.20   0.000    -.0083097   -.0056302\n       _cons |   1.644184   .0161148   102.03   0.000     1.612599    1.675768\n------------------------------------------------------------------------------\n\n------------------------------------------------------------------------------\n  Random-effects parameters  |   Estimate   Std. err.     [95% conf. interval]\n-----------------------------+------------------------------------------------\nidcode: Identity             |\n                  var(_cons) |   .1037959   .0026579      .0987151    .1091381\n-----------------------------+------------------------------------------------\n               var(Residual) |    .089042   .0008183      .0874526    .0906603\n------------------------------------------------------------------------------\nLR test vs. linear model: chibar2(01) = 11678.16      Prob &gt;= chibar2 = 0.0000\nThe fixed part of the equation, ln_w ttl_exp i.race age is the same as with linear regression, ln_w is the outcome and the rest are predictors, with race being categorical. The new part is || idcode:. The || separates the fixed on the left from the random effects on the right. idcode identifies individuals. The : is to enable the more complicated feature of random slopes which we won’t cover here; for our purposes the : is just required.\nLet’s walk through the output. Note that what we are calling the random effects (e.g. individuals in a repeated measures situation, classrooms in a students nested in classroom situation), Stata refers to as “groups” in much of the output.\n\nAt the very top, you’ll see that the solution is arrived at iteratively, similar to logistic regression (you probably also noticed how slow it is)!\nThe log likelihood is how the iteration works; essentially the model “guesses” choices for the coefficients, and finds the set of coefficients that minimize the log likelihood. Of course, the “guess” is much smarter than random. The actual value of the log likelihood is meaningless.\nSince we are dealing with repeated measures of some sort, instead of a single sample size, we record the total number of obs, the number of groups (unique entries in the random effects) and min/mean/max of the groups. As before, just ensure these numbers seem right.\nAs with logistic regression, the \\(^\\chi^2^\\) test tests the hypothesis that all coefficients are simultaneously 0.\n\nWe gave a significant p-value, so we continue with the interpretation.\n\nThe coefficients table is interpreted just as in linear regression, with the addendum that each coefficient is also controlling for the structure introduced by the random effects.\n\nIncreased values of ttl_exp is associated with higher log incomes.\nThe race baseline is “white”; compared to white, blacks have lower average income and others have higher average income.\nHigher age is associated with lower income.\n\nThe second table (“Random-effects parameters”) gives us information about the error structure. The “idcode:” section is examining whether there is variation across individuals above and beyond the differences in characteristics such as age and race. Since the estimate of var(_cons) (the estimated variance of the constant per person - the individual level random effect) is non-zero (and not close to zero), that is evidence that the random effect is beneficial. If the estimate was 0 or close to 0, that would be evidence that the random effect is unnecessary and that any difference between individuals is already accounted for by the covariates.\nThe estimated variance of the residuals is any additional variation between observations. This is akin to the residuals from linear regression.\nThe \\(^\\chi^2^\\) test at the bottom is a formal test of the inclusion of the random effects versus a linear regression model without the random effects. We reject the null that the models are equivalent, so it is appropriate to include the random effects."
  },
  {
    "objectID": "05-mixed-models.html#assumptions",
    "href": "05-mixed-models.html#assumptions",
    "title": "5  Mixed models",
    "section": "5.4 Assumptions",
    "text": "5.4 Assumptions\nThe linear additivity remains necessary. rvfplot will not work following a mixed command, but you can generate the residuals vs fitted plot manually.\nThe homogeneity of residuals assumption is violated by design in a mixed model. However, some forms of heterogeneity, such as increasing variance as fitted values increase, are not supported. Therefore we can still use the residuals vs fitted plot to examine this.\nAgain, the independence assumption is violated by design, but observations between groups (e.g. between individuals) should be independent."
  },
  {
    "objectID": "05-mixed-models.html#miscellaneous",
    "href": "05-mixed-models.html#miscellaneous",
    "title": "5  Mixed models",
    "section": "5.5 Miscellaneous",
    "text": "5.5 Miscellaneous\nAs we’ve discussed before, collinearity, overfitting, and model selection remain concerns.\nSample size considerations are tricky with mixed models. Typically these are done with simulations. At a rough pass, the rules of thumb from linear regression remain; 10-20 observations per predictor. Adding a new person will improve the power more than adding another observation for an existing group.\nThe margins and predict command work similarly to regress, however note that both (by default) ignore the random effects; that is, the results the produce are averaged across all individuals.\nAs with linear regression and logistic regression, mixed supports vce(robust) to enable robust standard errors."
  },
  {
    "objectID": "05-mixed-models.html#convergence-issues",
    "href": "05-mixed-models.html#convergence-issues",
    "title": "5  Mixed models",
    "section": "5.6 Convergence issues",
    "text": "5.6 Convergence issues\nAs with logistic regression, the solution is arrived at iteratively, which means it can fail to converge for a number of reasons. Separation isn’t an issue here (though it will be in logistic mixed models), but there can be other causes of a failure to converge.\nGenerally, failure to converge will be due to an issue with the data. Things to look for include:\n\nDifferent scales of predictors. For example, salary (in dollars) and number of children. The scales are drastically different which can cause issues. Try re-scaling any variables on extreme scales (you can do this with egen scaledvar = std(origvar)). This will affect interpretation (the estimated coefficient will be the average predicted change with a one standard deviation increase in the predictor) but not the overall model fit.\nHigh correlation can cause this. Check correlations (pwcorr or corr) between your predictors (including any categorical variables) and if you find a highly correlated pair, try removing one.\nIf the iteration keeps running (as opposed to ending and complaining about lack of convergence), try passing the option emiterate(#) with a few “large” (“large” is relative to sample size) numbers to tell the algorithm to stop after # iterations, regardless of convergence. You’re looking for two things:\n\nFirst, if there are any estimated standard errors that are extremely close to zero, that predictor may be causing the issue. Try removing it.\nSecond, if you try a few different max iterations (say 50, 100 and 200), and the estimated coefficients and standard errors are relatively constant, you could consider that model as “good enough”. You wouldn’t have much confidence in the point estimates of the coefficients, but you could at least gain insight into the direction and approximate magnitude of the effect.\n\nYou can try use the “reml” optimizer, by passing the reml option. This optimizer can be a bit easier to converge."
  },
  {
    "objectID": "05-mixed-models.html#logistic-mixed-model",
    "href": "05-mixed-models.html#logistic-mixed-model",
    "title": "5  Mixed models",
    "section": "5.7 Logistic Mixed Model",
    "text": "5.7 Logistic Mixed Model\nSimilar to logistic regression being an extension to linear regression, logistic mixed models are an extension to linear mixed models when the outcome variable is binary.\nThe command for logistic mixed models is melogit. The rest of the command works very similarly to mixed, and interpretation is the best of logistic regression (for fixed effects) and linear mixed models (for random effects). Unfortunately, neither lroc nor estat gof is supported, so goodness of fit must be measured solely on the \\(^\\chi^2^\\) test and perhaps a manual model fit comparison.\nBy default the log-odds are reported, give the or option to report the odds ratios.\n\n5.7.1 meqrlogit\nThere is a different solver that can be used based upon QR-decomposition. This is run with the command meqrlogit. It functions identically to melogit. If melogit has convergence issues, try using meqrlogit instead."
  },
  {
    "objectID": "05-mixed-models.html#exercise-6",
    "href": "05-mixed-models.html#exercise-6",
    "title": "5  Mixed models",
    "section": "5.8 Exercise 6",
    "text": "5.8 Exercise 6\nLoad up the “chicken” data set from Stata’s website:\nwebuse chicken, clear\nThe data contains order information from a number of restaurants and records whether the order resulted in a complaint. We’d like to see what attributes (if any) of the servers may increase the odds of a complaint. Since we have multiple orders per restaurant, it’s reasonable to assume that certain restaurants just recieve more complaints than others, regardless of the server, so we’ll need to include random effects for those.\nFit a mixed effects logistic regression model predicting complain, based upon server characteristics (grade, race, gender, tenure, age, income) and a few restaurant characteristics (genderm for gender of manager and nworkers for number of workers). Include a random effect for restaurant.\n\nDoes the model fit better than chance?\nInterpret the model. What predicts a higher odds of recieving a complaint?\nDoes it appear that adding the random effect was needed?"
  },
  {
    "objectID": "05-mixed-models.html#footnotes",
    "href": "05-mixed-models.html#footnotes",
    "title": "5  Mixed models",
    "section": "",
    "text": "She may have salary information for only a subset of those years, but would have missing values in the other years.↩︎\nThough why called it mixed at that point?↩︎\nTypically, salary information is very right-skewed, and a log transformation makes the data closer to normal.↩︎"
  },
  {
    "objectID": "06-survey-data.html#definitions",
    "href": "06-survey-data.html#definitions",
    "title": "6  Survey Data",
    "section": "6.1 Definitions",
    "text": "6.1 Definitions\nComplex survey design is a massive topic which there are entire departments devoted to (Program at Survey Methodology here at Michigan) and which we offer a separate full day workshop (Survey Design). A simple survey design takes a random sample from the population as a whole. There are various reasons why a simple random sample will not work.\n\nIt is often infeasible to do either because of time or cost.\nWith smaller sample sizes, it can be difficult to obtain enough individuals in a given subpopulation.\nFor some small subpopulations, it may be very difficult to even obtain any individuals in a simple random sample.\n\nA complex survey design allows researchers to consider these limitations and design a sampling pattern to overcome them. Three primary techniques are\n\nStratification. Rather than sample all individuals, instead target specific subpopulations and collect from them explicitly. For example, you may stratify by race and aim to collect 50 white, 50 black, 50 Hispanic, etc.\nClustering. Primarily a cost/time saving measure. Similar to stratification, but instead of sampling from all clusters, you take a random sample of clusters and then sample within them. A typical clustering variable is neighborhood or census tract or school.\nWeighting. If certain sets of characteristics are more or less common, or more or less desired, when randomly sampling individuals, we can down-weight those who we don’t want/are more common, and up-weight those we want/are less common.\n\nFor example, we might want to collect data on obesity in school children in Ann Arbor. Rather than randomly sampling across all schools, we cluster by schools and randomly select 3. Then at each of those schools, we stratify by race and take a random sample of all students of each race at each school, weighted by their weight to attempt to capture more overweight students.\nOne final term is primary sampling unit which is the first level at which we randomized. In this example, that would be schools."
  },
  {
    "objectID": "06-survey-data.html#describing-the-survey",
    "href": "06-survey-data.html#describing-the-survey",
    "title": "6  Survey Data",
    "section": "6.2 Describing the survey",
    "text": "6.2 Describing the survey\nThe general syntax is\nsvyset &lt;psu&gt; [pweight = &lt;weight&gt;], strata(&lt;strata&gt;)\nThe svyset command defines the variables identifying the complex design of the sample to Stata, and only needs to be submitted once in a given Stata session. The &lt;psu&gt; is a variable identifying the primary sampling unit (PSU) that an observation came from. The &lt;weight&gt; is a variable containing sampling weights. Finally, the &lt;strata&gt; is a variable identifying the sampling stratum that an observation came from.\nThe NHANES data we’ve been using in our examples is actually from a complex sample design, which we’ve been ignoring. Let’s incorporate the sampling into the analysis.\n. webuse nhanes2, clear\nThe three variables of interest in the data are finalwgt for the sampling weights, strata for the strata, and psu for the clusters.\n. describe finalwgt strata psu\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nfinalwgt        long    %9.0g                 Sampling weight (except lead)\nstrata          byte    %9.0g                 Stratum identifier\npsu             byte    %9.0g      psulbl     Primary sampling unit\nIt’s useful to know that to remove any existing survey design, you can run\n. svyset, clear\nLet’s set up the survey design now.\n. svyset psu [pweight = finalwgt], strata(strata)\n\nSampling weights: finalwgt\n             VCE: linearized\n     Single unit: missing\n        Strata 1: strata\n Sampling unit 1: psu\n           FPC 1: &lt;zero&gt;\nTo get information about the strata and cluster variables use the following command or menu:\n. svydescribe\n\nSurvey: Describing stage 1 sampling units\n\nSampling weights: finalwgt\n             VCE: linearized\n     Single unit: missing\n        Strata 1: strata\n Sampling unit 1: psu\n           FPC 1: &lt;zero&gt;\n\n                                    Number of obs per unit\n Stratum   # units     # obs       Min      Mean       Max\n----------------------------------------------------------\n       1         2       380       165     190.0       215\n       2         2       185        67      92.5       118\n       3         2       348       149     174.0       199\n       4         2       460       229     230.0       231\n       5         2       252       105     126.0       147\n       6         2       298       131     149.0       167\n       7         2       476       206     238.0       270\n       8         2       338       158     169.0       180\n       9         2       244       100     122.0       144\n      10         2       262       119     131.0       143\n      11         2       275       120     137.5       155\n      12         2       314       144     157.0       170\n      13         2       342       154     171.0       188\n      14         2       405       200     202.5       205\n      15         2       380       189     190.0       191\n      16         2       336       159     168.0       177\n      17         2       393       180     196.5       213\n      18         2       359       144     179.5       215\n      20         2       285       125     142.5       160\n      21         2       214       102     107.0       112\n      22         2       301       128     150.5       173\n      23         2       341       159     170.5       182\n      24         2       438       205     219.0       233\n      25         2       256       116     128.0       140\n      26         2       261       129     130.5       132\n      27         2       283       139     141.5       144\n      28         2       299       136     149.5       163\n      29         2       503       215     251.5       288\n      30         2       365       166     182.5       199\n      31         2       308       143     154.0       165\n      32         2       450       211     225.0       239\n----------------------------------------------------------\n      31        62    10,351        67     167.0       288\nOnce the survey is defined with svyset, most common commands can be prefaced by svy: to analyze the data with the sampling structure. The svy: tab command works exactly like the tabulate command, only taking the design of the sample into account when producing estimates and chi-square statistics.\n. svy: tab sex\n(running tabulate on estimation sample)\n\nNumber of strata = 31                            Number of obs   =      10,351\nNumber of PSUs   = 62                            Population size = 117,157,513\n                                                 Design df       =          31\n\n----------------------\n      Sex | proportion\n----------+-----------\n     Male |      .4794\n   Female |      .5206\n          | \n    Total |          1\n----------------------\nKey: proportion = Cell proportion\nNext, lets look at the mean weight by gender.\n. svy: mean weight, over(sex)\n(running mean on estimation sample)\n\nSurvey: Mean estimation\n\nNumber of strata = 31            Number of obs   =      10,351\nNumber of PSUs   = 62            Population size = 117,157,513\n                                 Design df       =          31\n\n--------------------------------------------------------------\n             |             Linearized\n             |       Mean   std. err.     [95% conf. interval]\n-------------+------------------------------------------------\nc.weight@sex |\n       Male  |   78.62789   .2097761      78.20004    79.05573\n     Female  |   65.70701    .266384      65.16372    66.25031\n--------------------------------------------------------------\nCompare this to the usual mean command, without the design information:\n. mean weight, over(sex)\n\nMean estimation                         Number of obs = 10,351\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\nc.weight@sex |\n       Male  |   77.98423   .1945289      77.60292    78.36555\n     Female  |   66.39418   .1998523      66.00243    66.78593\n--------------------------------------------------------------\nAnd compare the svy: results to the usual mean command, with only the weights considered:\n. mean weight [pweight=finalwgt], over(sex)\n\nMean estimation                         Number of obs = 10,351\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\nc.weight@sex |\n       Male  |   78.62789   .2272099      78.18251    79.07326\n     Female  |   65.70701   .2265547      65.26292     66.1511\n--------------------------------------------------------------\nWe see that the weights affect on the standard error, whereas the stratification and clustering also affects the estimates.\nMany of the usual commands such as regress or logit can be prefaced by svy:. If a command errors with the svy: prefix, a lot of the time the survey design will not affect it, and the documentation for the command will inform of that."
  },
  {
    "objectID": "06-survey-data.html#subset-analyses-for-complex-sample-survey-data",
    "href": "06-survey-data.html#subset-analyses-for-complex-sample-survey-data",
    "title": "6  Survey Data",
    "section": "6.3 Subset analyses for complex sample survey data",
    "text": "6.3 Subset analyses for complex sample survey data\nIn general, analysis of a particular subset of observations from a sample with a complex design should be handled very carefully. It is usually not appropriate to delete cases from the data-set that fall outside the sub-population of interest, or to use an if statement to filter them out. In Stata, sub-population analyses for this type of data are analyzed using a subpop indicator.\nSuppose we want to perform an analysis only for the cases where race is black in the NHANES data set. First, we must create an indicator variable that equals 1 for these cases.\n. gen race_black = race == 2\n\n. replace race_black = . if race == .\n(0 real changes made)\nNow we can run a simple regression model only on\n. svy, subpop(race_black): regress weight height i.sex\n(running regress on estimation sample)\n\nSurvey: Linear regression\n\nNumber of strata = 30                            Number of obs   =      10,013\nNumber of PSUs   = 60                            Population size = 113,415,086\n                                                 Subpop. no. obs =       1,086\n                                                 Subpop. size    =  11,189,236\n                                                 Design df       =          30\n                                                 F(2, 29)        =       50.12\n                                                 Prob &gt; F        =      0.0000\n                                                 R-squared       =      0.1131\n\n------------------------------------------------------------------------------\n             |             Linearized\n      weight | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |    .708568   .0728382     9.73   0.000     .5598126    .8573234\n             |\n         sex |\n     Female  |   3.508388   1.348297     2.60   0.014     .7547976    6.261979\n       _cons |  -46.10337   12.56441    -3.67   0.001    -71.76331   -20.44343\n------------------------------------------------------------------------------\nNote: 1 stratum omitted because it contains no subpopulation members.\nCompare the svy, subpop( ): results to the usual svy: regress command using an if statement:\n. svy: reg weight height i.sex if race_black == 1\n(running regress on estimation sample)\n\nSurvey: Linear regression\n\nNumber of strata = 30                             Number of obs   =      1,086\nNumber of PSUs   = 55                             Population size = 11,189,236\n                                                  Design df       =         25\n                                                  F(0, 25)        =          .\n                                                  Prob &gt; F        =          .\n                                                  R-squared       =     0.1131\n\n------------------------------------------------------------------------------\n             |             Linearized\n      weight | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |    .708568          .        .       .            .           .\n             |\n         sex |\n     Female  |   3.508388          .        .       .            .           .\n       _cons |  -46.10337          .        .       .            .           .\n------------------------------------------------------------------------------\nNote: Missing standard errors because of stratum with single sampling unit.\nThe point estimates and \\(^R^2^\\) are the same, but Stata refuses to even calculate standard errors."
  },
  {
    "objectID": "07-multiple-imputation.html#missing-at-random",
    "href": "07-multiple-imputation.html#missing-at-random",
    "title": "7  Multiple Imputation",
    "section": "7.1 Missing at random",
    "text": "7.1 Missing at random\nThere can be many causes of missing data. We can classify the reason data is missing into one of three categories:\n\nMissing completely at random (MCAR): This is missingness that is truly random - there is no cause of the missingness, it’s just due to chance. For example, you’re entering paper surveys into a spreadsheet and spill coffee on them, obscuring a few answers.\nMissing at random (MAR): The missingness here is due to observed data but not unobserved data. For example, women may be less likely to report their age, regardless of what their actual age is.\nMissing not at random (MNAR): Here the missingness is due to the missing value. For example, individuals with higher salary may be less willing to answer survey questions about their salary.\n\nThere is no statistical test1 to distinguish between these categories; instead you must use your knowledge of the data and its collection to argue which category it falls under.\nThis is important because most imputation methods (including MI) require MCAR or MAR for the data. If the data is MNAR, there is very little you can do. Generally if you believe the data is MNAR, you can assume MAR but discuss that a severe limitation of your analysis is the MAR assumption is likely invalid."
  },
  {
    "objectID": "07-multiple-imputation.html#mi",
    "href": "07-multiple-imputation.html#mi",
    "title": "7  Multiple Imputation",
    "section": "7.2 mi",
    "text": "7.2 mi\nThe mi set of commands in Stata perform the steps of multiple imputation. There are three steps, with a preliminary step to examine the missingness. We’ll be using the “mheart5” data from Stata’s website which has some missing data.\n. webuse mheart5, clear\n(Fictional heart attack data)\n\n. describe, short\n\nContains data from https://www.stata-press.com/data/r18/mheart5.dta\n Observations:           154                  Fictional heart attack data\n    Variables:             6                  19 Jun 2022 10:50\nSorted by: \n\n. summarize\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      attack |        154    .4480519    .4989166          0          1\n      smokes |        154    .4155844    .4944304          0          1\n         age |        142    56.43324    11.59131   20.73613   83.78423\n         bmi |        126    25.23523    4.029325   17.22643   38.24214\n      female |        154    .2467532    .4325285          0          1\n-------------+---------------------------------------------------------\n      hsgrad |        154    .7532468    .4325285          0          1\nWe see from the summary that both age and bmi have some missing data.\n\n7.2.1 Setting up data\nWe need to tell Stata how we’re going to be doing the imputations. First, use the mi set command to determine how the multiple data sets will be stored. Really which option you choose is up to you, I prefer to “flong” option, where each imputed data set is stacked on top of each other. If you have very large data, you might prefer “wide”, “mlong” or “mlongsep”, the last of which stores each imputed data set in a separate file. See help mi styles for more details. (Ultimately the decision is not that important, as you can switch later using mi convert &lt;new style&gt;.)\n. mi set flong\nNext, we need to tell Stata what each variable will be used for. The options are\n\nimputed: A variable with missing data that needs to be imputed.\nregular: Any variable that is complete or does not need imputation.\n\nTechnically we only need specify the imputed variables, as anything unspecified is assumed to be regular. We saw above that age and bmi have missing values:\n. mi register imputed age bmi\n(28 m=0 obs now marked as incomplete)\nWe can examine our setup with mi describe:\n. mi describe\n\nStyle: flong\n       last mi update 16aug2023 19:56:14, 0 seconds ago\n\nObservations:\n   Complete          126\n   Incomplete         28  (M = 0 imputations)\n   ---------------------\n   Total             154\n\nVariables:\n   Imputed: 2; age(12) bmi(28)\n\n   Passive: 0\n\n   Regular: 0\n\n   System:  3; _mi_m _mi_id _mi_miss\n\n   (there are 4 unregistered variables; attack smokes female hsgrad)\nWe see 126 complete observations with 28 incomplete, the two variables to be imputed, and the 4 unregistered variables which will automatically be registered as regular.\n\n7.2.1.1 Imputing transformations\nWhat happens if you had a transform of a variable? Say you had a variable for salary, and wanted to use a log transformation?\nYou can find literature suggesting either transforming first and then imputing, or imputing first and then transforming. Our suggestion, following current statistical literature is to transform first, impute second.(Hippel 2009)\nStata technically supports the other option via mi register passive, but we don’t recommend it’s usage. Instead, transform your original data, then flag both the variable and its transformations as “imputed”\n\n\n\n7.2.2 Performing the imputation\nNow that we’ve got the MI set up, we can perform the actual procedure. There are a very wide number of variations on how this imputation can be done (including defining your own!). You can see these as the options to mi impute. We’ll just be focusing on the “chained” approach, which is a good approach to start with.\nThe syntax for this is a bit complicated, but straightforward once you understand it.\nmi impute chained (&lt;method 1&gt;) &lt;variables to impute with method 1&gt; ///\n                  (&lt;method 2&gt;) &lt;variables to impute with method 2&gt; ///\n                  = &lt;all non-imputed variables&gt;, add(&lt;number of imputations&gt;)\nThe &lt;methods&gt; are essentially what type of model you would use to predict the outcome. For example, for continuous data, use regress. For binary data use logit. It also supports ologit (ordinal logistic regression, multiple categories with ordering), mlogit (multinomial logistic regression, multiple categories without ordering), poisson or nbreg (poisson regression or negative binomial regression, for count data), as well as some others. See help mi impute chained under “uvmethod” for the full list.\nThe add( ) option specifies how many imputed data sets to generate, we’ll discuss below how to choose this.\nContinuing with our example might make this more clear. To perform our imputation, we would use\n. mi impute chained (regress) bmi age = attack smokes female hsgrad, add(5)\nnote: missing-value pattern is monotone; no iteration performed.\n\nConditional models (monotone):\n               age: regress age attack smokes female hsgrad\n               bmi: regress bmi age attack smokes female hsgrad\n\nPerforming chained iterations ...\n\nMultivariate imputation                     Imputations =        5\nChained equations                                 added =        5\nImputed: m=1 through m=5                        updated =        0\n\nInitialization: monotone                     Iterations =        0\n                                                burn-in =        0\n\n               bmi: linear regression\n               age: linear regression\n\n------------------------------------------------------------------\n                   |               Observations per m             \n                   |----------------------------------------------\n          Variable |   Complete   Incomplete   Imputed |     Total\n-------------------+-----------------------------------+----------\n               bmi |        126           28        28 |       154\n               age |        142           12        12 |       154\n------------------------------------------------------------------\n(Complete + Incomplete = Total; Imputed is the minimum across m\n of the number of filled-in observations.)\nSince both bmi and age are continuous variables, we use method regress. Imagine if we were also imputing smokes, a binary variable. Then the imputation (after running mi register imputed smokes) would be:\nmi impute chained (regress) bmi age (logit) smokes = attack female hsgrad, add(5)\nHere, regress was used for bmi and age, and logit was used for smokes.\n\n7.2.2.1 Choosing the number of imputations\nClassic literature has suggested you need only 5 imputations to obtain valid results. This will address the efficiency of point estimates, but not standard errors. More modern literature increases this number, with a good starting point being 200 imputations. (Graham 2007, White et al 2011)\nIf your data set is large and the imputation is slow, a recent paper (Von Hippel 2018) gives a two-stage procedure to estimate the required number of imputations. This two-stage procedure first performs a small number of imputations and carries out the analysis. It then using the results of that analysis to inform a better estimate of the required sample size. You can install the user command how_many_imputations for details and examples\nssc install how_many_imputations\nhelp how_many_imputations\n\n\n7.2.2.2 _mi_ variables\nAfter you’ve performed your imputation2, three new variables are added to your data, and your data gets \\(^M^\\) additional copies of itself. In the example above, we added 5 imputations, so there are a total of 6 copies of the data - the raw data (with the missing values), and 5 copies with imputed values. The new variables added are:\n\n_mi_id is the ID number of each row corresponding to its position in the original data\n_mi_miss flags whether the row had missing data originally.\n_mi_m is which data-set we’re looking at. 0 represents the unimputed data, 1 represents the first imputation, 2 the second, etc.\n\n\n\n\n7.2.3 Analyzing mi data\nNow that we’ve got the data set up for multiple imputations, and done the imputation, most of the hard part is over. Analyzing MI data is straightforward, usually. (When it isn’t, you can do this manually.)\nBasically, take any analysis command you would normally run, e.g. regress y x, and preface it by mi estimate:. Let’s try to predict the odds of a heart attack based upon other characteristics in the data. We would run a logistic regression model,\nlogit attack smokes age bmi female hsgrad\nSo to run it with multiple imputations:\n. mi estimate: logit attack smokes age bmi female hsgrad\n\nMultiple-imputation estimates                   Imputations       =          5\nLogistic regression                             Number of obs     =        154\n                                                Average RVI       =     0.0966\n                                                Largest FMI       =     0.2750\nDF adjustment:   Large sample                   DF:     min       =      62.83\n                                                        avg       =  53,215.09\n                                                        max       = 146,351.98\nModel F test:       Equal FMI                   F(   5, 1243.8)   =       2.90\nWithin VCE type:          OIM                   Prob &gt; F          =     0.0130\n\n------------------------------------------------------------------------------\n      attack | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      smokes |   1.163433    .352684     3.30   0.001       .47217    1.854695\n         age |   .0284627   .0164787     1.73   0.086    -.0040684    .0609938\n         bmi |   .0800942   .0491285     1.63   0.108    -.0180864    .1782749\n      female |  -.0970499   .4091373    -0.24   0.812    -.8989527    .7048528\n      hsgrad |     .10968   .3991282     0.27   0.783    -.6726034    .8919634\n       _cons |  -4.390356   1.598513    -2.75   0.006    -7.531833   -1.248878\n------------------------------------------------------------------------------\nWe see a single model, even though 5 models (one for each imputation) were run in the background. The results from these models were pooled using something called “Rubin’s rules” to produce a single model output.\nWe see a few additional fit summaries about the multiple imputation that aren’t super relevant; but otherwise all the existing interpretations hold. Note that an F-test instead of \\(^\\chi^2^\\) test is run, but still tests the same hypothesis that all coefficients are identically zero. Among the coefficients, we see that smokers have significantly higher odds of having a heart attack, and there’s some weak evidence that age plays a role.\n\n7.2.3.1 MI Postestimation\nIn general, most postestimation commands will not work after MI. The general approach is to do the MI manually and run the postestimation for each imputation. One exception is that mi predict works how predict does."
  },
  {
    "objectID": "07-multiple-imputation.html#manual-mi",
    "href": "07-multiple-imputation.html#manual-mi",
    "title": "7  Multiple Imputation",
    "section": "7.3 Manual MI",
    "text": "7.3 Manual MI\nSince we set the data as flong, each imputed data set lives in the data with a separate _mi_m value. You can conditionally run analyses on each, e.g.\nlogit attack smokes age bmi female hsgrad if _mi_m == 0\nto run the model on only the original data.\nIt is tedious to do this over all imputed data, so instead we can run mi xeq: as a prefix to run a command on each separate data set. This is similar to mi estimate: except without the pooling.\n. mi xeq: summ age\n\nm=0 data:\n-&gt; summ age\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         age |        142    56.43324    11.59131   20.73613   83.78423\n\nm=1 data:\n-&gt; summ age\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         age |        154    56.20732    11.61166   20.73613   83.78423\n\nm=2 data:\n-&gt; summ age\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         age |        154    55.79566    11.88629    16.9347   83.78423\n\nm=3 data:\n-&gt; summ age\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         age |        154    56.35074    11.50551   20.73613   83.78423\n\nm=4 data:\n-&gt; summ age\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         age |        154    56.35633     11.8424   20.73613   86.11715\n\nm=5 data:\n-&gt; summ age\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         age |        154    56.40651    11.44234   20.73613   83.78423\nThis can also be useful if the analysis you want to execute is not supported by mi estimate yet.\n\n7.3.1 Rubin’s rules\nIf you wanted to pool the results yourself, you can obtain an estimate for the pooled parameter by simple average across imputations. The formula for variance is slightly more complicated so we don’t produce it here, however it can be found in the “Methods and formulas” section of the MI manual (run help mi estimate, click on “[MI] mi estimate” at the top of the file to open the manual."
  },
  {
    "objectID": "07-multiple-imputation.html#removing-the-mi-data",
    "href": "07-multiple-imputation.html#removing-the-mi-data",
    "title": "7  Multiple Imputation",
    "section": "7.4 Removing the MI data",
    "text": "7.4 Removing the MI data\nIdeally, you should save the data (or preserve it) prior to imputing, so you can easily recover the unimputed data if you wish. If you wanted to return to the original data, the following should work:\nmi unset\ndrop if mi_m != 0\ndrop mi_*\nThe first tells Stata not to treat it as imputed anymore; the second drops all imputed data sets; the third removes the MI variables that were generated.\nThis only works for mi set flong; if you use another method, you can tweak the above or use mi convert flong to switch to “flong” first."
  },
  {
    "objectID": "07-multiple-imputation.html#survey-and-multiple-imputation",
    "href": "07-multiple-imputation.html#survey-and-multiple-imputation",
    "title": "7  Multiple Imputation",
    "section": "7.5 Survey and multiple imputation",
    "text": "7.5 Survey and multiple imputation\nJust a quick note, if you want to utilize by complex survey design and multiple imputation simultaneously, proper ordering needs to be given. Note that only weights play a role in multiple imputation.\nmi set ...\nmi svyset ...\nmi impute ... [pweight = weight]\nmi estimate: svy: regress ...\n\nThere has been some discussion that imputation should not take into account any complex survey design features (because you want the imputation to reflect the sample, not necessarily the population). See for example Little and Vartivarian 2003. If you follow this advice, simply exclude the [pweight = ...] part of the mi impute command. In either case, estimation commands still need both the mi estimate: svy: prefixes in that order.\n\nThe above paragraph is no longer accurate. See Reist and Larsen 2012. Survey weights must be used in mulitple imputations."
  },
  {
    "objectID": "07-multiple-imputation.html#citations",
    "href": "07-multiple-imputation.html#citations",
    "title": "7  Multiple Imputation",
    "section": "7.6 Citations",
    "text": "7.6 Citations\n\nLittle, RJ, and S Vartivarian. 2003. On weighting the rates in non-response weights. Stat Med 22, no. 9: 1589-1599.\nVon Hippel, Paul T. “How to impute interactions, squares, and other transformed variables.” Sociological Methodology 39.1 (2009): 265-291.\nReist, Benjamin M., and Michael D. Larsen. “Post-Imputation Calibration Under Rubin’s Multiple Imputation Variance Estimator.” Section on Survey Research Methods, Joint Statistical Meeting. 2012.\nvon Hippel, Paul T. “How Many Imputations Do You Need? A Two-stage Calculation Using a Quadratic Rule.” Sociological Methods & Research (2018): 0049124117747303.\nWhite, I. R., P. Royston, and A. M. Wood. 2011. “Multiple Imputation Using Chained Equations: Issues and Guidance for Practice.” Statistics in Medicine 30:377-99."
  },
  {
    "objectID": "07-multiple-imputation.html#footnotes",
    "href": "07-multiple-imputation.html#footnotes",
    "title": "7  Multiple Imputation",
    "section": "",
    "text": "There is technically Little’s MCAR test to compare MCAR vs MAR, but the majority of imputation methods require only MAR, not MCAR, so it’s of limited use. Additionally, it is not yet supported in Stata.↩︎\nTechnically this happens as soon as you run mi set, but they’re not interesting until after mi impute.↩︎"
  },
  {
    "objectID": "solutions.html#exercise-1",
    "href": "solutions.html#exercise-1",
    "title": "8  Exercise solutions",
    "section": "8.1 Exercise 1",
    "text": "8.1 Exercise 1\n. webuse nhanes2, clear\n\n\n\n. describe, short\n\nContains data from https://www.stata-press.com/data/r18/nhanes2.dta\n Observations:        10,351                  \n    Variables:            58                  20 Dec 2022 10:07\nSorted by: \nThere are 10,351 observations of 59 variables. The full describe output is suppressed for space, but you should run it.\n\n\n\n. tab race, mi\n\n       Race |      Freq.     Percent        Cum.\n------------+-----------------------------------\n      White |      9,065       87.58       87.58\n      Black |      1,086       10.49       98.07\n      Other |        200        1.93      100.00\n------------+-----------------------------------\n      Total |     10,351      100.00\nNo missing data.\n. tab diabetes, mi\n\n    Diabetes |\n      status |      Freq.     Percent        Cum.\n-------------+-----------------------------------\nNot diabetic |      9,850       95.16       95.16\n    Diabetic |        499        4.82       99.98\n           . |          2        0.02      100.00\n-------------+-----------------------------------\n       Total |     10,351      100.00\nTwo missing values.\nlead is continuous, so a table isn’t the most effective.\n. codebook lead\n\n-------------------------------------------------------------------------------\nlead                                                              Lead (mcg/dL)\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [2,80]                        Units: 1\n         Unique values: 53                        Missing .: 5,403/10,351\n\n                  Mean: 14.3203\n             Std. dev.: 6.16647\n\n           Percentiles:     10%       25%       50%       75%       90%\n                              8        10        13        17        22\nThere’s a lot of missingness.\n\n\n\n. pwcorr height weight bp*\n\n             |   height   weight bpsystol  bpdiast\n-------------+------------------------------------\n      height |   1.0000 \n      weight |   0.4775   1.0000 \n    bpsystol |  -0.0364   0.2861   1.0000 \n     bpdiast |   0.0675   0.3799   0.6831   1.0000 \nBlood pressure is highly correlated, more-so than height and weight. Weight is also correlated with both forms of BP. Height looks to be completely independent of boood pressure."
  },
  {
    "objectID": "solutions.html#exercise-2",
    "href": "solutions.html#exercise-2",
    "title": "8  Exercise solutions",
    "section": "8.2 Exercise 2",
    "text": "8.2 Exercise 2\n. webuse nhanes2, clear\n\n. twoway (scatter bpdiast bpsystol if sex == 1, mcolor(blue)) ///\n&gt;        (scatter bpdiast bpsystol if sex == 2, mcolor(pink)) ///\n&gt;        (lfit bpdiast bpsystol if sex == 1, lcolor(blue)) ///\n&gt;        (lfit bpdiast bpsystol if sex == 2, lcolor(pink)), ///\n&gt;         legend(label(1 \"Men\") label(2 \"Women\") order(1 2))\n\nWe can see the correlation between blood pressure measures, with a bit stronger of a relationship for men."
  },
  {
    "objectID": "solutions.html#exercise-3",
    "href": "solutions.html#exercise-3",
    "title": "8  Exercise solutions",
    "section": "8.3 Exercise 3",
    "text": "8.3 Exercise 3\n. webuse nhanes2, clear\n\n\n\nThe sample size is massive, so the central limit theorem suffices.\n\n\n\n. ttest height == 176 if sex == 1\n\nOne-sample t test\n------------------------------------------------------------------------------\nVariable |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\n  height |   4,915    174.7421    .1026447    7.196115    174.5408    174.9433\n------------------------------------------------------------------------------\n    mean = mean(height)                                           t = -12.2553\nH0: mean = 176                                   Degrees of freedom =     4914\n\n   Ha: mean &lt; 176               Ha: mean != 176               Ha: mean &gt; 176\n Pr(T &lt; t) = 0.0000         Pr(|T| &gt; |t|) = 0.0000          Pr(T &gt; t) = 1.0000\nThe test rejects; the average height of men in the sample is lower than the national average.\n\n\n\n. ttest age, by(sex)\n\nTwo-sample t test with equal variances\n------------------------------------------------------------------------------\n   Group |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\n    Male |   4,915     47.4238    .2448869     17.1683    46.94372    47.90389\n  Female |   5,436    47.72057    .2340613    17.25716    47.26171    48.17942\n---------+--------------------------------------------------------------------\nCombined |  10,351    47.57965    .1692044    17.21483    47.24798    47.91133\n---------+--------------------------------------------------------------------\n    diff |           -.2967619     .338842               -.9609578    .3674339\n------------------------------------------------------------------------------\n    diff = mean(Male) - mean(Female)                              t =  -0.8758\nH0: diff = 0                                     Degrees of freedom =    10349\n\n    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0\n Pr(T &lt; t) = 0.1906         Pr(|T| &gt; |t|) = 0.3812          Pr(T &gt; t) = 0.8094\nWe fail to reject; there is no difference that the average age differs by gender.\n\n\n\n. tab race diabetes\n\n           |    Diabetes status\n      Race | Not diabe   Diabetic |     Total\n-----------+----------------------+----------\n     White |     8,659        404 |     9,063 \n     Black |     1,000         86 |     1,086 \n     Other |       191          9 |       200 \n-----------+----------------------+----------\n     Total |     9,850        499 |    10,349 \nGiven the different scales per race, it’s hard to draw a comparison. We can look at the rowwise percents. (If you ran tab diabetes race, you’d need the columnwise percents.)\n. tab race diabetes, row chi2\n\n+----------------+\n| Key            |\n|----------------|\n|   frequency    |\n| row percentage |\n+----------------+\n\n           |    Diabetes status\n      Race | Not diabe   Diabetic |     Total\n-----------+----------------------+----------\n     White |     8,659        404 |     9,063 \n           |     95.54       4.46 |    100.00 \n-----------+----------------------+----------\n     Black |     1,000         86 |     1,086 \n           |     92.08       7.92 |    100.00 \n-----------+----------------------+----------\n     Other |       191          9 |       200 \n           |     95.50       4.50 |    100.00 \n-----------+----------------------+----------\n     Total |     9,850        499 |    10,349 \n           |     95.18       4.82 |    100.00 \n\n          Pearson chi2(2) =  25.3630   Pr = 0.000\nNearly double the percent of blacks have diabetes and the \\(^\\chi^2^\\) test confirms the difference is statistically significant."
  },
  {
    "objectID": "solutions.html#exercise-4",
    "href": "solutions.html#exercise-4",
    "title": "8  Exercise solutions",
    "section": "8.4 Exercise 4",
    "text": "8.4 Exercise 4\n. webuse nhanes2, clear\n\n. regress lead i.sex i.race c.age c.weight c.height i.region\n\n      Source |       SS           df       MS      Number of obs   =     4,948\n-------------+----------------------------------   F(9, 4938)      =    129.98\n       Model |   36027.945         9    4003.105   Prob &gt; F        =    0.0000\n    Residual |   152083.33     4,938  30.7985682   R-squared       =    0.1915\n-------------+----------------------------------   Adj R-squared   =    0.1901\n       Total |  188111.275     4,947  38.0253234   Root MSE        =    5.5496\n\n------------------------------------------------------------------------------\n        lead | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         sex |\n     Female  |   -5.26415   .2259894   -23.29   0.000     -5.70719    -4.82111\n             |\n        race |\n      Black  |   2.937124   .2671182    11.00   0.000     2.413454    3.460794\n      Other  |  -.5521668   .5617011    -0.98   0.326    -1.653351     .549017\n             |\n         age |   .0196081   .0048918     4.01   0.000     .0100181    .0291981\n      weight |  -.0012435   .0059001    -0.21   0.833    -.0128103    .0103233\n      height |  -.0275532   .0127149    -2.17   0.030      -.05248   -.0026264\n             |\n      region |\n         MW  |  -.1099025    .233614    -0.47   0.638    -.5678897    .3480848\n          S  |  -1.858491   .2346487    -7.92   0.000    -2.318507   -1.398476\n          W  |  -.2854048   .2380554    -1.20   0.231    -.7520992    .1812897\n             |\n       _cons |   21.16891   2.199034     9.63   0.000     16.85783       25.48\n------------------------------------------------------------------------------\n\n\n\nThe F-test rejects so the model is informative. The \\(^R^2^\\) is low, so there is a lot of variability we’re not capturing in this model.\n\n\n\n. rvfplot\n\nThis doesn’t look great. We don’t see any signs of nonnormality, but we do see a lot of very large positive residuals. If you look at a histogram for lead,\n. hist lead\n(bin=36, start=2, width=2.1666667)\n\nWe see right skew. The maintainers of this data noticed the same concern, as they include a loglead variable in the data to attempt to address this.\n. desc loglead\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nloglead         float   %9.0g                 log(lead)\nPerhaps we should have run the model with loglead as the output instead.\n\n\n\n. estat vif\n\n    Variable |       VIF       1/VIF  \n-------------+----------------------\n       2.sex |      2.05    0.488262\n        race |\n          2  |      1.05    0.950220\n          3  |      1.06    0.941064\n         age |      1.13    0.885331\n      weight |      1.33    0.749151\n      height |      2.44    0.410091\n      region |\n          2  |      1.71    0.583352\n          3  |      1.77    0.566163\n          4  |      1.73    0.576881\n-------------+----------------------\n    Mean VIF |      1.59\nNothing of concern here. The only moderately high VIF’s are on sex and it’s interaction, which does not concern us (of course a main effect and interaction are collinear.).\n\n\n\nThe coffecient on “Female” is -5 and is statistically significant, so there is evidence that males have higher average lead levels.\n\n\n\nThe p-value is very small, so it is statistically significant. However, if we look at lead levels:\n. summ lead\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        lead |      4,948    14.32033    6.166468          2         80\nWe see that lead levels range from 2 to 80. The coefficient on age is about .02, so age would need to increase by about 50 years to see a higher value for the lead score. Unlikely to be clinically interesting! This is a side effect of the massive sample size.\n\n\n\n. margins region\n\nPredictive margins                                       Number of obs = 4,948\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      region |\n         NE  |   14.93498    .176734    84.51   0.000      14.5885    15.28145\n         MW  |   14.82507   .1532375    96.75   0.000     14.52466    15.12549\n          S  |   13.07649   .1525888    85.70   0.000     12.77734    13.37563\n          W  |   14.64957   .1588465    92.22   0.000     14.33816    14.96098\n------------------------------------------------------------------------------\n\n. margins region, pwcompare(pv)\n\nPairwise comparisons of predictive margins               Number of obs = 4,948\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\n\n-----------------------------------------------------\n             |            Delta-method    Unadjusted\n             |   Contrast   std. err.      t    P&gt;|t|\n-------------+---------------------------------------\n      region |\n   MW vs NE  |  -.1099025    .233614    -0.47   0.638\n    S vs NE  |  -1.858491   .2346487    -7.92   0.000\n    W vs NE  |  -.2854048   .2380554    -1.20   0.231\n    S vs MW  |  -1.748589   .2161764    -8.09   0.000\n    W vs MW  |  -.1755023   .2216647    -0.79   0.429\n     W vs S  |   1.573087   .2227974     7.06   0.000\n-----------------------------------------------------\nIt looks like South is significantly lower levels of lead than the other regions, which show no difference between them.\n\n\n\n. regress lead i.sex##c.age i.race c.weight c.height i.region\n\n      Source |       SS           df       MS      Number of obs   =     4,948\n-------------+----------------------------------   F(10, 4937)     =    123.65\n       Model |  37676.3642        10  3767.63642   Prob &gt; F        =    0.0000\n    Residual |   150434.91     4,937  30.4709156   R-squared       =    0.2003\n-------------+----------------------------------   Adj R-squared   =    0.1987\n       Total |  188111.275     4,947  38.0253234   Root MSE        =      5.52\n\n------------------------------------------------------------------------------\n        lead | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         sex |\n     Female  |  -8.485856   .4923314   -17.24   0.000    -9.451044   -7.520667\n         age |  -.0150619   .0067745    -2.22   0.026    -.0283429   -.0017809\n             |\n   sex#c.age |\n     Female  |   .0676205   .0091936     7.36   0.000     .0495969    .0856441\n             |\n        race |\n      Black  |   2.985719   .2657756    11.23   0.000     2.464681    3.506758\n      Other  |  -.5307013   .5587129    -0.95   0.342    -1.626027    .5646243\n             |\n      weight |  -.0044452   .0058847    -0.76   0.450    -.0159819    .0070915\n      height |  -.0266069   .0126477    -2.10   0.035    -.0514021   -.0018118\n             |\n      region |\n         MW  |  -.1417546   .2324084    -0.61   0.542    -.5973783    .3138691\n          S  |  -1.897967   .2334589    -8.13   0.000     -2.35565   -1.440284\n          W  |  -.2945939   .2367891    -1.24   0.214    -.7588057     .169618\n             |\n       _cons |   22.89998   2.199931    10.41   0.000     18.58714    27.21282\n------------------------------------------------------------------------------\n\n. margins sex, dydx(age)\n\nAverage marginal effects                                 Number of obs = 4,948\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  age\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nage          |\n         sex |\n       Male  |  -.0150619   .0067745    -2.22   0.026    -.0283429   -.0017809\n     Female  |   .0525586    .006614     7.95   0.000     .0395923    .0655249\n------------------------------------------------------------------------------\n\n. quietly margins sex, at(age = (20 45 70))\n\n. marginsplot\n\nVariables that uniquely identify margins: age sex\n\nWe see significance in the interaction, so we looked at the margins. It looks like men show a slight decline in lead as age increases (again, rescaling, -.015/year becomes -1.5 over 100 years - not very interesting) while women show a much more significant increase as age increases (roughly 1 unit every 20 years). The marginal plot helps us to visualize this. For men, from age 20 to 70, the average lead decreases barely half a point. For women, we see nearly a 3 point average increase."
  },
  {
    "objectID": "solutions.html#exercise-5",
    "href": "solutions.html#exercise-5",
    "title": "8  Exercise solutions",
    "section": "8.5 Exercise 5",
    "text": "8.5 Exercise 5\n. webuse nhanes2, clear\n\n. logit diabetes i.sex i.race c.age weight height i.region\n\nIteration 0:  Log likelihood = -1999.7591  \nIteration 1:  Log likelihood = -1819.9899  \nIteration 2:  Log likelihood = -1777.7462  \nIteration 3:  Log likelihood = -1776.9939  \nIteration 4:  Log likelihood = -1776.9935  \nIteration 5:  Log likelihood = -1776.9935  \n\nLogistic regression                                     Number of obs = 10,349\n                                                        LR chi2(9)    = 445.53\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -1776.9935                             Pseudo R2     = 0.1114\n\n------------------------------------------------------------------------------\n    diabetes | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         sex |\n     Female  |   .0934803    .137068     0.68   0.495     -.175168    .3621286\n             |\n        race |\n      Black  |   .5781259   .1326244     4.36   0.000     .3181869    .8380648\n      Other  |    .408093   .3623376     1.13   0.260    -.3020757    1.118262\n             |\n         age |    .058799   .0039646    14.83   0.000     .0510286    .0665695\n      weight |   .0268805   .0031111     8.64   0.000     .0207829    .0329782\n      height |  -.0220928   .0076659    -2.88   0.004    -.0371175    -.007068\n             |\n      region |\n         MW  |  -.0265833   .1418506    -0.19   0.851    -.3046054    .2514388\n          S  |   .0998071   .1375642     0.73   0.468    -.1698138    .3694281\n          W  |  -.0984779   .1457444    -0.68   0.499    -.3841316    .1871758\n             |\n       _cons |  -4.645124     1.3496    -3.44   0.001    -7.290292   -1.999956\n------------------------------------------------------------------------------\n\n\n\n. estat gof\n\nGoodness-of-fit test after logistic model\nVariable: diabetes\n\n      Number of observations =   10,349\nNumber of covariate patterns =   10,349\n         Pearson chi2(10339) = 10025.54\n                 Prob &gt; chi2 =   0.9860\n\n. estat gof, group(20)\nnote: obs collapsed on 20 quantiles of estimated probabilities.\n\nGoodness-of-fit test after logistic model\nVariable: diabetes\n\n  Number of observations = 10,349\n        Number of groups =     20\nHosmer–Lemeshow chi2(18) =  15.92\n             Prob &gt; chi2 = 0.5983\n\n. lroc\n\nLogistic model for diabetes\n\nNumber of observations =    10349\nArea under ROC curve   =   0.7665\n\nWe cannot reject the model fit (even once we switch to the proper Hosmer-Lemeshow test, which used 20 instead of 10 because we have 10 predictors). The ROC and AUC look decent but not great.\n\n\n\n. margins race, pwcompare(pv)\n\nPairwise comparisons of predictive margins              Number of obs = 10,349\nModel VCE: OIM\n\nExpression: Pr(diabetes), predict()\n\n--------------------------------------------------------\n                |            Delta-method    Unadjusted\n                |   Contrast   std. err.      z    P&gt;|z|\n----------------+---------------------------------------\n           race |\nBlack vs White  |   .0301076   .0081584     3.69   0.000\nOther vs White  |   .0197925   .0204751     0.97   0.334\nOther vs Black  |  -.0103152   .0220354    -0.47   0.640\n--------------------------------------------------------\n\n. margins region, pwcompare(pv)\n\nPairwise comparisons of predictive margins              Number of obs = 10,349\nModel VCE: OIM\n\nExpression: Pr(diabetes), predict()\n\n-----------------------------------------------------\n             |            Delta-method    Unadjusted\n             |   Contrast   std. err.      z    P&gt;|z|\n-------------+---------------------------------------\n      region |\n   MW vs NE  |  -.0011484   .0061371    -0.19   0.852\n    S vs NE  |   .0045414   .0062131     0.73   0.465\n    W vs NE  |  -.0041309   .0061392    -0.67   0.501\n    S vs MW  |   .0056899   .0056911     1.00   0.317\n    W vs MW  |  -.0029825   .0056876    -0.52   0.600\n     W vs S  |  -.0086724   .0057535    -1.51   0.132\n-----------------------------------------------------\nBlacks are more likely to have diabetes than whites or others. Age and weight are positive predictors whereas height is a negative predictor for some reason. There is no effect of gender or region."
  },
  {
    "objectID": "solutions.html#exercise-6",
    "href": "solutions.html#exercise-6",
    "title": "8  Exercise solutions",
    "section": "8.6 Exercise 6",
    "text": "8.6 Exercise 6\n. webuse chicken, clear\n\n. melogit complain grade i.race i.gender tenure age income ///\n&gt;     nworkers i.genderm || restaurant:\n\nFitting fixed-effects model:\n\nIteration 0:  Log likelihood = -1341.7541  \nIteration 1:  Log likelihood = -1337.7735  \nIteration 2:  Log likelihood = -1337.7659  \nIteration 3:  Log likelihood = -1337.7659  \n\nRefining starting values:\n\nGrid node 0:  Log likelihood =  -1331.542\n\nFitting full model:\n\nIteration 0:  Log likelihood =  -1331.542  \nIteration 1:  Log likelihood = -1323.2469  \nIteration 2:  Log likelihood = -1321.7555  \nIteration 3:  Log likelihood = -1321.7325  \nIteration 4:  Log likelihood = -1321.7325  \n\nMixed-effects logistic regression               Number of obs     =      2,763\nGroup variable: restaurant                      Number of groups  =        500\n\n                                                Obs per group:\n                                                              min =          3\n                                                              avg =        5.5\n                                                              max =          8\n\nIntegration method: mvaghermite                 Integration pts.  =          7\n\n                                                Wald chi2(9)      =     117.56\nLog likelihood = -1321.7325                     Prob &gt; chi2       =     0.0000\n------------------------------------------------------------------------------\n    complain | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |   .0616416   .0463355     1.33   0.183    -.0291742    .1524574\n             |\n        race |\n          2  |   .5512152    .140432     3.93   0.000     .2759736    .8264568\n          3  |   1.180759    .137298     8.60   0.000     .9116599    1.449858\n             |\n    1.gender |   .5866638   .1059402     5.54   0.000     .3790247    .7943029\n      tenure |  -.0699963   .1727861    -0.41   0.685    -.4086509    .2686582\n         age |  -.0748883   .0230589    -3.25   0.001    -.1200828   -.0296938\n      income |  -.0034531   .0016933    -2.04   0.041     -.006772   -.0001343\n    nworkers |  -.0303442   .0391781    -0.77   0.439    -.1071318    .0464435\n   1.genderm |   .0795912   .1245047     0.64   0.523    -.1644336    .3236159\n       _cons |   .3910687   1.103955     0.35   0.723    -1.772644    2.554781\n-------------+----------------------------------------------------------------\nrestaurant   |\n   var(_cons)|   .5755458   .1417722                      .3551455    .9327247\n------------------------------------------------------------------------------\nLR test vs. logistic model: chibar2(01) = 32.07       Prob &gt;= chibar2 = 0.0000\n\n\n\nWe can’t look at fit statistics, but the \\(^\\chi^2^\\) is significant, so we’re doing better than chance.\n\n\n\n. margins race, pwcompare(pv)\n\nPairwise comparisons of predictive margins               Number of obs = 2,763\nModel VCE: OIM\n\nExpression: Marginal predicted mean, predict()\n\n-----------------------------------------------------\n             |            Delta-method    Unadjusted\n             |   Contrast   std. err.      z    P&gt;|z|\n-------------+---------------------------------------\n        race |\n     2 vs 1  |   .0665377   .0166607     3.99   0.000\n     3 vs 1  |   .1685295   .0185599     9.08   0.000\n     3 vs 2  |   .1019918    .019303     5.28   0.000\n-----------------------------------------------------\nUnfortunately, this data is poorly labeled so we can’t talk in specifics about things, but generally\n\nRace 1, 2 and 3 have increasing odds of a complaint.\nGender 1 has significantly higher odds than gender 2.\nAge and income are negatively related to the odds of a complaint (older, more well paid employees are less likely to have complaints).\nNeither restaurant level characteristic is significant once server characteristics are accounted for.\n\n\n\n\nThe estimated random variance is non-zero, so yes, the random effects for restaurants are warranted."
  }
]